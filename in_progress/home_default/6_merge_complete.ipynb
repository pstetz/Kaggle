{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "# <u>Table of Contents</u>\n",
    "1.) [TODO](#todo)  \n",
    "2.) [Imports](#imports)  \n",
    "3.) [Load data](#load)  \n",
    "4.) [Bureau Balance](#bureau_bal)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.1.) [Merge into Bureau](#merge_bureau_bal)  \n",
    "5.) [POS CASH balance](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.1.) [Missing values](#pos_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.2.) [Merge into Previous Application](#merge_pos_cash)  \n",
    "6.) [Installment Payments](#install_pay)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.1.) [Missing values](#install_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.2.) [Merge into Previous Application](#merge_install_pay)  \n",
    "7.) [Credit Card Balance](#credit)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.1.) [Missing values](#credit_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.2.) [Merge into Previous Application](#merge_credit)  \n",
    "8.) [Modeling](#models)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.1.) [LGBM Regressor](#lgbm)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.2.) [Linear Regressor](#linear)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.3.) [Random Forest Regressor](#random_forest)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.4.) [Logistic Regression](#logistic)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.5.) [Lasso Regression](#lasso)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.6.) [Ridge Regression](#ridge)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.7.) [XGB Regressor](#xgb)  \n",
    "9.) [Save file to CSV](#save)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todo\"></a>\n",
    "\n",
    "# [^](#toc) <u>TODO</u>\n",
    "\n",
    "- Fix skew on columns\n",
    "- Tinker with the best way to replace missing values (dropping cols?)\n",
    "- Look for outliers\n",
    "- Merge db together\n",
    "- Include timeline relatoinships like MONTHS_BALANCE\n",
    "- Tune model parameters\n",
    "- Address [this](https://www.kaggle.com/c/home-credit-default-risk/discussion/57248)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "# [^](#toc) <u>Imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to create dummy variables of categorical features\n",
    "def get_dummies(df, cats):\n",
    "    for col in cats:\n",
    "        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"load\"></a>\n",
    "\n",
    "# [^](#toc) <u>Load data</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (307511, 123)\n",
      "Shape of test: (48744, 122)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../../data/home_default/\"\n",
    "\n",
    "train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "test  = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "\n",
    "test['is_train'] = 0\n",
    "train['is_train'] = 1\n",
    "\n",
    "print(\"Shape of train:\", train.shape)\n",
    "print(\"Shape of test:\",  test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load other data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau = pd.read_csv(DATA_PATH + \"bureau.csv\")\n",
    "bureau_balance = pd.read_csv(DATA_PATH + \"bureau_balance.csv\")\n",
    "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_bal\"></a>\n",
    "\n",
    "# [^](#toc) <u>Bureau Balance</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup bureau balance - get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau_balance = get_dummies(bureau_balance, [\"STATUS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau_bal\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Bureau</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = bureau_balance.drop([\"MONTHS_BALANCE\", \"STATUS\"], axis=1)\n",
    "\n",
    "# prep for merge\n",
    "bureau_balance = bureau_balance.groupby(\"SK_ID_BUREAU\").sum()\n",
    "\n",
    "# Merge\n",
    "bureau = bureau.merge(right=bureau_balance.reset_index(), how='left', on='SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in new missing values with -1\n",
    "\n",
    "There won't be any effect if we replace NaN with -1\n",
    "\n",
    "FIXME: make this simpler code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]] = (\n",
    "    bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]]\n",
    "        .fillna(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"pos_cash\"></a>\n",
    "\n",
    "# [^](#toc) <u>POS CASH balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcb = pd.read_csv(DATA_PATH + \"POS_CASH_balance.csv\")\n",
    "print(\"Shape of pcb:\",  pcb.shape)\n",
    "\n",
    "print(\"\\nColumns of pcb:\")\n",
    "print(\" --- \".join(pcb.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_nan\"></a>\n",
    "\n",
    "### [^](#toc) Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n",
    "    pcb[col] = pcb[col].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcb = pcb[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "\n",
    "pcb = get_dummies(pcb, [\"NAME_CONTRACT_STATUS\"])\n",
    "pcb = pcb.drop(\"NAME_CONTRACT_STATUS\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_pos_cash\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for merge\n",
    "merge_df = pcb.groupby(\"SK_ID_PREV\").sum()\n",
    "\n",
    "merged_cols = ['pos_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"install_pay\"></a>\n",
    "\n",
    "# [^](#toc) <u>Installment Payments</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_pay = pd.read_csv(DATA_PATH + \"installments_payments.csv\")\n",
    "print(\"Shape of install_pay:\",  install_pay.shape)\n",
    "\n",
    "print(\"\\nColumns of install_pay:\")\n",
    "print(\" --- \".join(install_pay.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install_nan\"></a>\n",
    "\n",
    "### [^](#toc) <u>Missing values</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in (\"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"):\n",
    "    install_pay[col + \"_nan\"] = install_pay[col].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "    install_pay[col] = install_pay[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_pay[\"AMT_MISSING\"] = install_pay[\"AMT_INSTALMENT\"] - install_pay[\"AMT_PAYMENT\"]\n",
    "temp = install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"]\n",
    "merge_df = pd.DataFrame({\n",
    "    \"INSTALL_missing_max\": temp.max(),\n",
    "    \"INSTALL_missing_min\": temp.min(),\n",
    "    \"INSTALL_missing_med\": temp.median(),\n",
    "    \"INSTALL_payment_nan\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_PAYMENT_nan\"].sum()\n",
    "})\n",
    "merged_cols = [\"INSTALL_missing_max\", \"INSTALL_missing_min\", \"INSTALL_missing_med\", \"INSTALL_payment_nan\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_install_pay\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_app[\"no_install\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in merged_cols:\n",
    "    not_null = prev_app[col].notnull()\n",
    "    mode = prev_app[not_null][col].mode()\n",
    "    prev_app[col] = prev_app[col].fillna(mode)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"credit\"></a>\n",
    "\n",
    "# [^](#toc) <u>Credit Card Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\n",
    "print(\"Shape of credit_card:\",  credit_card.shape)\n",
    "\n",
    "print(\"\\nColumns of credit_card:\")\n",
    "print(\" --- \".join(credit_card.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"credit_nan\"></a>\n",
    "\n",
    "### Missing Values and Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "### Remove outliers\n",
    "# Gets indices with outlier values\n",
    "temp = credit_card[credit_card.NAME_CONTRACT_STATUS.isin([\"Refused\", \"Approved\"])].index\n",
    "\n",
    "# Drops outlier values\n",
    "credit_card = credit_card.drop(temp, axis=0)\n",
    "\n",
    "# ------------------------------\n",
    "#### Fill in missing values\n",
    "cols = [\n",
    "        \"AMT_DRAWINGS_ATM_CURRENT\", \"AMT_DRAWINGS_OTHER_CURRENT\", \"AMT_DRAWINGS_POS_CURRENT\", \n",
    "        \"AMT_INST_MIN_REGULARITY\", \"AMT_PAYMENT_CURRENT\", \"CNT_DRAWINGS_ATM_CURRENT\", \n",
    "        \"CNT_DRAWINGS_OTHER_CURRENT\", \"CNT_DRAWINGS_POS_CURRENT\", \"CNT_INSTALMENT_MATURE_CUM\"\n",
    "]\n",
    "for col in cols:\n",
    "    not_null = credit_card[col].notnull()\n",
    "    mode = float(credit_card[not_null][col].mode())\n",
    "    credit_card[col] = credit_card[col].fillna(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = credit_card[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "\n",
    "temp = get_dummies(temp, [\"NAME_CONTRACT_STATUS\"])\n",
    "temp = temp.drop(\"NAME_CONTRACT_STATUS\", axis=1)\n",
    "temp = temp.groupby(\"SK_ID_PREV\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.DataFrame({\n",
    "    \"AMT_BALANCE\": credit_card.groupby(\"SK_ID_PREV\").AMT_BALANCE.mean(),\n",
    "    \"SK_DPD\": credit_card.groupby(\"SK_ID_PREV\").SK_DPD.max(),\n",
    "    \"SK_DPD_DEF\": credit_card.groupby(\"SK_ID_PREV\").SK_DPD_DEF.max()\n",
    "})\n",
    "\n",
    "merge_df = temp.join(merge_df)\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_credit\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_cols = ['credit_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in new NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app[\"no_credit\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in merged_cols:\n",
    "    not_null = prev_app[col].notnull()\n",
    "    median = prev_app[not_null][col].median()\n",
    "    prev_app[col] = prev_app[col].fillna(median)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unneeded SK_ID_PREV from prev_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_app = prev_app.drop(\"SK_ID_PREV\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split again into predictors, target, and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train.TARGET\n",
    "train_x = train.drop([\"TARGET\"], axis=1)\n",
    "\n",
    "test_id = test.SK_ID_CURR\n",
    "test_x  = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = pd.concat([train_x, test_x])\n",
    "train_N = len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "#### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_cat(df):\n",
    "    for col in [col for col in df if df[col].dtype==object]:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df\n",
    "\n",
    "full     = fillna_cat(full)\n",
    "bureau   = fillna_cat(bureau)\n",
    "prev_app = fillna_cat(prev_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillna_num(df):\n",
    "    missing_cols = [col for col in df.columns if any(df[col].isnull())]\n",
    "    for col in missing_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    return df\n",
    "\n",
    "full     = fillna_num(full)\n",
    "bureau   = fillna_num(bureau)\n",
    "prev_app = fillna_num(prev_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn categorical features to dummy columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_app = pd.get_dummies(prev_app)\n",
    "bureau   = pd.get_dummies(bureau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factorize_df(df, cats):\n",
    "    for col in cats:\n",
    "        df[col], _ = pd.factorize(df[col])\n",
    "    return df \n",
    "\n",
    "# Get categorical features\n",
    "data_cats = [col for col in full.columns if full[col].dtype == 'object']\n",
    "\n",
    "# Factorize the dataframe\n",
    "full = factorize_df(full, data_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Previous Application with full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_df         = prev_app.groupby('SK_ID_CURR').mean()\n",
    "merged_cols = ['p_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_NAME_CONTRACT_STATUS_Approved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_NAME_CONTRACT_STATUS_Approved\n",
      "p_NAME_CONTRACT_STATUS_Approved\n"
     ]
    }
   ],
   "source": [
    "full[\"no_prev_app\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in merged_cols:\n",
    "    try:\n",
    "        not_null  = full[col].notnull()\n",
    "        median    = full[not_null][col].median()\n",
    "        full[col] = full[col].fillna(median)    \n",
    "    except:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Bureau with full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average Values for all bureau features \n",
    "bureau_avg = bureau.groupby('SK_ID_CURR').mean()\n",
    "bureau_avg['buro_count'] = bureau[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n",
    "bureau_avg.columns = ['b_' + f_ for f_ in bureau_avg.columns]\n",
    "full = full.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I notice NaN values sneak into full after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = fillna_num(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split full back into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = full[:train_N]\n",
    "test_x = full[train_N:]\n",
    "del full, train_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# [^](#toc) <u>Models </u>\n",
    "\n",
    "### Model imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "kfold = KFold(n_splits=2)\n",
    "    \n",
    "def rmsle_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, train_x, train_y, cv=kfold, scoring=\"neg_mean_squared_error\"))\n",
    "    print(rmse)\n",
    "    print()\n",
    "    print(sum(rmse) / len(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lgbm\"></a>\n",
    "\n",
    "### [^](#toc) <u>LGBM Regressor</u>\n",
    "\n",
    "(Training takes 545 seconds)\n",
    "\n",
    "(Scoring takes 722 seconds)\n",
    "\n",
    "(open Markdown for notes on [LGBM](http://lightgbm.readthedocs.io/en/latest/Python-API.html#lightgbm.LGBMRegressor) and parameter search attempts)\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "# LGBM Parameters\n",
    "\n",
    "boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, **kwargs\n",
    "\n",
    "<\\div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 933 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lgbm_model = LGBMRegressor(\n",
    "                            learning_rate= 0.01,\n",
    "                            num_leaves= 48,\n",
    "                            num_iteration= 5000,\n",
    "                            max_depth= 7\n",
    "                          )\n",
    "lgbm_model.fit(train_x, train_y)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rmsle_cv(lgbm_model)\n",
    "print(\"\\nScoring took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear\"></a>\n",
    "\n",
    "### [^](#toc) <u>Linear Regressor</u>\n",
    "\n",
    "(Training takes 11 seconds)\n",
    "\n",
    "(Scoring takes 13 seconds)\n",
    "\n",
    "(open Markdown for notes on [Linear Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and parameter search attempts)\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "# Linear Parameters\n",
    "\n",
    "fit_intercept=True,\n",
    "normalize=False,\n",
    "copy_X=True,\n",
    "n_jobs=1\n",
    "\n",
    "<\\div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 11 seconds\n"
     ]
    }
   ],
   "source": [
    "lin_model = LinearRegression()\n",
    "\n",
    "start = time.time()\n",
    "lin_model.fit(train_x, train_y)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2640429   0.26175571]\n",
      "\n",
      "0.262899305367\n",
      "\n",
      "Scoring took 12 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rmsle_cv(lin_model)\n",
    "print(\"\\nScoring took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"random_forest\"></a>\n",
    "\n",
    "### [^](#toc) <u>Random Forest Regressor</u>\n",
    "\n",
    "WARNING: Takes a lot of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27978732  0.28013387  0.28058117]\n",
      "\n",
      "0.280167454734\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "start = time.time()\n",
    "rf_model.fit(train_x, train_y)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rmsle_cv(rf_model)\n",
    "print(\"\\nScoring took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logistic\"></a>\n",
    "\n",
    "### [^](#toc) <u>Logistic Regression</u>\n",
    "\n",
    "(Training takes 79 seconds)\n",
    "\n",
    "(Scoring takes 103 seconds)\n",
    "\n",
    "(open Markdown for notes on [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and parameter search attempts)\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "# Logistic Parameters\n",
    "\n",
    "penalty=’l2’,\n",
    "dual=False,\n",
    "tol=0.0001,\n",
    "C=1.0,\n",
    "fit_intercept=True,\n",
    "intercept_scaling=1,\n",
    "class_weight=None,\n",
    "random_state=None,\n",
    "solver=’liblinear’,\n",
    "max_iter=100,\n",
    "multi_class=’ovr’,\n",
    "verbose=0,\n",
    "warm_start=False,\n",
    "n_jobs=1\n",
    "\n",
    "<\\div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 76 seconds\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "\n",
    "start = time.time()\n",
    "log_model.fit(train_x, train_y)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28573124  0.2826771 ]\n",
      "\n",
      "0.284204172243\n",
      "\n",
      "Scoring took 115 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rmsle_cv(log_model)\n",
    "print(\"\\nScoring took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lasso\"></a>\n",
    "\n",
    "### [^](#toc) <u>Lasso Regression</u>\n",
    "\n",
    "(Training takes 71 seconds)\n",
    "\n",
    "(Scoring takes 72 seconds)\n",
    "\n",
    "(open Markdown for notes on [Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) and parameter search attempts)\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "# Lasso Parameters\n",
    "\n",
    "alpha=1.0,\n",
    "fit_intercept=True,\n",
    "normalize=False,\n",
    "precompute=False,\n",
    "copy_X=True,\n",
    "max_iter=1000,\n",
    "tol=0.0001,\n",
    "warm_start=False,\n",
    "positive=False,\n",
    "random_state=None,\n",
    "selection=’cyclic’\n",
    "\n",
    "<\\div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 93 seconds\n"
     ]
    }
   ],
   "source": [
    "las_model = Lasso(random_state=17)\n",
    "\n",
    "start = time.time()\n",
    "las_model.fit(train_x, train_y)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27082038  0.26825154]\n",
      "\n",
      "0.269535959388\n",
      "\n",
      "Scoring took 76 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rmsle_cv(las_model)\n",
    "print(\"\\nScoring took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ridge\"></a>\n",
    "\n",
    "### [^](#toc) <u>Ridge Regression</u>\n",
    "\n",
    "(Training takes 5 seconds)\n",
    "\n",
    "(Scoring takes 5 seconds)\n",
    "\n",
    "\n",
    "(open Markdown for notes on [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) and parameter search attempts)\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "# Ridge Parameters\n",
    "\n",
    "alpha=1.0,\n",
    "fit_intercept=True,\n",
    "normalize=False,\n",
    "copy_X=True,\n",
    "max_iter=None,\n",
    "tol=0.001,\n",
    "solver=’auto’,\n",
    "random_state=None\n",
    "\n",
    "<\\div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 6 seconds\n"
     ]
    }
   ],
   "source": [
    "rid_model = Ridge(random_state=17)\n",
    "\n",
    "start = time.time()\n",
    "rid_model.fit(train_x, train_y)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26399949  0.2616963 ]\n",
      "\n",
      "0.262847894361\n",
      "\n",
      "Scoring took 6 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rmsle_cv(rid_model)\n",
    "print(\"\\nScoring took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"xgb\"></a>\n",
    "\n",
    "### [^](#toc) <u>XGB Regressor</u>\n",
    "\n",
    "(Training takes 544, 533 seconds)\n",
    "\n",
    "(Scoring takes 453, 487 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 553 seconds\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBRegressor()\n",
    "\n",
    "start = time.time()\n",
    "xgb_model.fit(train_x, train_y)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26217886  0.2599844 ]\n",
      "\n",
      "0.261081632436\n",
      "\n",
      "Scoring took 487 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rmsle_cv(xgb_model)\n",
    "print(\"\\nScoring took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lgbm_pred = lgbm_model.predict(test_x)\n",
    "lin_pred  = lin_model.predict(test_x)\n",
    "# rf_pred   = rf_model.predict(test_x)\n",
    "log_pred  = log_model.predict(test_x)\n",
    "las_pred  = las_model.predict(test_x)\n",
    "rid_pred  = rid_model.predict(test_x)\n",
    "xgb_pred  = xgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_pred = 0.2 * lin_pred + 0.1*log_pred + 0.15*las_pred + 0.25*rid_pred + 0.3*xgb_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict predictions to appropriate range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred = np.clip(final_pred, 0, 1)\n",
    "\n",
    "# Sanity check\n",
    "any(final_pred < 0) or any(final_pred > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "\n",
    "# [^](#toc) <u>Save file to CSV</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"SK_ID_CURR\": test_id,\n",
    "    \"TARGET\": final_pred\n",
    "}).to_csv(\"../../submissions/stacked_merge_bur_bal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
