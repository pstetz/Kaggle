{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "# <u>Table of Contents</u>\n",
    "1.) [TODO](#todo)  \n",
    "2.) [Imports](#imports)  \n",
    "3.) [Bureau](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.1.) [Data Processing](#bureau_process)  \n",
    "4.) [Bureau Balance](#bureau_bal)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.1.) [Merge into Bureau](#merge_bureau_bal)  \n",
    "5.) [Previous Application](#prev_app)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.1.) [Data Processing](#prev_process)  \n",
    "6.) [POS CASH balance](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.1.) [Data Processing](#pos_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.2.) [Merge into Previous Application](#merge_pos_cash)  \n",
    "7.) [Installment Payments](#install_pay)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.1.) [Merge into Previous Application](#merge_install_pay)  \n",
    "8.) [Credit Card Balance](#credit)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.1.) [Data Processing](#credit_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.2.) [Merge into Previous Application](#merge_credit)  \n",
    "9.) [Miscellaneous clean up](#misc)  \n",
    "10.) [Final Data Prep](#final_merge)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.1.) [Data Processing](#final_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.2.) [Create Features](#train_feat)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.3.) [Categorical values](#train_cat)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.4.) [Merge Previous Application with Full](#merge_prev)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.5.) [Merge Bureau with Full](#merge_bureau)  \n",
    "11.) [Modeling](#models)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.1.) [Feature Reduction](#feat_reduction)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.2.) [Most important features](#important_feats)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.3.) [Parameter tuning](#param_tuning)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.4.) [CV Score](#cv)  \n",
    "12.) [Final submission](#final)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 12.1.) [Final predictions](#final_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todo\"></a>\n",
    "\n",
    "# [^](#toc) <u>TODO</u>\n",
    "\n",
    "- Fix skew on columns\n",
    "- Tinker with the best way to replace missing values (dropping cols?)\n",
    "- Look for outliers\n",
    "- Include timeline relatoinships like MONTHS_BALANCE\n",
    "- Address [this](https://www.kaggle.com/c/home-credit-default-risk/discussion/57248)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "# [^](#toc) <u>Imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Time keeper\n",
    "import time\n",
    "\n",
    "# Randomize seeds\n",
    "import random\n",
    "\n",
    "# Garbage collector\n",
    "import gc\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Removes warnings from output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dummies(df, cats):\n",
    "    for col in cats:\n",
    "        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "    return df \n",
    "\n",
    "def factorize_df(df, cats):\n",
    "    for col in cats:\n",
    "        df[col], _ = pd.factorize(df[col])\n",
    "    return df \n",
    "\n",
    "DATA_PATH = \"../data/home_default/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"bureau\"></a>\n",
    "\n",
    "# [^](#toc) Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau: (1716428, 17)\n",
      "\n",
      "Columns of bureau:\n",
      "SK_ID_CURR --- SK_ID_BUREAU --- CREDIT_ACTIVE --- CREDIT_CURRENCY --- DAYS_CREDIT --- CREDIT_DAY_OVERDUE --- DAYS_CREDIT_ENDDATE --- DAYS_ENDDATE_FACT --- AMT_CREDIT_MAX_OVERDUE --- CNT_CREDIT_PROLONG --- AMT_CREDIT_SUM --- AMT_CREDIT_SUM_DEBT --- AMT_CREDIT_SUM_LIMIT --- AMT_CREDIT_SUM_OVERDUE --- CREDIT_TYPE --- DAYS_CREDIT_UPDATE --- AMT_ANNUITY\n"
     ]
    }
   ],
   "source": [
    "bureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\n",
    "print(\"Shape of bureau:\", bureau.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau:\")\n",
    "print(\" --- \".join(bureau.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Lump together values with low counts\n",
    "# CREDIT_CURRENCY\n",
    "cols = [\"currency 3\", \"currency 4\"]\n",
    "bureau.CREDIT_CURRENCY = bureau.CREDIT_CURRENCY.map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# CREDIT_TYPE\n",
    "cols = [\"Cash loan (non-earmarked)\", \"Real estate loan\", \"Loan for the purchase of equipment\",\n",
    "        \"Loan for purchase of shares (margin lending)\", \"Interbank credit\", \"Mobile operator loan\"]\n",
    "bureau.CREDIT_TYPE = bureau.CREDIT_TYPE.map(lambda x: \"MISC\" if x in cols else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_bal\"></a>\n",
    "\n",
    "# [^](#toc) <u>Bureau Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau_balance: (27299925, 3)\n",
      "\n",
      "Columns of bureau_balance:\n",
      "SK_ID_BUREAU --- MONTHS_BALANCE --- STATUS\n"
     ]
    }
   ],
   "source": [
    "bureau_balance = pd.read_csv(DATA_PATH + \"bureau_balance.csv\")\n",
    "print(\"Shape of bureau_balance:\",  bureau_balance.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau_balance:\")\n",
    "print(\" --- \".join(bureau_balance.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau_bal\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Bureau</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get sum of counts in categorical column\n",
    "merge_df = get_dummies(bureau_balance, [\"STATUS\"])\n",
    "cols = ['STATUS_0', 'STATUS_1', 'STATUS_2', 'STATUS_3', 'STATUS_4', 'STATUS_5', 'STATUS_C', 'STATUS_X']\n",
    "for col in cols:\n",
    "    merge_df[col] = merge_df[col] / (merge_df[\"MONTHS_BALANCE\"] - 1)\n",
    "merge_df = merge_df.drop([\"MONTHS_BALANCE\", \"STATUS\"], axis=1)\n",
    "merge_df = merge_df.groupby(\"SK_ID_BUREAU\").sum().reset_index()\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = bureau_balance.groupby(\"SK_ID_BUREAU\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_BUREAU\").set_index(\"SK_ID_BUREAU\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['bur_bal_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "bureau = bureau.merge(right=merge_df.reset_index(), how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "# Mark missing values\n",
    "bureau[\"no_bureau_bal\"] = bureau[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del bureau_balance, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"prev_app\"></a>\n",
    "\n",
    "# [^](#toc) <u>Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prev_app: (1670214, 37)\n",
      "\n",
      "Columns of prev_app:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NAME_CONTRACT_TYPE --- AMT_ANNUITY --- AMT_APPLICATION --- AMT_CREDIT --- AMT_DOWN_PAYMENT --- AMT_GOODS_PRICE --- WEEKDAY_APPR_PROCESS_START --- HOUR_APPR_PROCESS_START --- FLAG_LAST_APPL_PER_CONTRACT --- NFLAG_LAST_APPL_IN_DAY --- RATE_DOWN_PAYMENT --- RATE_INTEREST_PRIMARY --- RATE_INTEREST_PRIVILEGED --- NAME_CASH_LOAN_PURPOSE --- NAME_CONTRACT_STATUS --- DAYS_DECISION --- NAME_PAYMENT_TYPE --- CODE_REJECT_REASON --- NAME_TYPE_SUITE --- NAME_CLIENT_TYPE --- NAME_GOODS_CATEGORY --- NAME_PORTFOLIO --- NAME_PRODUCT_TYPE --- CHANNEL_TYPE --- SELLERPLACE_AREA --- NAME_SELLER_INDUSTRY --- CNT_PAYMENT --- NAME_YIELD_GROUP --- PRODUCT_COMBINATION --- DAYS_FIRST_DRAWING --- DAYS_FIRST_DUE --- DAYS_LAST_DUE_1ST_VERSION --- DAYS_LAST_DUE --- DAYS_TERMINATION --- NFLAG_INSURED_ON_APPROVAL\n"
     ]
    }
   ],
   "source": [
    "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\n",
    "print(\"Shape of prev_app:\",  prev_app.shape)\n",
    "\n",
    "print(\"\\nColumns of prev_app:\")\n",
    "print(\" --- \".join(prev_app.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prev_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fill in values that should be null\n",
    "prev_app['DAYS_FIRST_DRAWING'       ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_FIRST_DUE'           ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_LAST_DUE'            ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_TERMINATION'         ].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "### Lump together values with low counts\n",
    "# NAME_GOODS_CATEGORY\n",
    "prev_app.NAME_GOODS_CATEGORY = prev_app.NAME_GOODS_CATEGORY.map(\n",
    "    lambda x: \"MISC\" if x in [\"Weapon\", \"Insurance\"] else x)\n",
    "\n",
    "# NAME_CASH_LOAN_PURPOSE\n",
    "prev_app.NAME_CASH_LOAN_PURPOSE = prev_app.NAME_CASH_LOAN_PURPOSE.map(\n",
    "    lambda x: \"MISC\" if x in [\"Buying a garage\", \"Misc\"] else x)\n",
    "\n",
    "# Create features\n",
    "prev_app[\"APP_CREDIT_PERC\"] = prev_app['AMT_APPLICATION'] / prev_app['AMT_CREDIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"pos_cash\"></a>\n",
    "\n",
    "# [^](#toc) <u>POS CASH balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pcb: (10001358, 8)\n",
      "\n",
      "Columns of pcb:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- CNT_INSTALMENT --- CNT_INSTALMENT_FUTURE --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "pcb = pd.read_csv(DATA_PATH + \"POS_CASH_balance.csv\")\n",
    "print(\"Shape of pcb:\",  pcb.shape)\n",
    "\n",
    "print(\"\\nColumns of pcb:\")\n",
    "print(\" --- \".join(pcb.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Outliers\n",
    "pcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_pos_cash\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get Dummies\n",
    "merge_df = pcb[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "merge_df = get_dummies(merge_df, [\"NAME_CONTRACT_STATUS\"])\n",
    "merge_df = merge_df.drop(\"NAME_CONTRACT_STATUS\", axis=1)\n",
    "\n",
    "# Prep for merge\n",
    "count    = merge_df.groupby(\"SK_ID_PREV\").count()\n",
    "merge_df = merge_df.groupby(\"SK_ID_PREV\").sum().reset_index()\n",
    "merge_df[\"N\"] = list(count.iloc[:,0])\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = pcb.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['pos_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_pcb\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del pcb, count, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"install_pay\"></a>\n",
    "\n",
    "# [^](#toc) <u>Installment Payments</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of install_pay: (13605401, 8)\n",
      "\n",
      "Columns of install_pay:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NUM_INSTALMENT_VERSION --- NUM_INSTALMENT_NUMBER --- DAYS_INSTALMENT --- DAYS_ENTRY_PAYMENT --- AMT_INSTALMENT --- AMT_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "install_pay = pd.read_csv(DATA_PATH + \"installments_payments.csv\")\n",
    "print(\"Shape of install_pay:\",  install_pay.shape)\n",
    "\n",
    "print(\"\\nColumns of install_pay:\")\n",
    "print(\" --- \".join(install_pay.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_install_pay\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create new feature\n",
    "install_pay[\"AMT_MISSING\"]  = install_pay[\"AMT_INSTALMENT\"]     - install_pay[\"AMT_PAYMENT\"]\n",
    "install_pay['PAYMENT_PERC'] = install_pay['AMT_PAYMENT']        / install_pay['AMT_INSTALMENT']\n",
    "\n",
    "# Days past due and days before due (no negative values)\n",
    "install_pay['DPD']          = install_pay['DAYS_ENTRY_PAYMENT'] - install_pay['DAYS_INSTALMENT']\n",
    "install_pay['DBD']          = install_pay['DAYS_INSTALMENT']    - install_pay['DAYS_ENTRY_PAYMENT']\n",
    "install_pay['DPD']          = install_pay['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "install_pay['DBD']          = install_pay['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "# Amount of values missing in AMT_PAYMENT\n",
    "install_pay[\"temp\"]         = install_pay[\"AMT_PAYMENT\"].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Select important features\n",
    "merge_df = pd.DataFrame({\n",
    "    \"missing_max\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].max(),\n",
    "    \"missing_min\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].min(),\n",
    "    \"payment_max\": install_pay.groupby(\"SK_ID_PREV\")['PAYMENT_PERC'].max(),\n",
    "    \"payment_min\": install_pay.groupby(\"SK_ID_PREV\")['PAYMENT_PERC'].min(),\n",
    "    \n",
    "    \"payment_nan\": install_pay.groupby(\"SK_ID_PREV\")[\"temp\"].sum(),\n",
    "    \"N\":           install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].count(),\n",
    "    \"unique_ver\":  install_pay.groupby(\"SK_ID_PREV\")[\"NUM_INSTALMENT_VERSION\"].unique()\n",
    "})\n",
    "\n",
    "# Delete temp column\n",
    "install_pay = install_pay.drop(\"temp\", axis=1)\n",
    "\n",
    "# Select median of everything\n",
    "right = install_pay.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "\n",
    "### Merge the two\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['install_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_install\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del install_pay, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"credit\"></a>\n",
    "\n",
    "# [^](#toc) <u>Credit Card Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of credit_card: (3840312, 23)\n",
      "\n",
      "Columns of credit_card:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- AMT_BALANCE --- AMT_CREDIT_LIMIT_ACTUAL --- AMT_DRAWINGS_ATM_CURRENT --- AMT_DRAWINGS_CURRENT --- AMT_DRAWINGS_OTHER_CURRENT --- AMT_DRAWINGS_POS_CURRENT --- AMT_INST_MIN_REGULARITY --- AMT_PAYMENT_CURRENT --- AMT_PAYMENT_TOTAL_CURRENT --- AMT_RECEIVABLE_PRINCIPAL --- AMT_RECIVABLE --- AMT_TOTAL_RECEIVABLE --- CNT_DRAWINGS_ATM_CURRENT --- CNT_DRAWINGS_CURRENT --- CNT_DRAWINGS_OTHER_CURRENT --- CNT_DRAWINGS_POS_CURRENT --- CNT_INSTALMENT_MATURE_CUM --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\n",
    "print(\"Shape of credit_card:\",  credit_card.shape)\n",
    "\n",
    "print(\"\\nColumns of credit_card:\")\n",
    "print(\" --- \".join(credit_card.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"credit_process\"></a>\n",
    "\n",
    "### [^](#toc) <u>Data Processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gets indices with outlier values\n",
    "temp = credit_card[credit_card.NAME_CONTRACT_STATUS.isin([\"Refused\", \"Approved\"])].index\n",
    "\n",
    "# Drops outlier values\n",
    "credit_card = credit_card.drop(temp, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_credit\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create features\n",
    "merge_df = pd.DataFrame({\n",
    "    \"AMT_BALANCE\": credit_card.groupby(\"SK_ID_PREV\").AMT_BALANCE.mean(),\n",
    "    \"SK_DPD\":      credit_card.groupby(\"SK_ID_PREV\").SK_DPD.max(),\n",
    "    \"SK_DPD_DEF\":  credit_card.groupby(\"SK_ID_PREV\").SK_DPD_DEF.max(),\n",
    "    \"N\":           credit_card.groupby(\"SK_ID_PREV\").count().iloc[:,0]\n",
    "})\n",
    "\n",
    "### Categorical column\n",
    "temp = get_dummies(credit_card, [\"NAME_CONTRACT_STATUS\"])\n",
    "cols = ['NAME_CONTRACT_STATUS_Active',\n",
    "       'NAME_CONTRACT_STATUS_Completed', 'NAME_CONTRACT_STATUS_Demand',\n",
    "       'NAME_CONTRACT_STATUS_Sent proposal', 'NAME_CONTRACT_STATUS_Signed']\n",
    "for col in cols:\n",
    "    temp[col] = temp[col] / (temp[\"MONTHS_BALANCE\"] - 1)\n",
    "cols.extend([\"SK_ID_PREV\"])\n",
    "temp = temp[cols]\n",
    "temp = temp.groupby(\"SK_ID_PREV\").sum()\n",
    "\n",
    "# Merge categorical and numerical df\n",
    "merge_df = temp.join(merge_df)\n",
    "\n",
    "### Add the rest of the columns\n",
    "right = credit_card.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['credit_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_credit\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del credit_card, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"misc\"></a>\n",
    "\n",
    "# [^](#toc) <u>Miscellaneous clean up</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Drop unneeded ID columns\n",
    "prev_app = prev_app.drop(\"SK_ID_PREV\", axis=1)\n",
    "bureau   = bureau.drop(\"SK_ID_BUREAU\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"final_merge\"></a>\n",
    "\n",
    "# [^](#toc) <u>Final Data Prep</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (307511, 122)\n",
      "Shape of test: (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "test  = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "\n",
    "print(\"Shape of train:\", train.shape)\n",
    "print(\"Shape of test:\",  test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into predictors, target, and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train.TARGET\n",
    "train_x = train.drop([\"TARGET\"], axis=1)\n",
    "\n",
    "test_id = test.SK_ID_CURR\n",
    "test_x  = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full    = pd.concat([train_x, test_x])\n",
    "train_N = len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final_process\"></a>\n",
    "\n",
    "### [^](#toc) <u>Data Processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Replace maxed values with NaN\n",
    "full['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "### Fill in outlier values\n",
    "full[\"CODE_GENDER\"]        = full[\"CODE_GENDER\"].map(lambda x: \"F\" if x == \"XNA\" else x)\n",
    "full[\"NAME_FAMILY_STATUS\"] = full[\"NAME_FAMILY_STATUS\"].map(lambda x: \"Married\" if x == \"Unknown\" else x)\n",
    "\n",
    "# NAME_INCOME_TYPE\n",
    "cols = [\"Unemployed\", \"Student\", \"Businessman\", \"Maternity leave\"]\n",
    "full[\"NAME_INCOME_TYPE\"] = full[\"NAME_INCOME_TYPE\"].map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# ORGANIZATION_TYPE\n",
    "cols = [\"Trade: type 4\", \"Trade: type 5\"]\n",
    "full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Trade\" if x in cols else x)\n",
    "cols = [\"Industry: type 13\", \"Industry: type 8\"]\n",
    "full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Industry\" if x in cols else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_cat\"></a>\n",
    "\n",
    "### [^](#toc) Categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get dummies\n",
    "cols  = [\"WALLSMATERIAL_MODE\", \"NAME_TYPE_SUITE\", \"NAME_INCOME_TYPE\", \"NAME_FAMILY_STATUS\",\n",
    "               \"NAME_HOUSING_TYPE\", \"OCCUPATION_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"ORGANIZATION_TYPE\",\n",
    "               \"FONDKAPREMONT_MODE\", \"NAME_EDUCATION_TYPE\"]\n",
    "full = get_dummies(full, cols)\n",
    "full = full.drop(cols, axis=1)\n",
    "\n",
    "### Factorize the dataframe\n",
    "cols = [\"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"FLAG_OWN_CAR\",\n",
    "               \"FLAG_OWN_REALTY\", \"HOUSETYPE_MODE\", \"EMERGENCYSTATE_MODE\"]\n",
    "full = factorize_df(full, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_feat\"></a>\n",
    "\n",
    "### [^](#toc) Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full['DAYS_EMPLOYED_PERC']  = full['DAYS_EMPLOYED']    / full['DAYS_BIRTH']\n",
    "full['INCOME_CREDIT_PERC']  = full['AMT_INCOME_TOTAL'] / full['AMT_CREDIT']\n",
    "full['INCOME_PER_PERSON']   = full['AMT_INCOME_TOTAL'] / full['CNT_FAM_MEMBERS']\n",
    "full['ANNUITY_INCOME_PERC'] = full['AMT_ANNUITY']      / full['AMT_INCOME_TOTAL']\n",
    "\n",
    "### Create feature marking number of flags\n",
    "full[\"NUM_FLAGS\"]           = np.zeros(len(full))\n",
    "for flag in ('FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4',\n",
    "             'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7',\n",
    "             'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10',\n",
    "             'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13',\n",
    "             'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16',\n",
    "             'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19',\n",
    "             'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21'):\n",
    "    full[\"NUM_FLAGS\"] += full[flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_prev\"></a>\n",
    "\n",
    "### [^](#toc) Merge Previous Application with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected median of numerical columns\n",
      "Selected max of numerical columns\n",
      "Merged median and max\n",
      "Selected min of numerical columns\n",
      "Merged min with median and max\n",
      "Selected categorical columns\n",
      "Merged categorical and numerical\n",
      "Merged into full\n",
      "Marked missing values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = [\n",
    "        \"NAME_CONTRACT_TYPE\", \"WEEKDAY_APPR_PROCESS_START\",\n",
    "        \"FLAG_LAST_APPL_PER_CONTRACT\", \"NAME_CASH_LOAN_PURPOSE\",\n",
    "        \"NAME_CONTRACT_STATUS\", \"NAME_PAYMENT_TYPE\",\n",
    "        \"CODE_REJECT_REASON\", \"NAME_TYPE_SUITE\", \"NAME_CLIENT_TYPE\",\n",
    "        \"NAME_GOODS_CATEGORY\", \"NAME_PORTFOLIO\", \"NAME_PRODUCT_TYPE\",\n",
    "        \"CHANNEL_TYPE\", \"NAME_SELLER_INDUSTRY\", \"NAME_YIELD_GROUP\",\n",
    "        \"PRODUCT_COMBINATION\", \"SK_ID_CURR\"]\n",
    "min_max_cols = ['AMT_ANNUITY', 'AMT_APPLICATION', 'AMT_CREDIT',\n",
    "                 'AMT_DOWN_PAYMENT', 'AMT_GOODS_PRICE', 'HOUR_APPR_PROCESS_START',\n",
    "                 'NFLAG_LAST_APPL_IN_DAY', 'RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY',\n",
    "                 'RATE_INTEREST_PRIVILEGED', 'DAYS_DECISION', 'SELLERPLACE_AREA',\n",
    "                 'CNT_PAYMENT', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE',\n",
    "                 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION',\n",
    "                 'NFLAG_INSURED_ON_APPROVAL', 'APP_CREDIT_PERC', \"SK_ID_CURR\"]\n",
    "num_cols = [col for col in prev_app.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "### numerical columns - median\n",
    "merge_df         = prev_app[num_cols].groupby('SK_ID_CURR').median()\n",
    "merge_df.columns = [\"MED_\" + col for col in merge_df.columns]\n",
    "print(\"Selected median of numerical columns\")\n",
    "\n",
    "### numerical columns - max\n",
    "right         = prev_app[min_max_cols].groupby(\"SK_ID_CURR\").max()\n",
    "right.columns = [\"MAX_\" + col for col in right.columns]\n",
    "print(\"Selected max of numerical columns\")\n",
    "\n",
    "### Merge median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged median and max\")\n",
    "\n",
    "### numerical columns - min\n",
    "right = prev_app[min_max_cols].groupby(\"SK_ID_CURR\").min()\n",
    "right.columns = [\"MIN_\" + col for col in right.columns]\n",
    "print(\"Selected min of numerical columns\")\n",
    "\n",
    "### Merge min with median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged min with median and max\")\n",
    "\n",
    "### Categorical columns\n",
    "right = prev_app[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "print(\"Selected categorical columns\")\n",
    "\n",
    "### Merge categorical and numerical\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged categorical and numerical\")\n",
    "\n",
    "### Prefix column names\n",
    "merge_df[\"N\"]    = prev_app.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "merged_cols      = ['p_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')\n",
    "print(\"Merged into full\")\n",
    "\n",
    "# Mark missing values\n",
    "full[\"no_prev_app\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "print(\"Marked missing values\")\n",
    "\n",
    "### Delete old variables\n",
    "del merge_df, merged_cols, right, cat_cols, num_cols\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau\"></a>\n",
    "\n",
    "### [^](#toc) Merge Bureau with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected median of numerical columns\n",
      "Selected max of numerical columns\n",
      "Merged median and max\n",
      "Selected min of numerical columns\n",
      "Merged min with median and max\n",
      "Selected categorical columns\n",
      "Merged categorical and numerical\n",
      "Merged into full\n",
      "Marked missing values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE', 'SK_ID_CURR']\n",
    "num_cols = [col for col in bureau.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "### Numeric columns - median\n",
    "merge_df         = bureau[num_cols].groupby('SK_ID_CURR').median()\n",
    "merge_df.columns = [\"MED_\" + col for col in merge_df.columns]\n",
    "print(\"Selected median of numerical columns\")\n",
    "\n",
    "### Numeric columns - max\n",
    "right         = bureau[num_cols].groupby(\"SK_ID_CURR\").max()\n",
    "right.columns = [\"MAX_\" + col for col in right.columns]\n",
    "print(\"Selected max of numerical columns\")\n",
    "\n",
    "### Merge median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged median and max\")\n",
    "\n",
    "### Numeric columns - min\n",
    "right = bureau[num_cols].groupby(\"SK_ID_CURR\").min()\n",
    "right.columns = [\"MIN_\" + col for col in right.columns]\n",
    "print(\"Selected min of numerical columns\")\n",
    "\n",
    "### Merge min with median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged min with median and max\")\n",
    "\n",
    "### Categorical columns\n",
    "right = bureau[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "print(\"Selected categorical columns\")\n",
    "\n",
    "### Merge categorical and numeric\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged categorical and numerical\")\n",
    "\n",
    "### Prefix column names\n",
    "merge_df[\"N\"] = bureau.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "merged_cols      = ['b_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')\n",
    "print(\"Merged into full\")\n",
    "\n",
    "# Mark missing values\n",
    "full[\"no_bureau\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "print(\"Marked missing values\")\n",
    "\n",
    "### Delete old variables\n",
    "del merge_df, merged_cols, right, cat_cols, num_cols\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = full.drop(\"SK_ID_CURR\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split full back into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>...</th>\n",
       "      <th>b_CREDIT_TYPE_Consumer credit</th>\n",
       "      <th>b_CREDIT_TYPE_Credit card</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for business development</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for working capital replenishment</th>\n",
       "      <th>b_CREDIT_TYPE_MISC</th>\n",
       "      <th>b_CREDIT_TYPE_Microloan</th>\n",
       "      <th>b_CREDIT_TYPE_Mortgage</th>\n",
       "      <th>b_CREDIT_TYPE_Unknown type of loan</th>\n",
       "      <th>b_N</th>\n",
       "      <th>no_bureau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 585 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0                   0            0             0                0   \n",
       "1                   0            1             0                1   \n",
       "2                   1            0             1                0   \n",
       "3                   0            1             0                0   \n",
       "4                   0            0             0                0   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "0             0          202500.0    406597.5      24700.5         351000.0   \n",
       "1             0          270000.0   1293502.5      35698.5        1129500.0   \n",
       "2             0           67500.0    135000.0       6750.0         135000.0   \n",
       "3             0          135000.0    312682.5      29686.5         297000.0   \n",
       "4             0          121500.0    513000.0      21865.5         513000.0   \n",
       "\n",
       "   REGION_POPULATION_RELATIVE    ...      b_CREDIT_TYPE_Consumer credit  \\\n",
       "0                    0.018801    ...                                4.0   \n",
       "1                    0.003541    ...                                2.0   \n",
       "2                    0.010032    ...                                2.0   \n",
       "3                    0.008019    ...                                NaN   \n",
       "4                    0.028663    ...                                1.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Credit card  b_CREDIT_TYPE_Loan for business development  \\\n",
       "0                        4.0                                          0.0   \n",
       "1                        2.0                                          0.0   \n",
       "2                        0.0                                          0.0   \n",
       "3                        NaN                                          NaN   \n",
       "4                        0.0                                          0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Loan for working capital replenishment  b_CREDIT_TYPE_MISC  \\\n",
       "0                                                0.0                    0.0   \n",
       "1                                                0.0                    0.0   \n",
       "2                                                0.0                    0.0   \n",
       "3                                                NaN                    NaN   \n",
       "4                                                0.0                    0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Microloan  b_CREDIT_TYPE_Mortgage  \\\n",
       "0                      0.0                     0.0   \n",
       "1                      0.0                     0.0   \n",
       "2                      0.0                     0.0   \n",
       "3                      NaN                     NaN   \n",
       "4                      0.0                     0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Unknown type of loan  b_N  no_bureau  \n",
       "0                                 0.0  8.0          0  \n",
       "1                                 0.0  4.0          0  \n",
       "2                                 0.0  2.0          0  \n",
       "3                                 NaN  NaN          1  \n",
       "4                                 0.0  1.0          0  \n",
       "\n",
       "[5 rows x 585 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = full[:train_N]\n",
    "test_x = full[train_N:]\n",
    "\n",
    "### Processed data look\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# [^](#toc) <u>Modeling</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics         import roc_auc_score, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm                import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat_reduction\"></a>\n",
    "\n",
    "### [^](#toc) Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[200]\tvalid_0's auc: 0.741063\n",
      "[400]\tvalid_0's auc: 0.756603\n",
      "[600]\tvalid_0's auc: 0.768568\n",
      "[800]\tvalid_0's auc: 0.775112\n",
      "[1000]\tvalid_0's auc: 0.778152\n",
      "[1200]\tvalid_0's auc: 0.779786\n",
      "[1400]\tvalid_0's auc: 0.780904\n",
      "[1600]\tvalid_0's auc: 0.781531\n",
      "[1800]\tvalid_0's auc: 0.781982\n",
      "[2000]\tvalid_0's auc: 0.782147\n",
      "[2200]\tvalid_0's auc: 0.782333\n",
      "[2400]\tvalid_0's auc: 0.782354\n",
      "[2600]\tvalid_0's auc: 0.782361\n",
      "Early stopping, best iteration is:\n",
      "[2498]\tvalid_0's auc: 0.782466\n",
      "Training took 1062 seconds\n"
     ]
    }
   ],
   "source": [
    "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)\n",
    "lgb_train = lgb.Dataset(data=training_x, label=training_y)\n",
    "lgb_eval  = lgb.Dataset(data=val_x, label=val_y)\n",
    "\n",
    "# try feature_fraction\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n",
    "          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n",
    "          'min_split_gain':.01, 'min_child_weight':1}\n",
    "\n",
    "start = time.time()\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"important_feats\"></a>\n",
    "\n",
    "### [^](#toc) Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATS = 350\n",
    "\n",
    "feats = sorted(list(zip(model.feature_importance(), train_x.columns)))\n",
    "feats = list(list(zip(*feats[-NUM_FEATS:]))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"param_tuning\"></a>\n",
    "\n",
    "### [^](#toc) Parameter tuning\n",
    "\n",
    "TODO: Add parameters:\n",
    "\n",
    "    min_data_in_leaf\n",
    "    feature_fraction\n",
    "    feature_fraction_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: lambda tuning (L2 regularization)\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "lambda = 10, num_iter = 2500\n",
    "- [2500] train: 0.871737, test: 0.78456\n",
    "\n",
    "lambda = 20, num_iter = 5000\n",
    "- [3270] train: 0.882077, test: 0.785751\n",
    "\n",
    "lambda = 40, num_iter = 3000\n",
    "- [3000] train: 0.868533, test: 0.78583\n",
    "\n",
    "##### Implemented random_state=17\n",
    "\n",
    "lambda = 80, num_iter = 3000\n",
    "- [3000] train: 0.859416, test: 0.785235\n",
    "\n",
    "lambda = 160, num_iter = 4000\n",
    "- [3789] train: 0.86189, test: 0.785736\n",
    "\n",
    "lambda = 0.1, num_iter = 4000\n",
    "- [2603] train: 0.895891, test: 0.784205\n",
    "\n",
    "##### change max_depth from 6 to 7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Other variables\n",
    "--------------\n",
    "\n",
    "learning_rate = 0.01,\n",
    "num_leaves = 48,\n",
    "colsample_bytree = 0.8,\n",
    "subsample = 0.9,\n",
    "max_depth = 6,\n",
    "reg_alpha = 0.1,\n",
    "min_split_gain = 0.01,\n",
    "min_child_weight = 1,\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: max_depth tuning\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "max_depth = 6, num_iter = 3000\n",
    "- [3000] train: 0.859416, test: 0.785235\n",
    "\n",
    "max_depth = 7, num_iter = 4000\n",
    "- [3415] train: 0.878744, test: 0.786247\n",
    "\n",
    "max_depth = 8, num_iter = 4000\n",
    "- [2965] train: 0.875784, test: 0.786024\n",
    "\n",
    "##### Change reg_alpha from 0.1 to 40\n",
    "\n",
    "max_depth = 8, num_iter = 4000\n",
    "- [3420] train: 0.858222, test: 0.785642\n",
    "\n",
    "max_depth = 9, num_iter = 4000\n",
    "- [] train: , test: \n",
    "\n",
    "max_depth = 10, num_iter = 4000\n",
    "- [] train: , test: \n",
    "\n",
    "Other variables\n",
    "--------------\n",
    "\n",
    "learning_rate = 0.01,\n",
    "num_leaves = 48,\n",
    "colsample_bytree = 0.8,\n",
    "subsample = 0.9,\n",
    "reg_alpha = 0.1,\n",
    "reg_lambda = 80,\n",
    "min_split_gain = 0.01,\n",
    "min_child_weight = 1,\n",
    "random_state=17\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: alpha tuning (L1 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_x, val_x, training_y, val_y = train_test_split(train_x[feats], train_y, test_size=0.2, random_state=17)\n",
    "\n",
    "model = LGBMClassifier(\n",
    "                        learning_rate = 0.01,\n",
    "                        num_leaves = 48,\n",
    "                        colsample_bytree = 0.8,\n",
    "                        subsample = 0.9,\n",
    "                        max_depth = 7,\n",
    "                        reg_alpha = 0.1,\n",
    "                        reg_lambda = 40,\n",
    "                        min_split_gain = 0.01,\n",
    "                        min_child_weight = 1,\n",
    "                        num_iteration = 4000,\n",
    "                        random_state=17\n",
    ")\n",
    "\n",
    "model.fit(training_x, training_y, \n",
    "          eval_set= [(training_x, training_y), (val_x, val_y)], \n",
    "          eval_metric='auc', verbose=100, early_stopping_rounds=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cv\"></a>\n",
    "\n",
    "### [^](#toc) CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's auc: 0.728685 + 0.00197275\n",
      "[200]\tcv_agg's auc: 0.742231 + 0.00223602\n",
      "[300]\tcv_agg's auc: 0.752246 + 0.00210872\n",
      "[400]\tcv_agg's auc: 0.761042 + 0.00188143\n",
      "[500]\tcv_agg's auc: 0.767395 + 0.00179124\n",
      "[600]\tcv_agg's auc: 0.772201 + 0.00170439\n",
      "[700]\tcv_agg's auc: 0.775731 + 0.00159968\n",
      "[800]\tcv_agg's auc: 0.778187 + 0.00156327\n",
      "[900]\tcv_agg's auc: 0.779986 + 0.00160404\n",
      "[1000]\tcv_agg's auc: 0.781391 + 0.00164479\n",
      "[1100]\tcv_agg's auc: 0.782433 + 0.00169232\n",
      "[1200]\tcv_agg's auc: 0.78324 + 0.00168115\n",
      "[1300]\tcv_agg's auc: 0.783937 + 0.00163168\n",
      "[1400]\tcv_agg's auc: 0.784519 + 0.00159277\n",
      "[1500]\tcv_agg's auc: 0.784978 + 0.0015947\n",
      "[1600]\tcv_agg's auc: 0.785432 + 0.00157407\n",
      "[1700]\tcv_agg's auc: 0.785781 + 0.0015493\n",
      "[1800]\tcv_agg's auc: 0.786105 + 0.00156533\n",
      "[1900]\tcv_agg's auc: 0.786391 + 0.00156107\n",
      "[2000]\tcv_agg's auc: 0.786633 + 0.00156858\n",
      "[2100]\tcv_agg's auc: 0.786865 + 0.00154762\n",
      "[2200]\tcv_agg's auc: 0.787007 + 0.00153578\n",
      "[2300]\tcv_agg's auc: 0.78718 + 0.00150114\n",
      "[2400]\tcv_agg's auc: 0.787339 + 0.00148027\n",
      "[2500]\tcv_agg's auc: 0.787448 + 0.00145805\n",
      "[2600]\tcv_agg's auc: 0.787524 + 0.00143014\n",
      "[2700]\tcv_agg's auc: 0.787595 + 0.00144052\n",
      "[2800]\tcv_agg's auc: 0.787668 + 0.00141059\n",
      "[2900]\tcv_agg's auc: 0.787717 + 0.0013835\n",
      "[3000]\tcv_agg's auc: 0.787748 + 0.00135984\n",
      "[3100]\tcv_agg's auc: 0.787785 + 0.00136921\n",
      "[3200]\tcv_agg's auc: 0.787806 + 0.00136967\n",
      "[3300]\tcv_agg's auc: 0.787834 + 0.00137968\n",
      "[3400]\tcv_agg's auc: 0.787838 + 0.00138455\n",
      "[3500]\tcv_agg's auc: 0.787854 + 0.00137515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78785864122411065"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(train_x, train_y, usecols, params, dropcols=[]):  \n",
    "    dtrain = lgb.Dataset(train_x[usecols].drop(dropcols, axis=1), train_y)\n",
    "    eval = lgb.cv(params,\n",
    "             dtrain,\n",
    "             nfold=5,\n",
    "             stratified=True,\n",
    "             num_boost_round=20000,\n",
    "             early_stopping_rounds=200,\n",
    "             verbose_eval=100,\n",
    "             seed = 5,\n",
    "             show_stdv=True)\n",
    "    return max(eval['auc-mean'])\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 3500, 'verbose': 0 ,\n",
    "          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':40, \n",
    "          'min_split_gain':.01, 'min_child_weight':1}\n",
    "    \n",
    "get_score(train_x, train_y, feats, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"final\"></a>\n",
    "\n",
    "# [^](#toc) <u>Final submission</u>\n",
    "\n",
    "Help from [Dmitriy Kisil](https://www.kaggle.com/oysiyl) and [his kernel](https://www.kaggle.com/oysiyl/good-fun-with-ligthgbm/code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.739145\tvalid_1's auc: 0.72141\n",
      "[200]\ttraining's auc: 0.755157\tvalid_1's auc: 0.734595\n",
      "[300]\ttraining's auc: 0.771063\tvalid_1's auc: 0.746886\n",
      "[400]\ttraining's auc: 0.783347\tvalid_1's auc: 0.756159\n",
      "[500]\ttraining's auc: 0.793723\tvalid_1's auc: 0.763541\n",
      "[600]\ttraining's auc: 0.802127\tvalid_1's auc: 0.768948\n",
      "[700]\ttraining's auc: 0.808974\tvalid_1's auc: 0.772818\n",
      "[800]\ttraining's auc: 0.814616\tvalid_1's auc: 0.775662\n",
      "[900]\ttraining's auc: 0.819388\tvalid_1's auc: 0.777724\n",
      "[1000]\ttraining's auc: 0.823686\tvalid_1's auc: 0.779236\n",
      "[1100]\ttraining's auc: 0.827613\tvalid_1's auc: 0.780341\n",
      "[1200]\ttraining's auc: 0.831232\tvalid_1's auc: 0.781264\n",
      "[1300]\ttraining's auc: 0.834612\tvalid_1's auc: 0.781969\n",
      "[1400]\ttraining's auc: 0.837893\tvalid_1's auc: 0.782652\n",
      "[1500]\ttraining's auc: 0.840863\tvalid_1's auc: 0.783169\n",
      "[1600]\ttraining's auc: 0.843903\tvalid_1's auc: 0.783651\n",
      "[1700]\ttraining's auc: 0.846852\tvalid_1's auc: 0.784103\n",
      "[1800]\ttraining's auc: 0.849726\tvalid_1's auc: 0.784441\n",
      "[1900]\ttraining's auc: 0.852591\tvalid_1's auc: 0.784721\n",
      "[2000]\ttraining's auc: 0.855285\tvalid_1's auc: 0.784918\n",
      "[2100]\ttraining's auc: 0.857929\tvalid_1's auc: 0.785071\n",
      "[2200]\ttraining's auc: 0.86055\tvalid_1's auc: 0.785278\n",
      "[2300]\ttraining's auc: 0.863155\tvalid_1's auc: 0.78539\n",
      "[2400]\ttraining's auc: 0.865766\tvalid_1's auc: 0.785585\n",
      "[2500]\ttraining's auc: 0.868381\tvalid_1's auc: 0.785757\n",
      "[2600]\ttraining's auc: 0.870742\tvalid_1's auc: 0.785817\n",
      "[2700]\ttraining's auc: 0.873073\tvalid_1's auc: 0.785858\n",
      "[2800]\ttraining's auc: 0.875422\tvalid_1's auc: 0.78599\n",
      "[2900]\ttraining's auc: 0.877699\tvalid_1's auc: 0.786041\n",
      "[3000]\ttraining's auc: 0.87995\tvalid_1's auc: 0.78608\n",
      "[3100]\ttraining's auc: 0.882182\tvalid_1's auc: 0.78611\n",
      "[3200]\ttraining's auc: 0.884097\tvalid_1's auc: 0.786116\n",
      "[3300]\ttraining's auc: 0.886201\tvalid_1's auc: 0.786118\n",
      "[3400]\ttraining's auc: 0.888288\tvalid_1's auc: 0.786184\n",
      "[3500]\ttraining's auc: 0.890349\tvalid_1's auc: 0.786182\n",
      "Early stopping, best iteration is:\n",
      "[3480]\ttraining's auc: 0.889885\tvalid_1's auc: 0.786206\n",
      "Fold  1 AUC : 0.786206\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.739564\tvalid_1's auc: 0.733966\n",
      "[200]\ttraining's auc: 0.75686\tvalid_1's auc: 0.746347\n",
      "[300]\ttraining's auc: 0.771366\tvalid_1's auc: 0.755765\n",
      "[400]\ttraining's auc: 0.783976\tvalid_1's auc: 0.762635\n",
      "[500]\ttraining's auc: 0.794414\tvalid_1's auc: 0.768288\n",
      "[600]\ttraining's auc: 0.802764\tvalid_1's auc: 0.772643\n",
      "[700]\ttraining's auc: 0.809374\tvalid_1's auc: 0.775789\n",
      "[800]\ttraining's auc: 0.81496\tvalid_1's auc: 0.778077\n",
      "[900]\ttraining's auc: 0.819733\tvalid_1's auc: 0.779842\n",
      "[1000]\ttraining's auc: 0.823925\tvalid_1's auc: 0.781143\n",
      "[1100]\ttraining's auc: 0.827898\tvalid_1's auc: 0.782291\n",
      "[1200]\ttraining's auc: 0.831557\tvalid_1's auc: 0.783224\n",
      "[1300]\ttraining's auc: 0.834936\tvalid_1's auc: 0.78403\n",
      "[1400]\ttraining's auc: 0.838109\tvalid_1's auc: 0.784679\n",
      "[1500]\ttraining's auc: 0.841324\tvalid_1's auc: 0.785307\n",
      "[1600]\ttraining's auc: 0.844238\tvalid_1's auc: 0.785746\n",
      "[1700]\ttraining's auc: 0.84716\tvalid_1's auc: 0.786068\n",
      "[1800]\ttraining's auc: 0.849962\tvalid_1's auc: 0.786464\n",
      "[1900]\ttraining's auc: 0.852701\tvalid_1's auc: 0.786765\n",
      "[2000]\ttraining's auc: 0.855383\tvalid_1's auc: 0.786969\n",
      "[2100]\ttraining's auc: 0.858095\tvalid_1's auc: 0.787194\n",
      "[2200]\ttraining's auc: 0.860562\tvalid_1's auc: 0.787345\n",
      "[2300]\ttraining's auc: 0.863156\tvalid_1's auc: 0.787477\n",
      "[2400]\ttraining's auc: 0.865751\tvalid_1's auc: 0.787621\n",
      "[2500]\ttraining's auc: 0.868304\tvalid_1's auc: 0.787762\n",
      "[2600]\ttraining's auc: 0.870661\tvalid_1's auc: 0.787849\n",
      "[2700]\ttraining's auc: 0.873115\tvalid_1's auc: 0.787925\n",
      "[2800]\ttraining's auc: 0.875369\tvalid_1's auc: 0.788041\n",
      "[2900]\ttraining's auc: 0.877558\tvalid_1's auc: 0.788137\n",
      "[3000]\ttraining's auc: 0.879717\tvalid_1's auc: 0.788227\n",
      "[3100]\ttraining's auc: 0.881812\tvalid_1's auc: 0.78824\n",
      "[3200]\ttraining's auc: 0.88399\tvalid_1's auc: 0.788225\n",
      "Early stopping, best iteration is:\n",
      "[3122]\ttraining's auc: 0.882267\tvalid_1's auc: 0.788257\n",
      "Fold  2 AUC : 0.788257\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.740425\tvalid_1's auc: 0.730411\n",
      "[200]\ttraining's auc: 0.755173\tvalid_1's auc: 0.740607\n",
      "[300]\ttraining's auc: 0.769971\tvalid_1's auc: 0.752276\n",
      "[400]\ttraining's auc: 0.78311\tvalid_1's auc: 0.762195\n",
      "[500]\ttraining's auc: 0.793727\tvalid_1's auc: 0.769244\n",
      "[600]\ttraining's auc: 0.801981\tvalid_1's auc: 0.773888\n",
      "[700]\ttraining's auc: 0.808804\tvalid_1's auc: 0.777152\n",
      "[800]\ttraining's auc: 0.814429\tvalid_1's auc: 0.779406\n",
      "[900]\ttraining's auc: 0.819235\tvalid_1's auc: 0.781197\n",
      "[1000]\ttraining's auc: 0.823439\tvalid_1's auc: 0.782431\n",
      "[1100]\ttraining's auc: 0.827273\tvalid_1's auc: 0.783336\n",
      "[1200]\ttraining's auc: 0.830887\tvalid_1's auc: 0.784206\n",
      "[1300]\ttraining's auc: 0.834304\tvalid_1's auc: 0.784881\n",
      "[1400]\ttraining's auc: 0.837643\tvalid_1's auc: 0.785546\n",
      "[1500]\ttraining's auc: 0.840816\tvalid_1's auc: 0.786084\n",
      "[1600]\ttraining's auc: 0.84391\tvalid_1's auc: 0.786537\n",
      "[1700]\ttraining's auc: 0.846893\tvalid_1's auc: 0.786906\n",
      "[1800]\ttraining's auc: 0.84969\tvalid_1's auc: 0.787173\n",
      "[1900]\ttraining's auc: 0.852539\tvalid_1's auc: 0.787446\n",
      "[2000]\ttraining's auc: 0.855253\tvalid_1's auc: 0.787678\n",
      "[2100]\ttraining's auc: 0.857903\tvalid_1's auc: 0.787889\n",
      "[2200]\ttraining's auc: 0.860614\tvalid_1's auc: 0.7881\n",
      "[2300]\ttraining's auc: 0.863062\tvalid_1's auc: 0.788284\n",
      "[2400]\ttraining's auc: 0.865582\tvalid_1's auc: 0.788427\n",
      "[2500]\ttraining's auc: 0.867951\tvalid_1's auc: 0.788452\n",
      "[2600]\ttraining's auc: 0.870328\tvalid_1's auc: 0.788505\n",
      "Early stopping, best iteration is:\n",
      "[2590]\ttraining's auc: 0.870111\tvalid_1's auc: 0.788522\n",
      "Fold  3 AUC : 0.788522\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.738421\tvalid_1's auc: 0.731823\n",
      "[200]\ttraining's auc: 0.75523\tvalid_1's auc: 0.744017\n",
      "[300]\ttraining's auc: 0.770002\tvalid_1's auc: 0.753601\n",
      "[400]\ttraining's auc: 0.78264\tvalid_1's auc: 0.762131\n",
      "[500]\ttraining's auc: 0.79297\tvalid_1's auc: 0.768837\n",
      "[600]\ttraining's auc: 0.801423\tvalid_1's auc: 0.773783\n",
      "[700]\ttraining's auc: 0.808245\tvalid_1's auc: 0.777212\n",
      "[800]\ttraining's auc: 0.814002\tvalid_1's auc: 0.779634\n",
      "[900]\ttraining's auc: 0.818885\tvalid_1's auc: 0.781275\n",
      "[1000]\ttraining's auc: 0.823164\tvalid_1's auc: 0.782562\n",
      "[1100]\ttraining's auc: 0.827017\tvalid_1's auc: 0.783417\n",
      "[1200]\ttraining's auc: 0.830719\tvalid_1's auc: 0.784266\n",
      "[1300]\ttraining's auc: 0.834151\tvalid_1's auc: 0.784886\n",
      "[1400]\ttraining's auc: 0.837266\tvalid_1's auc: 0.785333\n",
      "[1500]\ttraining's auc: 0.840328\tvalid_1's auc: 0.785798\n",
      "[1600]\ttraining's auc: 0.84315\tvalid_1's auc: 0.786244\n",
      "[1700]\ttraining's auc: 0.846009\tvalid_1's auc: 0.786553\n",
      "[1800]\ttraining's auc: 0.848768\tvalid_1's auc: 0.786788\n",
      "[1900]\ttraining's auc: 0.851486\tvalid_1's auc: 0.787036\n",
      "[2000]\ttraining's auc: 0.854223\tvalid_1's auc: 0.787293\n",
      "[2100]\ttraining's auc: 0.856888\tvalid_1's auc: 0.787435\n",
      "[2200]\ttraining's auc: 0.859478\tvalid_1's auc: 0.787598\n",
      "[2300]\ttraining's auc: 0.862041\tvalid_1's auc: 0.787769\n",
      "[2400]\ttraining's auc: 0.864557\tvalid_1's auc: 0.787901\n",
      "[2500]\ttraining's auc: 0.867059\tvalid_1's auc: 0.788022\n",
      "[2600]\ttraining's auc: 0.869459\tvalid_1's auc: 0.788025\n",
      "[2700]\ttraining's auc: 0.871918\tvalid_1's auc: 0.788045\n",
      "Early stopping, best iteration is:\n",
      "[2642]\ttraining's auc: 0.870516\tvalid_1's auc: 0.788055\n",
      "Fold  4 AUC : 0.788055\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.739303\tvalid_1's auc: 0.731643\n",
      "[200]\ttraining's auc: 0.757162\tvalid_1's auc: 0.744335\n",
      "[300]\ttraining's auc: 0.770857\tvalid_1's auc: 0.754113\n",
      "[400]\ttraining's auc: 0.782862\tvalid_1's auc: 0.762503\n",
      "[500]\ttraining's auc: 0.793188\tvalid_1's auc: 0.769271\n",
      "[600]\ttraining's auc: 0.801565\tvalid_1's auc: 0.77404\n",
      "[700]\ttraining's auc: 0.808382\tvalid_1's auc: 0.777669\n",
      "[800]\ttraining's auc: 0.81405\tvalid_1's auc: 0.780338\n",
      "[900]\ttraining's auc: 0.818905\tvalid_1's auc: 0.782268\n",
      "[1000]\ttraining's auc: 0.823182\tvalid_1's auc: 0.783836\n",
      "[1100]\ttraining's auc: 0.826985\tvalid_1's auc: 0.785074\n",
      "[1200]\ttraining's auc: 0.830584\tvalid_1's auc: 0.786095\n",
      "[1300]\ttraining's auc: 0.833906\tvalid_1's auc: 0.786881\n",
      "[1400]\ttraining's auc: 0.837232\tvalid_1's auc: 0.787617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500]\ttraining's auc: 0.840345\tvalid_1's auc: 0.788182\n",
      "[1600]\ttraining's auc: 0.843377\tvalid_1's auc: 0.78869\n",
      "[1700]\ttraining's auc: 0.846347\tvalid_1's auc: 0.789097\n",
      "[1800]\ttraining's auc: 0.849075\tvalid_1's auc: 0.789491\n",
      "[1900]\ttraining's auc: 0.851941\tvalid_1's auc: 0.789835\n",
      "[2000]\ttraining's auc: 0.854655\tvalid_1's auc: 0.790127\n",
      "[2100]\ttraining's auc: 0.857227\tvalid_1's auc: 0.790321\n",
      "[2200]\ttraining's auc: 0.859682\tvalid_1's auc: 0.790518\n",
      "[2300]\ttraining's auc: 0.862293\tvalid_1's auc: 0.790769\n",
      "[2400]\ttraining's auc: 0.864877\tvalid_1's auc: 0.790947\n",
      "[2500]\ttraining's auc: 0.867407\tvalid_1's auc: 0.791072\n",
      "[2600]\ttraining's auc: 0.869817\tvalid_1's auc: 0.791187\n",
      "[2700]\ttraining's auc: 0.872195\tvalid_1's auc: 0.791243\n",
      "[2800]\ttraining's auc: 0.874527\tvalid_1's auc: 0.791368\n",
      "[2900]\ttraining's auc: 0.876742\tvalid_1's auc: 0.791338\n",
      "Early stopping, best iteration is:\n",
      "[2819]\ttraining's auc: 0.874913\tvalid_1's auc: 0.791381\n",
      "Fold  5 AUC : 0.791381\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=17)\n",
    "oof_preds = np.zeros(train_x.shape[0])\n",
    "sub_preds = np.zeros(test_x.shape[0])\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x)):\n",
    "    trn_x, trn_y = train_x[feats].iloc[trn_idx], train_y.iloc[trn_idx]\n",
    "    val_x, val_y = train_x[feats].iloc[val_idx], train_y.iloc[val_idx]\n",
    "  \n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        learning_rate = 0.01,\n",
    "        num_leaves = 48,\n",
    "        colsample_bytree = 0.8,\n",
    "        subsample = 0.9,\n",
    "        max_depth = 7,\n",
    "        reg_alpha = 0.1,\n",
    "        reg_lambda = 40,\n",
    "        min_split_gain = 0.01,\n",
    "        min_child_weight = 1,\n",
    "        num_iteration = 4000,\n",
    "        random_state=17 #random.randint(1, 100) # Recommended to make the seed random\n",
    "    )\n",
    "        \n",
    "    model.fit(trn_x, trn_y, \n",
    "            eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "            eval_metric='auc', verbose=100, early_stopping_rounds=100\n",
    "           )\n",
    "    \n",
    "    oof_preds[val_idx] = model.predict_proba(val_x, num_iteration=model.best_iteration_)[:, 1]\n",
    "    sub_preds         += model.predict_proba(\n",
    "                                             test_x[feats], \n",
    "                                             num_iteration=model.best_iteration_\n",
    "                                            )[:, 1] / folds.n_splits\n",
    "    \n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "    del model, trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final_pred\"></a>\n",
    "\n",
    "### [^](#toc) Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"SK_ID_CURR\": test_id,\n",
    "    \"TARGET\": sub_preds\n",
    "}).to_csv(\"../submissions/lambda_40.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
