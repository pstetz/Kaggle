{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score:\n",
    "\n",
    "CV: 0.792171 + 0.00135745 [4000]  \n",
    "LB: 0.792"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "# <u>Table of Contents</u>\n",
    "1.) [TODO](#todo)  \n",
    "2.) [Imports](#imports)  \n",
    "3.) [Bureau](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.1.) [Data Processing](#bureau_process)  \n",
    "4.) [Bureau Balance](#bureau_bal)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.1.) [Merge into Bureau](#merge_bureau_bal)  \n",
    "5.) [Previous Application](#prev_app)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.1.) [Create Features](#prev_feat)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.2.) [Data Processing](#prev_process)  \n",
    "6.) [POS CASH balance](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.1.) [Data Processing](#pos_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.2.) [Merge into Previous Application](#merge_pos_cash)  \n",
    "7.) [Installment Payments](#install_pay)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.1.) [Create Features](#install_feat)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.2.) [Merge into Previous Application](#merge_install_pay)  \n",
    "8.) [Credit Card Balance](#credit)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.1.) [Data Processing](#credit_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.2.) [Merge into Previous Application](#merge_credit)  \n",
    "9.) [Miscellaneous clean up](#misc)  \n",
    "10.) [Final Data Prep](#final_merge)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.1.) [Data Processing](#final_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.2.) [Create Features](#train_feat)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.3.) [Categorical values](#train_cat)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.4.) [Merge Previous Application with Full](#merge_prev)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.5.) [Merge Bureau with Full](#merge_bureau)  \n",
    "11.) [Modeling](#models)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.1.) [Feature Reduction](#feat_reduction)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.2.) [Most important features](#important_feats)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.3.) [Parameter tuning](#param_tuning)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.4.) [CV Score](#cv)  \n",
    "12.) [Final submission](#final)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 12.1.) [Final predictions](#final_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todo\"></a>\n",
    "\n",
    "# [^](#toc) <u>TODO</u>\n",
    "\n",
    "- Fix skew on columns\n",
    "- Tinker with the best way to replace missing values (dropping cols?)\n",
    "- Look for outliers\n",
    "- Include timeline relatoinships like MONTHS_BALANCE\n",
    "- Added coded value of -1 to missing numerical values\n",
    "- Address [this](https://www.kaggle.com/c/home-credit-default-risk/discussion/57248)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "# [^](#toc) <u>Imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Time keeper\n",
    "import time\n",
    "\n",
    "# Randomize seeds\n",
    "import random\n",
    "\n",
    "# Garbage collector\n",
    "import gc\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Removes warnings from output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dummies(df, cats):\n",
    "    for col in cats:\n",
    "        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "    return df \n",
    "\n",
    "def factorize_df(df, cats):\n",
    "    for col in cats:\n",
    "        df[col], _ = pd.factorize(df[col])\n",
    "    return df\n",
    "\n",
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load(filename):\n",
    "    df = pd.read_csv(DATA_PATH + filename)\n",
    "    return reduce_mem_usage(df)\n",
    "\n",
    "DATA_PATH = \"../data/home_default/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"bureau\"></a>\n",
    "\n",
    "# [^](#toc) Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 222.62 MB\n",
      "Memory usage after optimization is: 112.95 MB\n",
      "Decreased by 49.3%\n",
      "\n",
      "Shape of bureau: (1716428, 17)\n",
      "\n",
      "Columns of bureau:\n",
      "SK_ID_CURR --- SK_ID_BUREAU --- CREDIT_ACTIVE --- CREDIT_CURRENCY --- DAYS_CREDIT --- CREDIT_DAY_OVERDUE --- DAYS_CREDIT_ENDDATE --- DAYS_ENDDATE_FACT --- AMT_CREDIT_MAX_OVERDUE --- CNT_CREDIT_PROLONG --- AMT_CREDIT_SUM --- AMT_CREDIT_SUM_DEBT --- AMT_CREDIT_SUM_LIMIT --- AMT_CREDIT_SUM_OVERDUE --- CREDIT_TYPE --- DAYS_CREDIT_UPDATE --- AMT_ANNUITY\n"
     ]
    }
   ],
   "source": [
    "bureau = load(\"bureau.csv\")\n",
    "print(\"\\nShape of bureau:\", bureau.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau:\")\n",
    "print(\" --- \".join(bureau.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Lump together values with low counts\n",
    "# CREDIT_CURRENCY\n",
    "cols = [\"currency 3\", \"currency 4\"]\n",
    "bureau.CREDIT_CURRENCY = bureau.CREDIT_CURRENCY.map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# # CREDIT_TYPE\n",
    "# cols = [\"Cash loan (non-earmarked)\", \"Real estate loan\", \"Loan for the purchase of equipment\",\n",
    "#         \"Loan for purchase of shares (margin lending)\", \"Interbank credit\", \"Mobile operator loan\"]\n",
    "# bureau.CREDIT_TYPE = bureau.CREDIT_TYPE.map(lambda x: \"MISC\" if x in cols else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_bal\"></a>\n",
    "\n",
    "# [^](#toc) <u>Bureau Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 624.85 MB\n",
      "Memory usage after optimization is: 338.46 MB\n",
      "Decreased by 45.8%\n",
      "Shape of bureau_balance: (27299925, 3)\n",
      "\n",
      "Columns of bureau_balance:\n",
      "SK_ID_BUREAU --- MONTHS_BALANCE --- STATUS\n"
     ]
    }
   ],
   "source": [
    "bureau_balance = load(\"bureau_balance.csv\")\n",
    "print(\"Shape of bureau_balance:\",  bureau_balance.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau_balance:\")\n",
    "print(\" --- \".join(bureau_balance.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau_bal\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Bureau</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get sum of counts in categorical column\n",
    "merge_df = get_dummies(bureau_balance, [\"STATUS\"])\n",
    "cols = ['STATUS_0', 'STATUS_1', 'STATUS_2', 'STATUS_3', 'STATUS_4', 'STATUS_5', 'STATUS_C', 'STATUS_X']\n",
    "for col in cols:\n",
    "    merge_df[col] = merge_df[col] / (merge_df[\"MONTHS_BALANCE\"] - 1)\n",
    "merge_df = merge_df.drop([\"MONTHS_BALANCE\", \"STATUS\"], axis=1)\n",
    "merge_df = merge_df.groupby(\"SK_ID_BUREAU\").sum().reset_index()\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = bureau_balance.groupby(\"SK_ID_BUREAU\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_BUREAU\").set_index(\"SK_ID_BUREAU\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['bur_bal_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "bureau = bureau.merge(right=merge_df.reset_index(), how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "# Mark missing values\n",
    "bureau[\"no_bureau_bal\"] = bureau[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del bureau_balance, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"prev_app\"></a>\n",
    "\n",
    "# [^](#toc) <u>Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 471.48 MB\n",
      "Memory usage after optimization is: 309.01 MB\n",
      "Decreased by 34.5%\n",
      "Shape of prev_app: (1670214, 37)\n",
      "\n",
      "Columns of prev_app:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NAME_CONTRACT_TYPE --- AMT_ANNUITY --- AMT_APPLICATION --- AMT_CREDIT --- AMT_DOWN_PAYMENT --- AMT_GOODS_PRICE --- WEEKDAY_APPR_PROCESS_START --- HOUR_APPR_PROCESS_START --- FLAG_LAST_APPL_PER_CONTRACT --- NFLAG_LAST_APPL_IN_DAY --- RATE_DOWN_PAYMENT --- RATE_INTEREST_PRIMARY --- RATE_INTEREST_PRIVILEGED --- NAME_CASH_LOAN_PURPOSE --- NAME_CONTRACT_STATUS --- DAYS_DECISION --- NAME_PAYMENT_TYPE --- CODE_REJECT_REASON --- NAME_TYPE_SUITE --- NAME_CLIENT_TYPE --- NAME_GOODS_CATEGORY --- NAME_PORTFOLIO --- NAME_PRODUCT_TYPE --- CHANNEL_TYPE --- SELLERPLACE_AREA --- NAME_SELLER_INDUSTRY --- CNT_PAYMENT --- NAME_YIELD_GROUP --- PRODUCT_COMBINATION --- DAYS_FIRST_DRAWING --- DAYS_FIRST_DUE --- DAYS_LAST_DUE_1ST_VERSION --- DAYS_LAST_DUE --- DAYS_TERMINATION --- NFLAG_INSURED_ON_APPROVAL\n"
     ]
    }
   ],
   "source": [
    "prev_app = load(\"previous_application.csv\")\n",
    "print(\"Shape of prev_app:\",  prev_app.shape)\n",
    "\n",
    "print(\"\\nColumns of prev_app:\")\n",
    "print(\" --- \".join(prev_app.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prev_feat\"></a>\n",
    "\n",
    "### [^](#toc) Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_app[\"ASKED_ACTUAL_RATIO\"] = prev_app[\"AMT_APPLICATION\"] / prev_app[\"AMT_CREDIT\"]\n",
    "prev_app['APPROVED']           = (prev_app['NAME_CONTRACT_STATUS'] == 'Approved').astype(int)\n",
    "prev_app['REFUSED']            = (prev_app['NAME_CONTRACT_STATUS'] == 'Refused').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prev_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fill in values that should be null\n",
    "prev_app['DAYS_FIRST_DRAWING'       ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_FIRST_DUE'           ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_LAST_DUE'            ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_TERMINATION'         ].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "### Lump together values with low counts\n",
    "# NAME_GOODS_CATEGORY\n",
    "prev_app.NAME_GOODS_CATEGORY = prev_app.NAME_GOODS_CATEGORY.map(\n",
    "    lambda x: \"MISC\" if x in [\"Weapon\", \"Insurance\"] else x)\n",
    "\n",
    "# NAME_CASH_LOAN_PURPOSE\n",
    "prev_app.NAME_CASH_LOAN_PURPOSE = prev_app.NAME_CASH_LOAN_PURPOSE.map(\n",
    "    lambda x: \"MISC\" if x in [\"Buying a garage\", \"Misc\"] else x)\n",
    "\n",
    "# Create features\n",
    "prev_app[\"APP_CREDIT_PERC\"] = prev_app['AMT_APPLICATION'] / prev_app['AMT_CREDIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"pos_cash\"></a>\n",
    "\n",
    "# [^](#toc) <u>POS CASH balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 610.43 MB\n",
      "Memory usage after optimization is: 238.45 MB\n",
      "Decreased by 60.9%\n",
      "Shape of pcb: (10001358, 8)\n",
      "\n",
      "Columns of pcb:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- CNT_INSTALMENT --- CNT_INSTALMENT_FUTURE --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "pcb = load(\"POS_CASH_balance.csv\")\n",
    "print(\"Shape of pcb:\",  pcb.shape)\n",
    "\n",
    "print(\"\\nColumns of pcb:\")\n",
    "print(\" --- \".join(pcb.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Outliers\n",
    "pcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_pos_cash\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get Dummies\n",
    "merge_df = pcb[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "merge_df = get_dummies(merge_df, [\"NAME_CONTRACT_STATUS\"])\n",
    "merge_df = merge_df.drop(\"NAME_CONTRACT_STATUS\", axis=1)\n",
    "\n",
    "# Prep for merge\n",
    "count    = merge_df.groupby(\"SK_ID_PREV\").count()\n",
    "merge_df = merge_df.groupby(\"SK_ID_PREV\").sum().reset_index()\n",
    "merge_df[\"N\"] = list(count.iloc[:,0])\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = pcb.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['pos_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_pcb\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del pcb, count, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"install_pay\"></a>\n",
    "\n",
    "# [^](#toc) <u>Installment Payments</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 830.41 MB\n",
      "Memory usage after optimization is: 311.40 MB\n",
      "Decreased by 62.5%\n",
      "Shape of install_pay: (13605401, 8)\n",
      "\n",
      "Columns of install_pay:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NUM_INSTALMENT_VERSION --- NUM_INSTALMENT_NUMBER --- DAYS_INSTALMENT --- DAYS_ENTRY_PAYMENT --- AMT_INSTALMENT --- AMT_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "install_pay = load(\"installments_payments.csv\")\n",
    "print(\"Shape of install_pay:\",  install_pay.shape)\n",
    "\n",
    "print(\"\\nColumns of install_pay:\")\n",
    "print(\" --- \".join(install_pay.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install_feat\"></a>\n",
    "\n",
    "### [^](#toc) Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create new feature\n",
    "install_pay[\"AMT_MISSING\"]  = install_pay[\"AMT_INSTALMENT\"]     - install_pay[\"AMT_PAYMENT\"]\n",
    "install_pay['PAYMENT_PERC'] = install_pay['AMT_PAYMENT']        / install_pay['AMT_INSTALMENT']\n",
    "\n",
    "# Days past due and days before due (no negative values)\n",
    "install_pay['DPD']          = install_pay['DAYS_ENTRY_PAYMENT'] - install_pay['DAYS_INSTALMENT']\n",
    "install_pay['DBD']          = install_pay['DAYS_INSTALMENT']    - install_pay['DAYS_ENTRY_PAYMENT']\n",
    "install_pay['DPD']          = install_pay['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "install_pay['DBD']          = install_pay['DBD'].apply(lambda x: x if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_install_pay\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Amount of values missing in AMT_PAYMENT\n",
    "install_pay[\"temp\"]         = install_pay[\"AMT_PAYMENT\"].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Select important features\n",
    "merge_df = pd.DataFrame({\n",
    "    \"missing_max\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].max(),\n",
    "    \"missing_min\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].min(),\n",
    "    \"payment_max\": install_pay.groupby(\"SK_ID_PREV\")['PAYMENT_PERC'].max(),\n",
    "    \"payment_min\": install_pay.groupby(\"SK_ID_PREV\")['PAYMENT_PERC'].min(),\n",
    "    \n",
    "    \"payment_nan\": install_pay.groupby(\"SK_ID_PREV\")[\"temp\"].sum(),\n",
    "    \"N\":           install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].count(),\n",
    "    \n",
    "    \"unique_ver\":  install_pay.groupby(\"SK_ID_PREV\")[\"NUM_INSTALMENT_VERSION\"].nunique()\n",
    "})\n",
    "\n",
    "# Delete temp column\n",
    "install_pay = install_pay.drop(\"temp\", axis=1)\n",
    "\n",
    "# Select median of everything\n",
    "right = install_pay.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "\n",
    "### Merge the two\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['install_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_install\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del install_pay, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"credit\"></a>\n",
    "\n",
    "# [^](#toc) <u>Credit Card Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 673.88 MB\n",
      "Memory usage after optimization is: 289.33 MB\n",
      "Decreased by 57.1%\n",
      "Shape of credit_card: (3840312, 23)\n",
      "\n",
      "Columns of credit_card:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- AMT_BALANCE --- AMT_CREDIT_LIMIT_ACTUAL --- AMT_DRAWINGS_ATM_CURRENT --- AMT_DRAWINGS_CURRENT --- AMT_DRAWINGS_OTHER_CURRENT --- AMT_DRAWINGS_POS_CURRENT --- AMT_INST_MIN_REGULARITY --- AMT_PAYMENT_CURRENT --- AMT_PAYMENT_TOTAL_CURRENT --- AMT_RECEIVABLE_PRINCIPAL --- AMT_RECIVABLE --- AMT_TOTAL_RECEIVABLE --- CNT_DRAWINGS_ATM_CURRENT --- CNT_DRAWINGS_CURRENT --- CNT_DRAWINGS_OTHER_CURRENT --- CNT_DRAWINGS_POS_CURRENT --- CNT_INSTALMENT_MATURE_CUM --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "credit_card = load(\"credit_card_balance.csv\")\n",
    "print(\"Shape of credit_card:\",  credit_card.shape)\n",
    "\n",
    "print(\"\\nColumns of credit_card:\")\n",
    "print(\" --- \".join(credit_card.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"credit_process\"></a>\n",
    "\n",
    "### [^](#toc) <u>Data Processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gets indices with outlier values\n",
    "temp = credit_card[credit_card.NAME_CONTRACT_STATUS.isin([\"Refused\", \"Approved\"])].index\n",
    "\n",
    "# Drops outlier values\n",
    "credit_card = credit_card.drop(temp, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit_card[\"AVG_DRAWINGS_ATM_CURRENT\"]   = (credit_card[\"AMT_DRAWINGS_ATM_CURRENT\"] /\n",
    "                                             credit_card[\"CNT_DRAWINGS_ATM_CURRENT\"])\n",
    "\n",
    "credit_card[\"AVG_DRAWINGS_CURRENT\"]       = (credit_card[\"AMT_DRAWINGS_CURRENT\"] /\n",
    "                                             credit_card[\"CNT_DRAWINGS_CURRENT\"])\n",
    "\n",
    "credit_card[\"AVG_DRAWINGS_OTHER_CURRENT\"] = (credit_card[\"AMT_DRAWINGS_OTHER_CURRENT\"] /\n",
    "                                             credit_card[\"CNT_DRAWINGS_OTHER_CURRENT\"])\n",
    "\n",
    "credit_card[\"AVG_DRAWINGS_POS_CURRENT\"]   = (credit_card[\"AMT_DRAWINGS_POS_CURRENT\"] /\n",
    "                                             credit_card[\"CNT_DRAWINGS_POS_CURRENT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_credit\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create features\n",
    "merge_df = pd.DataFrame({\n",
    "    \"AMT_BALANCE\": credit_card.groupby(\"SK_ID_PREV\").AMT_BALANCE.mean(),\n",
    "    \"SK_DPD\":      credit_card.groupby(\"SK_ID_PREV\").SK_DPD.max(),\n",
    "    \"SK_DPD_DEF\":  credit_card.groupby(\"SK_ID_PREV\").SK_DPD_DEF.max(),\n",
    "    \"N\":           credit_card.groupby(\"SK_ID_PREV\").count().iloc[:,0]\n",
    "})\n",
    "\n",
    "### Categorical column\n",
    "temp = get_dummies(credit_card, [\"NAME_CONTRACT_STATUS\"])\n",
    "cols = ['NAME_CONTRACT_STATUS_Active',\n",
    "       'NAME_CONTRACT_STATUS_Completed', 'NAME_CONTRACT_STATUS_Demand',\n",
    "       'NAME_CONTRACT_STATUS_Sent proposal', 'NAME_CONTRACT_STATUS_Signed']\n",
    "for col in cols:\n",
    "    temp[col] = temp[col] / (temp[\"MONTHS_BALANCE\"] - 1)\n",
    "cols.extend([\"SK_ID_PREV\"])\n",
    "temp = temp[cols]\n",
    "temp = temp.groupby(\"SK_ID_PREV\").sum()\n",
    "\n",
    "# Merge categorical and numerical df\n",
    "merge_df = temp.join(merge_df)\n",
    "\n",
    "### Add the rest of the columns\n",
    "right = credit_card.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['credit_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_credit\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del credit_card, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"misc\"></a>\n",
    "\n",
    "# [^](#toc) <u>Miscellaneous clean up</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Drop unneeded ID columns\n",
    "prev_app = prev_app.drop(\"SK_ID_PREV\", axis=1)\n",
    "bureau   = bureau.drop(\"SK_ID_BUREAU\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"final_merge\"></a>\n",
    "\n",
    "# [^](#toc) <u>Final Data Prep</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 286.23 MB\n",
      "Memory usage after optimization is: 92.38 MB\n",
      "Decreased by 67.7%\n",
      "Memory usage of dataframe is 45.00 MB\n",
      "Memory usage after optimization is: 14.60 MB\n",
      "Decreased by 67.6%\n",
      "Shape of train: (307511, 122)\n",
      "Shape of test: (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train = load(\"train.csv\")\n",
    "test  = load(\"test.csv\")\n",
    "\n",
    "print(\"Shape of train:\", train.shape)\n",
    "print(\"Shape of test:\",  test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into predictors, target, and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train.TARGET\n",
    "train_x = train.drop([\"TARGET\"], axis=1)\n",
    "\n",
    "test_id = test.SK_ID_CURR\n",
    "test_x  = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full    = pd.concat([train_x, test_x])\n",
    "train_N = len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final_process\"></a>\n",
    "\n",
    "### [^](#toc) <u>Data Processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Replace maxed values with NaN\n",
    "full['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "### Fill in outlier values\n",
    "full[\"CODE_GENDER\"]        = full[\"CODE_GENDER\"].map(lambda x: \"F\" if x == \"XNA\" else x)\n",
    "full[\"NAME_FAMILY_STATUS\"] = full[\"NAME_FAMILY_STATUS\"].map(lambda x: \"Married\" if x == \"Unknown\" else x)\n",
    "\n",
    "# NAME_INCOME_TYPE\n",
    "cols = [\"Unemployed\", \"Student\", \"Businessman\", \"Maternity leave\"]\n",
    "full[\"NAME_INCOME_TYPE\"] = full[\"NAME_INCOME_TYPE\"].map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# ORGANIZATION_TYPE\n",
    "cols = [\"Trade: type 4\", \"Trade: type 5\"]\n",
    "full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Trade\" if x in cols else x)\n",
    "cols = [\"Industry: type 13\", \"Industry: type 8\"]\n",
    "full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Industry\" if x in cols else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_cat\"></a>\n",
    "\n",
    "### [^](#toc) Categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Get dummies\n",
    "# cols  = [\"WALLSMATERIAL_MODE\", \"NAME_TYPE_SUITE\", \"NAME_INCOME_TYPE\", \"NAME_FAMILY_STATUS\",\n",
    "#                \"NAME_HOUSING_TYPE\", \"OCCUPATION_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"ORGANIZATION_TYPE\",\n",
    "#                \"FONDKAPREMONT_MODE\", \"NAME_EDUCATION_TYPE\"]\n",
    "# full = get_dummies(full, cols)\n",
    "# full = full.drop(cols, axis=1)\n",
    "\n",
    "# ### Factorize the dataframe\n",
    "# cols = [\"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"FLAG_OWN_CAR\",\n",
    "#                \"FLAG_OWN_REALTY\", \"HOUSETYPE_MODE\", \"EMERGENCYSTATE_MODE\"]\n",
    "# full = factorize_df(full, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_feat\"></a>\n",
    "\n",
    "### [^](#toc) Create Features\n",
    "\n",
    "FIXME:\n",
    "\n",
    "Monthly debt payments, alimony,living costs divided by monthy gross income\n",
    "\n",
    "Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 167.52it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 236.27it/s]\n"
     ]
    }
   ],
   "source": [
    "full['DAYS_EMPLOYED_PERC']   = full['DAYS_EMPLOYED']    / full['DAYS_BIRTH']\n",
    "full['INCOME_CREDIT_PERC']   = full['AMT_INCOME_TOTAL'] / full['AMT_CREDIT']\n",
    "full['INCOME_PER_PERSON']    = full['AMT_INCOME_TOTAL'] / full['CNT_FAM_MEMBERS']\n",
    "full['CHILDREN_RATIO']       = full['CNT_CHILDREN']     / full['CNT_FAM_MEMBERS']\n",
    "\n",
    "full['NUM_PROPERTY']         = full['FLAG_OWN_CAR']     + full['FLAG_OWN_REALTY']\n",
    "\n",
    "full['ANNUITY_INCOME_PERC']  = full['AMT_ANNUITY']      / full['AMT_INCOME_TOTAL']\n",
    "full[\"PAYMENT_RATE\"]         = full['AMT_ANNUITY']      / full['AMT_CREDIT']\n",
    "full['LOAN_INCOME_RATIO']    = full['AMT_CREDIT']       / full['AMT_INCOME_TOTAL']\n",
    "full['CONSUMER_GOODS_RATIO'] = full['AMT_CREDIT']       / full['AMT_GOODS_PRICE']\n",
    "full['ANNUITY LENGTH']       = full['AMT_CREDIT']       / full['AMT_ANNUITY']\n",
    "full['ANN_EMPLOYED_RATIO']   = full['ANNUITY LENGTH'] / full['DAYS_EMPLOYED']\n",
    "\n",
    "full[\"YEARS_OLD\"]            = full.DAYS_BIRTH.map(lambda x: -int(x / 365))\n",
    "\n",
    "### Create feature marking number of enquires\n",
    "full[\"NUM_ENQUIRIES\"] = np.zeros(len(full))\n",
    "for enquiry in tqdm((\"AMT_REQ_CREDIT_BUREAU_HOUR\", \"AMT_REQ_CREDIT_BUREAU_DAY\",\n",
    "                     \"AMT_REQ_CREDIT_BUREAU_WEEK\", \"AMT_REQ_CREDIT_BUREAU_MON\",\n",
    "                     \"AMT_REQ_CREDIT_BUREAU_QRT\",  \"AMT_REQ_CREDIT_BUREAU_YEAR\")):\n",
    "    full[\"NUM_ENQUIRIES\"] += full[enquiry]\n",
    "    \n",
    "### Create feature marking number of discrepancies\n",
    "full[\"DISCREPANCIES\"] = np.zeros(len(full))\n",
    "for discrepancy in (\"REG_REGION_NOT_LIVE_REGION\", \"REG_REGION_NOT_WORK_REGION\",\n",
    "                    \"LIVE_REGION_NOT_WORK_REGION\", \"REG_CITY_NOT_LIVE_CITY\",\n",
    "                    \"REG_CITY_NOT_WORK_CITY\", \"LIVE_CITY_NOT_WORK_CITY\"):\n",
    "    full[\"DISCREPANCIES\"] += full[discrepancy]\n",
    "    \n",
    "### Create feature marking number of info provided\n",
    "full[\"PROVIDE_INFO\"] = np.zeros(len(full))\n",
    "for info in (\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\", \"FLAG_WORK_PHONE\",\n",
    "             \"FLAG_PHONE\", \"FLAG_EMAIL\"):\n",
    "    full[\"PROVIDE_INFO\"] += full[info]\n",
    "\n",
    "### Create feature marking number of flags\n",
    "full[\"NUM_FLAGS\"] = np.zeros(len(full))\n",
    "for flag in tqdm(('FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4',\n",
    "                 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7',\n",
    "                 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10',\n",
    "                 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13',\n",
    "                 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16',\n",
    "                 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19',\n",
    "                 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21')):\n",
    "    full[\"NUM_FLAGS\"] += full[flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_prev\"></a>\n",
    "\n",
    "### [^](#toc) Merge Previous Application with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected median of numerical columns\n",
      "Selected max of numerical columns\n",
      "Merged median and max\n",
      "Selected min of numerical columns\n",
      "Merged min with median and max\n",
      "Selected categorical columns\n",
      "Merged categorical and numerical\n",
      "Merged into full\n",
      "Marked missing values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2786"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = [\n",
    "        \"NAME_CONTRACT_TYPE\", \"WEEKDAY_APPR_PROCESS_START\",\n",
    "        \"FLAG_LAST_APPL_PER_CONTRACT\", \"NAME_CASH_LOAN_PURPOSE\",\n",
    "        \"NAME_CONTRACT_STATUS\", \"NAME_PAYMENT_TYPE\",\n",
    "        \"CODE_REJECT_REASON\", \"NAME_TYPE_SUITE\", \"NAME_CLIENT_TYPE\",\n",
    "        \"NAME_GOODS_CATEGORY\", \"NAME_PORTFOLIO\", \"NAME_PRODUCT_TYPE\",\n",
    "        \"CHANNEL_TYPE\", \"NAME_SELLER_INDUSTRY\", \"NAME_YIELD_GROUP\",\n",
    "        \"PRODUCT_COMBINATION\", \"SK_ID_CURR\"]\n",
    "min_max_cols = ['AMT_ANNUITY', 'AMT_APPLICATION', 'AMT_CREDIT',\n",
    "                 'AMT_DOWN_PAYMENT', 'AMT_GOODS_PRICE', 'HOUR_APPR_PROCESS_START',\n",
    "                 'NFLAG_LAST_APPL_IN_DAY', 'RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY',\n",
    "                 'RATE_INTEREST_PRIVILEGED', 'DAYS_DECISION', 'SELLERPLACE_AREA',\n",
    "                 'CNT_PAYMENT', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE',\n",
    "                 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION',\n",
    "                 'NFLAG_INSURED_ON_APPROVAL', 'APP_CREDIT_PERC', \"SK_ID_CURR\"]\n",
    "num_cols = [col for col in prev_app.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "### numerical columns - median\n",
    "merge_df         = prev_app[num_cols].groupby('SK_ID_CURR').median()\n",
    "merge_df.columns = [\"MED_\" + col for col in merge_df.columns]\n",
    "print(\"Selected median of numerical columns\")\n",
    "\n",
    "### numerical columns - max\n",
    "right         = prev_app[min_max_cols].groupby(\"SK_ID_CURR\").max()\n",
    "right.columns = [\"MAX_\" + col for col in right.columns]\n",
    "print(\"Selected max of numerical columns\")\n",
    "\n",
    "### Merge median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged median and max\")\n",
    "\n",
    "### numerical columns - min\n",
    "right = prev_app[min_max_cols].groupby(\"SK_ID_CURR\").min()\n",
    "right.columns = [\"MIN_\" + col for col in right.columns]\n",
    "print(\"Selected min of numerical columns\")\n",
    "\n",
    "### Merge min with median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged min with median and max\")\n",
    "\n",
    "### Categorical columns\n",
    "right = prev_app[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "print(\"Selected categorical columns\")\n",
    "\n",
    "### Merge categorical and numerical\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged categorical and numerical\")\n",
    "\n",
    "### Prefix column names\n",
    "merge_df[\"N\"]    = prev_app.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "merged_cols      = ['p_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')\n",
    "print(\"Merged into full\")\n",
    "\n",
    "# Mark missing values\n",
    "full[\"no_prev_app\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "print(\"Marked missing values\")\n",
    "\n",
    "### Delete old variables\n",
    "del merge_df, merged_cols, right, cat_cols, num_cols\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau\"></a>\n",
    "\n",
    "### [^](#toc) Merge Bureau with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected median of numerical columns\n",
      "Selected max of numerical columns\n",
      "Merged median and max\n",
      "Selected min of numerical columns\n",
      "Merged min with median and max\n",
      "Selected categorical columns\n",
      "Merged categorical and numerical\n",
      "Merged into full\n",
      "Marked missing values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE', 'SK_ID_CURR']\n",
    "num_cols = [col for col in bureau.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "### Numeric columns - median\n",
    "merge_df         = bureau[num_cols].groupby('SK_ID_CURR').median()\n",
    "merge_df.columns = [\"MED_\" + col for col in merge_df.columns]\n",
    "print(\"Selected median of numerical columns\")\n",
    "\n",
    "### Numeric columns - max\n",
    "right         = bureau[num_cols].groupby(\"SK_ID_CURR\").max()\n",
    "right.columns = [\"MAX_\" + col for col in right.columns]\n",
    "print(\"Selected max of numerical columns\")\n",
    "\n",
    "### Merge median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged median and max\")\n",
    "\n",
    "### Numeric columns - min\n",
    "right = bureau[num_cols].groupby(\"SK_ID_CURR\").min()\n",
    "right.columns = [\"MIN_\" + col for col in right.columns]\n",
    "print(\"Selected min of numerical columns\")\n",
    "\n",
    "### Merge min with median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged min with median and max\")\n",
    "\n",
    "### Categorical columns\n",
    "right = bureau[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum()\n",
    "right[\"NUM_UNQ_CREDIT\"] = bureau.groupby(\"SK_ID_CURR\")[\"CREDIT_TYPE\"].nunique()\n",
    "print(\"Selected categorical columns\")\n",
    "\n",
    "### Merge categorical and numeric\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged categorical and numerical\")\n",
    "\n",
    "### Prefix column names\n",
    "merge_df[\"N\"] = bureau.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "merged_cols      = ['b_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')\n",
    "print(\"Merged into full\")\n",
    "\n",
    "# Mark missing values\n",
    "full[\"no_bureau\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "print(\"Marked missing values\")\n",
    "\n",
    "### Delete old variables\n",
    "del merge_df, merged_cols, right, cat_cols, num_cols\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = full.drop(\"SK_ID_CURR\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorize and save Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = [col for col in full.columns if full[col].dtype == object]\n",
    "full     = factorize_df(full, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split full back into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE</th>\n",
       "      <th>...</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for the purchase of equipment</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for working capital replenishment</th>\n",
       "      <th>b_CREDIT_TYPE_Microloan</th>\n",
       "      <th>b_CREDIT_TYPE_Mobile operator loan</th>\n",
       "      <th>b_CREDIT_TYPE_Mortgage</th>\n",
       "      <th>b_CREDIT_TYPE_Real estate loan</th>\n",
       "      <th>b_CREDIT_TYPE_Unknown type of loan</th>\n",
       "      <th>b_NUM_UNQ_CREDIT</th>\n",
       "      <th>b_N</th>\n",
       "      <th>no_bureau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0                   0            0             0                0   \n",
       "1                   0            1             0                1   \n",
       "2                   1            0             1                0   \n",
       "3                   0            1             0                0   \n",
       "4                   0            0             0                0   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "0             0          202500.0    406597.5      24700.5         351000.0   \n",
       "1             0          270000.0   1293502.5      35698.5        1129500.0   \n",
       "2             0           67500.0    135000.0       6750.0         135000.0   \n",
       "3             0          135000.0    312682.5      29686.5         297000.0   \n",
       "4             0          121500.0    513000.0      21865.5         513000.0   \n",
       "\n",
       "   NAME_TYPE_SUITE    ...      \\\n",
       "0                0    ...       \n",
       "1                1    ...       \n",
       "2                0    ...       \n",
       "3                0    ...       \n",
       "4                0    ...       \n",
       "\n",
       "   b_CREDIT_TYPE_Loan for the purchase of equipment  \\\n",
       "0                                               0.0   \n",
       "1                                               0.0   \n",
       "2                                               0.0   \n",
       "3                                               NaN   \n",
       "4                                               0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Loan for working capital replenishment  \\\n",
       "0                                                0.0      \n",
       "1                                                0.0      \n",
       "2                                                0.0      \n",
       "3                                                NaN      \n",
       "4                                                0.0      \n",
       "\n",
       "   b_CREDIT_TYPE_Microloan  b_CREDIT_TYPE_Mobile operator loan  \\\n",
       "0                      0.0                                 0.0   \n",
       "1                      0.0                                 0.0   \n",
       "2                      0.0                                 0.0   \n",
       "3                      NaN                                 NaN   \n",
       "4                      0.0                                 0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Mortgage  b_CREDIT_TYPE_Real estate loan  \\\n",
       "0                     0.0                             0.0   \n",
       "1                     0.0                             0.0   \n",
       "2                     0.0                             0.0   \n",
       "3                     NaN                             NaN   \n",
       "4                     0.0                             0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Unknown type of loan  b_NUM_UNQ_CREDIT  b_N  no_bureau  \n",
       "0                                 0.0               2.0  8.0          0  \n",
       "1                                 0.0               2.0  4.0          0  \n",
       "2                                 0.0               1.0  2.0          0  \n",
       "3                                 NaN               NaN  NaN          1  \n",
       "4                                 0.0               1.0  1.0          0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = full[:train_N]\n",
    "test_x  = full[train_N:]\n",
    "\n",
    "### Processed data look\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# [^](#toc) <u>Modeling</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics         import roc_auc_score, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm                import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat_reduction\"></a>\n",
    "\n",
    "### [^](#toc) Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[200]\tvalid_0's auc: 0.771043\n",
      "[400]\tvalid_0's auc: 0.782521\n",
      "[600]\tvalid_0's auc: 0.785934\n",
      "[800]\tvalid_0's auc: 0.787306\n",
      "[1000]\tvalid_0's auc: 0.787954\n",
      "[1200]\tvalid_0's auc: 0.788289\n",
      "Early stopping, best iteration is:\n",
      "[1230]\tvalid_0's auc: 0.788337\n",
      "Training took 525 seconds\n"
     ]
    }
   ],
   "source": [
    "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)\n",
    "\n",
    "lgb_train = lgb.Dataset(data=training_x, label=training_y, categorical_feature=cat_cols)\n",
    "lgb_eval  = lgb.Dataset(data=val_x, label=val_y, categorical_feature=cat_cols)\n",
    "\n",
    "# try feature_fraction\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.03, 'num_leaves': 55, 'num_iteration': 2000, 'verbose': 0 ,\n",
    "          'subsample':.9, 'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "          'min_split_gain':.05, 'min_child_weight':1, \"min_data_in_leaf\": 40,\n",
    "          \"feature_fraction\":0.5}\n",
    "\n",
    "start = time.time()\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"important_feats\"></a>\n",
    "\n",
    "### [^](#toc) Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_FEATS = 350\n",
    "\n",
    "feats = sorted(list(zip(model.feature_importance(), train_x.columns)))\n",
    "feats = list(list(zip(*feats[-NUM_FEATS:]))[1])\n",
    "\n",
    "cat_cols_test = list(set(cat_cols).intersection(set(feats)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"param_tuning\"></a>\n",
    "\n",
    "### [^](#toc) Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD \n",
    "\n",
    "lambda tuning (L2 regularization)\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "lambda = 10, num_iter = 2500\n",
    "- [2500] train: 0.871737, test: 0.78456\n",
    "\n",
    "lambda = 20, num_iter = 5000\n",
    "- [3270] train: 0.882077, test: 0.785751\n",
    "\n",
    "lambda = 40, num_iter = 3000\n",
    "- [3000] train: 0.868533, test: 0.78583\n",
    "\n",
    "##### Implemented random_state=17\n",
    "\n",
    "lambda = 80, num_iter = 3000\n",
    "- [3000] train: 0.859416, test: 0.785235\n",
    "\n",
    "lambda = 160, num_iter = 4000\n",
    "- [3789] train: 0.86189, test: 0.785736\n",
    "\n",
    "lambda = 0.1, num_iter = 4000\n",
    "- [2603] train: 0.895891, test: 0.784205\n",
    "\n",
    "##### change max_depth from 6 to 7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Other variables\n",
    "--------------\n",
    "\n",
    "learning_rate = 0.01,\n",
    "num_leaves = 48,\n",
    "colsample_bytree = 0.8,\n",
    "subsample = 0.9,\n",
    "max_depth = 6,\n",
    "reg_alpha = 0.1,\n",
    "min_split_gain = 0.01,\n",
    "min_child_weight = 1,\n",
    "\n",
    "</div>\n",
    "\n",
    "max_depth tuning\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "max_depth = 6, num_iter = 3000\n",
    "- [3000] train: 0.859416, test: 0.785235\n",
    "\n",
    "max_depth = 7, num_iter = 4000\n",
    "- [3415] train: 0.878744, test: 0.786247\n",
    "\n",
    "max_depth = 8, num_iter = 4000\n",
    "- [2965] train: 0.875784, test: 0.786024\n",
    "\n",
    "##### Change reg_alpha from 0.1 to 40\n",
    "\n",
    "max_depth = 8, num_iter = 4000\n",
    "- [3420] train: 0.858222, test: 0.785642\n",
    "\n",
    "max_depth = 9, num_iter = 4000\n",
    "- [] train: , test: \n",
    "\n",
    "max_depth = 10, num_iter = 4000\n",
    "- [] train: , test: \n",
    "\n",
    "Other variables\n",
    "--------------\n",
    "\n",
    "learning_rate = 0.01,\n",
    "num_leaves = 48,\n",
    "colsample_bytree = 0.8,\n",
    "subsample = 0.9,\n",
    "reg_alpha = 0.1,\n",
    "reg_lambda = 80,\n",
    "min_split_gain = 0.01,\n",
    "min_child_weight = 1,\n",
    "random_state=17\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16062018 - Reduce Overfitting 1\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "Starting params\n",
    "\n",
    "'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "'subsample':.9, 'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "'min_split_gain':.05, 'min_child_weight':1, \"min_data_in_leaf\": 40,\n",
    "\"bagging_freq\": 4, \"bagging_fraction\":0.5\n",
    "\n",
    "___Initial___\n",
    "\n",
    " - Validation: 0.786\n",
    " - Test:       0.779\n",
    "\n",
    "Remove bagging_freq (=4), bagging_fraction (=0.5), add feature_fraction (=0.5).\n",
    "\n",
    " - Validation: 0.789602\n",
    " - Test:       0.785184954089\n",
    " \n",
    "2nd run\n",
    "\n",
    " - Validation: 0.789004\n",
    " - Test:       0.784571125086\n",
    " \n",
    "Change min_data_in_leaf (40 --> 60)\n",
    "\n",
    " - Validation: 0.789002\n",
    " - Test:       0.784891310112\n",
    " \n",
    "Change min_data_in_leaf (60 --> 20)\n",
    "\n",
    " - Validation: 0.788626\n",
    " - Test:       0.78465452505\n",
    " \n",
    "Change min_data_in_leaf (20 --> 40) and feature_fraction (0.5 --> 0.6)\n",
    "\n",
    " - Validation: 0.788533\n",
    " - Test:       0.784679323774\n",
    " \n",
    "Change max_depth (7 --> 6) and feature_fraction (0.6 --> 0.5)\n",
    "\n",
    " - Validation: 0.788849\n",
    " - Test:       0.784880849041\n",
    " \n",
    "Change learning_rate (0.01 --> 0.03) and max_depth (6 --> 7)\n",
    "\n",
    " - Validation: 0.788195\n",
    " - Test:       0.784589015086\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16062018 - Reduce Overfitting 2\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "Starting params\n",
    "\n",
    "'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "'subsample':0.9, 'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "'min_split_gain':0.05, 'min_child_weight':1, \"min_data_in_leaf\": 40,\n",
    "\"feature_fraction\":0.5\n",
    "\n",
    "___Initial___\n",
    "\n",
    " - Validation: 0.788926\n",
    " - Test:       0.784734949879\n",
    " \n",
    "Change subsample (0.9 --> 0.7)\n",
    "\n",
    " - Validation: 0.788926\n",
    " - Test:       0.784734949879\n",
    " \n",
    "Change min_child_weight (1 --> 4) and remove subsample\n",
    "\n",
    " - Validation: 0.78953\n",
    " - Test:       0.785232093044\n",
    " \n",
    "Change min_split_gain (0.05 --> 0.1)\n",
    " \n",
    " - Validation: 0.789338\n",
    " - Test:       0.784930068808\n",
    " \n",
    "Change min_child_weight (4 --> 10) and min_split_gain (0.1 --> 0.05)\n",
    " \n",
    " - Validation: 0.788998\n",
    " - Test:       0.784947160688\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.73649\n",
      "[200]\tvalid_0's auc: 0.749785\n",
      "[300]\tvalid_0's auc: 0.759906\n",
      "[400]\tvalid_0's auc: 0.766861\n",
      "[500]\tvalid_0's auc: 0.772775\n",
      "[600]\tvalid_0's auc: 0.777293\n",
      "[700]\tvalid_0's auc: 0.78059\n",
      "[800]\tvalid_0's auc: 0.782888\n",
      "[900]\tvalid_0's auc: 0.784692\n",
      "[1000]\tvalid_0's auc: 0.78611\n",
      "[1100]\tvalid_0's auc: 0.787333\n",
      "[1200]\tvalid_0's auc: 0.788266\n",
      "[1300]\tvalid_0's auc: 0.788893\n",
      "[1400]\tvalid_0's auc: 0.789397\n",
      "[1500]\tvalid_0's auc: 0.789768\n",
      "[1600]\tvalid_0's auc: 0.790086\n",
      "[1700]\tvalid_0's auc: 0.790485\n",
      "[1800]\tvalid_0's auc: 0.790804\n",
      "[1900]\tvalid_0's auc: 0.791184\n",
      "[2000]\tvalid_0's auc: 0.791417\n",
      "[2100]\tvalid_0's auc: 0.791614\n",
      "[2200]\tvalid_0's auc: 0.791859\n",
      "[2300]\tvalid_0's auc: 0.792074\n",
      "[2400]\tvalid_0's auc: 0.792266\n",
      "[2500]\tvalid_0's auc: 0.792452\n",
      "[2600]\tvalid_0's auc: 0.792509\n",
      "[2700]\tvalid_0's auc: 0.792692\n",
      "[2800]\tvalid_0's auc: 0.792746\n",
      "[2900]\tvalid_0's auc: 0.792786\n",
      "[3000]\tvalid_0's auc: 0.792803\n",
      "[3100]\tvalid_0's auc: 0.792833\n",
      "[3200]\tvalid_0's auc: 0.792864\n",
      "[3300]\tvalid_0's auc: 0.792856\n",
      "Early stopping, best iteration is:\n",
      "[3206]\tvalid_0's auc: 0.792885\n",
      "Training took 770 seconds\n",
      "Testing score: 0.787572244635\n"
     ]
    }
   ],
   "source": [
    "training_x, testing_x, training_y, testing_y = train_test_split(train_x[feats], train_y, test_size=0.2, random_state=17)\n",
    "training_x, val_x, training_y, val_y = train_test_split(training_x, training_y, test_size=0.25, random_state=17)\n",
    "\n",
    "lgb_train = lgb.Dataset(data=training_x,\n",
    "                        label=training_y,\n",
    "                        categorical_feature=cat_cols_test)\n",
    "lgb_eval  = lgb.Dataset(data=val_x,\n",
    "                        label=val_y,\n",
    "                        categorical_feature=cat_cols_test)\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "          'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "          'min_split_gain':0.05, 'min_child_weight':4, \"min_data_in_leaf\": 40,\n",
    "          \"feature_fraction\":0.5}\n",
    "\n",
    "start = time.time()\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=100)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))\n",
    "\n",
    "prediction = model.predict(testing_x)\n",
    "\n",
    "score = roc_auc_score(testing_y, prediction)\n",
    "\n",
    "print(\"Testing score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cv\"></a>\n",
    "\n",
    "### [^](#toc) CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's auc: 0.736707 + 0.00123144\n",
      "[200]\tcv_agg's auc: 0.74894 + 0.00167459\n",
      "[300]\tcv_agg's auc: 0.758434 + 0.00156598\n",
      "[400]\tcv_agg's auc: 0.765252 + 0.00167507\n",
      "[500]\tcv_agg's auc: 0.770688 + 0.00169804\n",
      "[600]\tcv_agg's auc: 0.77479 + 0.00168653\n",
      "[700]\tcv_agg's auc: 0.77799 + 0.00164319\n",
      "[800]\tcv_agg's auc: 0.780447 + 0.00162795\n",
      "[900]\tcv_agg's auc: 0.782327 + 0.00156226\n",
      "[1000]\tcv_agg's auc: 0.783797 + 0.00157045\n",
      "[1100]\tcv_agg's auc: 0.784921 + 0.00164287\n",
      "[1200]\tcv_agg's auc: 0.785871 + 0.00171086\n",
      "[1300]\tcv_agg's auc: 0.7867 + 0.0017237\n",
      "[1400]\tcv_agg's auc: 0.78733 + 0.00173943\n",
      "[1500]\tcv_agg's auc: 0.787884 + 0.00175024\n",
      "[1600]\tcv_agg's auc: 0.788369 + 0.00176239\n",
      "[1700]\tcv_agg's auc: 0.788777 + 0.00174917\n",
      "[1800]\tcv_agg's auc: 0.789142 + 0.00174201\n",
      "[1900]\tcv_agg's auc: 0.789489 + 0.00178217\n",
      "[2000]\tcv_agg's auc: 0.789801 + 0.00177117\n",
      "[2100]\tcv_agg's auc: 0.790046 + 0.00174049\n",
      "[2200]\tcv_agg's auc: 0.790283 + 0.00169146\n",
      "[2300]\tcv_agg's auc: 0.790533 + 0.00166305\n",
      "[2400]\tcv_agg's auc: 0.790747 + 0.00165028\n",
      "[2500]\tcv_agg's auc: 0.790919 + 0.0016406\n",
      "[2600]\tcv_agg's auc: 0.791079 + 0.00159219\n",
      "[2700]\tcv_agg's auc: 0.791222 + 0.00156227\n",
      "[2800]\tcv_agg's auc: 0.791331 + 0.00154674\n",
      "[2900]\tcv_agg's auc: 0.791455 + 0.00154585\n",
      "[3000]\tcv_agg's auc: 0.791551 + 0.00153485\n",
      "[3100]\tcv_agg's auc: 0.791649 + 0.0015066\n",
      "[3200]\tcv_agg's auc: 0.791749 + 0.0014854\n",
      "[3300]\tcv_agg's auc: 0.791849 + 0.00145762\n",
      "[3400]\tcv_agg's auc: 0.791916 + 0.001454\n",
      "[3500]\tcv_agg's auc: 0.79199 + 0.00143323\n",
      "[3600]\tcv_agg's auc: 0.792068 + 0.00141393\n",
      "[3700]\tcv_agg's auc: 0.792129 + 0.00140012\n",
      "[3800]\tcv_agg's auc: 0.792151 + 0.00138585\n",
      "[3900]\tcv_agg's auc: 0.792165 + 0.00136162\n",
      "[4000]\tcv_agg's auc: 0.792171 + 0.00135745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79217123512612342"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(train_x, train_y, usecols, params, dropcols=[]):  \n",
    "    dtrain = lgb.Dataset(train_x[usecols].drop(dropcols, axis=1),\n",
    "                         train_y,\n",
    "                         categorical_feature=cat_cols_test)\n",
    "    \n",
    "    eval = lgb.cv(params,\n",
    "             dtrain,\n",
    "             nfold=5,\n",
    "             stratified=True,\n",
    "             num_boost_round=20000,\n",
    "             early_stopping_rounds=200,\n",
    "             verbose_eval=100,\n",
    "             seed = 5,\n",
    "             show_stdv=True)\n",
    "    return max(eval['auc-mean'])\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "          'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "          'min_split_gain':0.05, 'min_child_weight':4, \"min_data_in_leaf\": 40,\n",
    "          \"feature_fraction\":0.5,}\n",
    "    \n",
    "get_score(train_x, train_y, feats, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"final\"></a>\n",
    "\n",
    "# [^](#toc) <u>Final submission</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.730671\n",
      "[200]\tvalid_0's auc: 0.742626\n",
      "[300]\tvalid_0's auc: 0.752882\n",
      "[400]\tvalid_0's auc: 0.760501\n",
      "[500]\tvalid_0's auc: 0.766525\n",
      "[600]\tvalid_0's auc: 0.771075\n",
      "[700]\tvalid_0's auc: 0.774528\n",
      "[800]\tvalid_0's auc: 0.777253\n",
      "[900]\tvalid_0's auc: 0.77926\n",
      "[1000]\tvalid_0's auc: 0.780819\n",
      "[1100]\tvalid_0's auc: 0.782038\n",
      "[1200]\tvalid_0's auc: 0.782997\n",
      "[1300]\tvalid_0's auc: 0.783774\n",
      "[1400]\tvalid_0's auc: 0.784369\n",
      "[1500]\tvalid_0's auc: 0.784874\n",
      "[1600]\tvalid_0's auc: 0.785358\n",
      "[1700]\tvalid_0's auc: 0.785749\n",
      "[1800]\tvalid_0's auc: 0.786072\n",
      "[1900]\tvalid_0's auc: 0.786416\n",
      "[2000]\tvalid_0's auc: 0.78668\n",
      "[2100]\tvalid_0's auc: 0.786994\n",
      "[2200]\tvalid_0's auc: 0.787245\n",
      "[2300]\tvalid_0's auc: 0.787497\n",
      "[2400]\tvalid_0's auc: 0.787732\n",
      "[2500]\tvalid_0's auc: 0.787966\n",
      "[2600]\tvalid_0's auc: 0.788165\n",
      "[2700]\tvalid_0's auc: 0.788348\n",
      "[2800]\tvalid_0's auc: 0.788436\n",
      "[2900]\tvalid_0's auc: 0.78847\n",
      "[3000]\tvalid_0's auc: 0.788528\n",
      "[3100]\tvalid_0's auc: 0.788577\n",
      "[3200]\tvalid_0's auc: 0.788664\n",
      "[3300]\tvalid_0's auc: 0.788692\n",
      "[3400]\tvalid_0's auc: 0.788767\n",
      "[3500]\tvalid_0's auc: 0.788869\n",
      "[3600]\tvalid_0's auc: 0.788922\n",
      "[3700]\tvalid_0's auc: 0.788937\n",
      "[3800]\tvalid_0's auc: 0.789012\n",
      "[3900]\tvalid_0's auc: 0.788979\n",
      "Early stopping, best iteration is:\n",
      "[3825]\tvalid_0's auc: 0.789035\n",
      "\n",
      " Done with 1 fold\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.73929\n",
      "[200]\tvalid_0's auc: 0.751426\n",
      "[300]\tvalid_0's auc: 0.760828\n",
      "[400]\tvalid_0's auc: 0.767256\n",
      "[500]\tvalid_0's auc: 0.77189\n",
      "[600]\tvalid_0's auc: 0.775412\n",
      "[700]\tvalid_0's auc: 0.77809\n",
      "[800]\tvalid_0's auc: 0.78039\n",
      "[900]\tvalid_0's auc: 0.782042\n",
      "[1000]\tvalid_0's auc: 0.783383\n",
      "[1100]\tvalid_0's auc: 0.784309\n",
      "[1200]\tvalid_0's auc: 0.785249\n",
      "[1300]\tvalid_0's auc: 0.785928\n",
      "[1400]\tvalid_0's auc: 0.786597\n",
      "[1500]\tvalid_0's auc: 0.787107\n",
      "[1600]\tvalid_0's auc: 0.787579\n",
      "[1700]\tvalid_0's auc: 0.787953\n",
      "[1800]\tvalid_0's auc: 0.788266\n",
      "[1900]\tvalid_0's auc: 0.788499\n",
      "[2000]\tvalid_0's auc: 0.788742\n",
      "[2100]\tvalid_0's auc: 0.788839\n",
      "[2200]\tvalid_0's auc: 0.789037\n",
      "[2300]\tvalid_0's auc: 0.789269\n",
      "[2400]\tvalid_0's auc: 0.789405\n",
      "[2500]\tvalid_0's auc: 0.789526\n",
      "[2600]\tvalid_0's auc: 0.789657\n",
      "[2700]\tvalid_0's auc: 0.789711\n",
      "[2800]\tvalid_0's auc: 0.789752\n",
      "[2900]\tvalid_0's auc: 0.789848\n",
      "[3000]\tvalid_0's auc: 0.789889\n",
      "[3100]\tvalid_0's auc: 0.789985\n",
      "[3200]\tvalid_0's auc: 0.790043\n",
      "[3300]\tvalid_0's auc: 0.790095\n",
      "[3400]\tvalid_0's auc: 0.790144\n",
      "[3500]\tvalid_0's auc: 0.790154\n",
      "Early stopping, best iteration is:\n",
      "[3434]\tvalid_0's auc: 0.790207\n",
      "\n",
      " Done with 2 fold\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.737851\n",
      "[200]\tvalid_0's auc: 0.748737\n",
      "[300]\tvalid_0's auc: 0.757917\n",
      "[400]\tvalid_0's auc: 0.764724\n",
      "[500]\tvalid_0's auc: 0.770561\n",
      "[600]\tvalid_0's auc: 0.77488\n",
      "[700]\tvalid_0's auc: 0.778078\n",
      "[800]\tvalid_0's auc: 0.780375\n",
      "[900]\tvalid_0's auc: 0.782287\n",
      "[1000]\tvalid_0's auc: 0.783859\n",
      "[1100]\tvalid_0's auc: 0.784999\n",
      "[1200]\tvalid_0's auc: 0.785976\n",
      "[1300]\tvalid_0's auc: 0.786628\n",
      "[1400]\tvalid_0's auc: 0.787239\n",
      "[1500]\tvalid_0's auc: 0.787761\n",
      "[1600]\tvalid_0's auc: 0.788255\n",
      "[1700]\tvalid_0's auc: 0.788751\n",
      "[1800]\tvalid_0's auc: 0.789162\n",
      "[1900]\tvalid_0's auc: 0.789519\n",
      "[2000]\tvalid_0's auc: 0.789811\n",
      "[2100]\tvalid_0's auc: 0.79002\n",
      "[2200]\tvalid_0's auc: 0.790327\n",
      "[2300]\tvalid_0's auc: 0.79066\n",
      "[2400]\tvalid_0's auc: 0.790919\n",
      "[2500]\tvalid_0's auc: 0.791115\n",
      "[2600]\tvalid_0's auc: 0.791299\n",
      "[2700]\tvalid_0's auc: 0.791383\n",
      "[2800]\tvalid_0's auc: 0.791513\n",
      "[2900]\tvalid_0's auc: 0.791616\n",
      "[3000]\tvalid_0's auc: 0.791775\n",
      "[3100]\tvalid_0's auc: 0.791846\n",
      "[3200]\tvalid_0's auc: 0.791878\n",
      "[3300]\tvalid_0's auc: 0.792002\n",
      "[3400]\tvalid_0's auc: 0.792039\n",
      "[3500]\tvalid_0's auc: 0.792109\n",
      "[3600]\tvalid_0's auc: 0.792175\n",
      "[3700]\tvalid_0's auc: 0.792176\n",
      "[3800]\tvalid_0's auc: 0.792247\n",
      "[3900]\tvalid_0's auc: 0.792281\n",
      "[4000]\tvalid_0's auc: 0.792306\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3998]\tvalid_0's auc: 0.792307\n",
      "\n",
      " Done with 3 fold\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.738809\n",
      "[200]\tvalid_0's auc: 0.75126\n",
      "[300]\tvalid_0's auc: 0.760741\n",
      "[400]\tvalid_0's auc: 0.766796\n",
      "[500]\tvalid_0's auc: 0.772105\n",
      "[600]\tvalid_0's auc: 0.775976\n",
      "[700]\tvalid_0's auc: 0.779007\n",
      "[800]\tvalid_0's auc: 0.781381\n",
      "[900]\tvalid_0's auc: 0.783161\n",
      "[1000]\tvalid_0's auc: 0.784557\n",
      "[1100]\tvalid_0's auc: 0.785533\n",
      "[1200]\tvalid_0's auc: 0.786351\n",
      "[1300]\tvalid_0's auc: 0.786986\n",
      "[1400]\tvalid_0's auc: 0.787538\n",
      "[1500]\tvalid_0's auc: 0.788021\n",
      "[1600]\tvalid_0's auc: 0.788502\n",
      "[1700]\tvalid_0's auc: 0.788806\n",
      "[1800]\tvalid_0's auc: 0.789118\n",
      "[1900]\tvalid_0's auc: 0.789509\n",
      "[2000]\tvalid_0's auc: 0.789839\n",
      "[2100]\tvalid_0's auc: 0.790101\n",
      "[2200]\tvalid_0's auc: 0.790286\n",
      "[2300]\tvalid_0's auc: 0.790493\n",
      "[2400]\tvalid_0's auc: 0.790672\n",
      "[2500]\tvalid_0's auc: 0.790862\n",
      "[2600]\tvalid_0's auc: 0.791084\n",
      "[2700]\tvalid_0's auc: 0.791259\n",
      "[2800]\tvalid_0's auc: 0.791338\n",
      "[2900]\tvalid_0's auc: 0.791442\n",
      "[3000]\tvalid_0's auc: 0.791503\n",
      "[3100]\tvalid_0's auc: 0.791522\n",
      "[3200]\tvalid_0's auc: 0.791572\n",
      "[3300]\tvalid_0's auc: 0.7916\n",
      "[3400]\tvalid_0's auc: 0.791599\n",
      "Early stopping, best iteration is:\n",
      "[3322]\tvalid_0's auc: 0.791644\n",
      "\n",
      " Done with 4 fold\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.738374\n",
      "[200]\tvalid_0's auc: 0.750902\n",
      "[300]\tvalid_0's auc: 0.76051\n",
      "[400]\tvalid_0's auc: 0.767333\n",
      "[500]\tvalid_0's auc: 0.772642\n",
      "[600]\tvalid_0's auc: 0.776574\n",
      "[700]\tvalid_0's auc: 0.779822\n",
      "[800]\tvalid_0's auc: 0.782461\n",
      "[900]\tvalid_0's auc: 0.784512\n",
      "[1000]\tvalid_0's auc: 0.78604\n",
      "[1100]\tvalid_0's auc: 0.787237\n",
      "[1200]\tvalid_0's auc: 0.788285\n",
      "[1300]\tvalid_0's auc: 0.789094\n",
      "[1400]\tvalid_0's auc: 0.789731\n",
      "[1500]\tvalid_0's auc: 0.790252\n",
      "[1600]\tvalid_0's auc: 0.790732\n",
      "[1700]\tvalid_0's auc: 0.79116\n",
      "[1800]\tvalid_0's auc: 0.791561\n",
      "[1900]\tvalid_0's auc: 0.791975\n",
      "[2000]\tvalid_0's auc: 0.792348\n",
      "[2100]\tvalid_0's auc: 0.792591\n",
      "[2200]\tvalid_0's auc: 0.792922\n",
      "[2300]\tvalid_0's auc: 0.793183\n",
      "[2400]\tvalid_0's auc: 0.793427\n",
      "[2500]\tvalid_0's auc: 0.793698\n",
      "[2600]\tvalid_0's auc: 0.793885\n",
      "[2700]\tvalid_0's auc: 0.793949\n",
      "[2800]\tvalid_0's auc: 0.794128\n",
      "[2900]\tvalid_0's auc: 0.794257\n",
      "[3000]\tvalid_0's auc: 0.794336\n",
      "[3100]\tvalid_0's auc: 0.794402\n",
      "[3200]\tvalid_0's auc: 0.794475\n",
      "[3300]\tvalid_0's auc: 0.794483\n",
      "[3400]\tvalid_0's auc: 0.794481\n",
      "Early stopping, best iteration is:\n",
      "[3286]\tvalid_0's auc: 0.794505\n",
      "\n",
      " Done with 5 fold\n"
     ]
    }
   ],
   "source": [
    "folds       = KFold(n_splits=5, shuffle=True, random_state=17)\n",
    "predictions = np.zeros(test_x.shape[0])\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x)):\n",
    "    trn_x, trn_y = train_x[feats].iloc[trn_idx], train_y.iloc[trn_idx]\n",
    "    val_x, val_y = train_x[feats].iloc[val_idx], train_y.iloc[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(data=trn_x, label=trn_y, categorical_feature=cat_cols_test)\n",
    "    lgb_eval  = lgb.Dataset(data=val_x, label=val_y, categorical_feature=cat_cols_test)\n",
    "\n",
    "    params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "              'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "              'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "              'min_split_gain':0.05, 'min_child_weight':4, \"min_data_in_leaf\": 40,\n",
    "              \"feature_fraction\":0.5,\n",
    "              \"random_state\": random.randint(1, 100)}# Recommended to make the seed random\n",
    "\n",
    "\n",
    "    model = lgb.train(params, lgb_train, valid_sets=lgb_eval,\n",
    "                      early_stopping_rounds=150, verbose_eval=100) \n",
    "    \n",
    "    predictions += model.predict(test_x[feats]) / folds.n_splits\n",
    "    \n",
    "    print(\"\\n Done with {} fold\".format(n_fold + 1))\n",
    "    \n",
    "    del model, trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different model parameters\n",
    "\n",
    "Doesn't work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.271162\n",
      "[100]\tvalid_0's binary_logloss: 0.265485\n",
      "[150]\tvalid_0's binary_logloss: 0.261319\n",
      "[200]\tvalid_0's binary_logloss: 0.258687\n",
      "[250]\tvalid_0's binary_logloss: 0.256905\n",
      "[300]\tvalid_0's binary_logloss: 0.257087\n",
      "Early stopping, best iteration is:\n",
      "[275]\tvalid_0's binary_logloss: 0.255776\n",
      "\n",
      " Done with 1 fold\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.269917\n",
      "[100]\tvalid_0's binary_logloss: 0.264683\n",
      "[150]\tvalid_0's binary_logloss: 0.260896\n",
      "[200]\tvalid_0's binary_logloss: 0.258399\n",
      "[250]\tvalid_0's binary_logloss: 0.256813\n",
      "[300]\tvalid_0's binary_logloss: 0.25693\n",
      "[350]\tvalid_0's binary_logloss: 0.255589\n",
      "[400]\tvalid_0's binary_logloss: 0.253889\n",
      "[450]\tvalid_0's binary_logloss: 0.254301\n",
      "Early stopping, best iteration is:\n",
      "[424]\tvalid_0's binary_logloss: 0.253734\n",
      "\n",
      " Done with 2 fold\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.27039\n",
      "[100]\tvalid_0's binary_logloss: 0.264664\n",
      "[150]\tvalid_0's binary_logloss: 0.260628\n",
      "[200]\tvalid_0's binary_logloss: 0.258074\n",
      "[250]\tvalid_0's binary_logloss: 0.25638\n",
      "[300]\tvalid_0's binary_logloss: 0.256436\n",
      "[350]\tvalid_0's binary_logloss: 0.255043\n",
      "[400]\tvalid_0's binary_logloss: 0.253364\n",
      "[450]\tvalid_0's binary_logloss: 0.253557\n",
      "Early stopping, best iteration is:\n",
      "[424]\tvalid_0's binary_logloss: 0.253133\n",
      "\n",
      " Done with 3 fold\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.270232\n",
      "[100]\tvalid_0's binary_logloss: 0.26471\n",
      "[150]\tvalid_0's binary_logloss: 0.260618\n",
      "[200]\tvalid_0's binary_logloss: 0.258258\n",
      "[250]\tvalid_0's binary_logloss: 0.256547\n",
      "[300]\tvalid_0's binary_logloss: 0.256556\n",
      "[350]\tvalid_0's binary_logloss: 0.255166\n",
      "[400]\tvalid_0's binary_logloss: 0.253468\n",
      "[450]\tvalid_0's binary_logloss: 0.253666\n",
      "Early stopping, best iteration is:\n",
      "[424]\tvalid_0's binary_logloss: 0.2532\n",
      "\n",
      " Done with 4 fold\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.268727\n",
      "[100]\tvalid_0's binary_logloss: 0.263211\n",
      "[150]\tvalid_0's binary_logloss: 0.259101\n",
      "[200]\tvalid_0's binary_logloss: 0.256698\n",
      "[250]\tvalid_0's binary_logloss: 0.254948\n",
      "[300]\tvalid_0's binary_logloss: 0.255008\n",
      "[350]\tvalid_0's binary_logloss: 0.253606\n",
      "[400]\tvalid_0's binary_logloss: 0.251899\n",
      "[450]\tvalid_0's binary_logloss: 0.252238\n",
      "Early stopping, best iteration is:\n",
      "[424]\tvalid_0's binary_logloss: 0.251837\n",
      "\n",
      " Done with 5 fold\n"
     ]
    }
   ],
   "source": [
    "folds         = KFold(n_splits=5, shuffle=True, random_state=17)\n",
    "predictions_2 = np.zeros(test_x.shape[0])\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x)):\n",
    "    trn_x, trn_y = train_x[feats].iloc[trn_idx], train_y.iloc[trn_idx]\n",
    "    val_x, val_y = train_x[feats].iloc[val_idx], train_y.iloc[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(data=trn_x, label=trn_y, categorical_feature=cat_cols_test)\n",
    "    lgb_eval  = lgb.Dataset(data=val_x, label=val_y, categorical_feature=cat_cols_test)\n",
    "\n",
    "    params = {  'boosting': 'dart',\n",
    "                'application': 'binary',\n",
    "                'learning_rate': 0.1,\n",
    "                'min_data_in_leaf': 30,\n",
    "                'num_leaves': 31,\n",
    "                'max_depth': -1,\n",
    "                'feature_fraction': 0.5,\n",
    "                'scale_pos_weight': 2,\n",
    "                'drop_rate': 0.02,\n",
    "                \"metrics\": 'auc',\n",
    "                \"random_state\": random.randint(1, 100)}# Recommended to make the seed random\n",
    "              \n",
    "\n",
    "    model = lgb.train(params, lgb_train, valid_sets=lgb_eval,\n",
    "                      num_boost_round=600, early_stopping_rounds=50,\n",
    "                      verbose_eval=50) \n",
    "    \n",
    "    predictions_2 += model.predict(test_x[feats]) / folds.n_splits\n",
    "    \n",
    "    print(\"\\n Done with {} fold\".format(n_fold + 1))\n",
    "    \n",
    "    del model, trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final_pred\"></a>\n",
    "\n",
    "### [^](#toc) Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"SK_ID_CURR\": test_id,\n",
    "    \"TARGET\": predictions_2\n",
    "}).to_csv(\"../submissions/dart.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
