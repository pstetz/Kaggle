{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score: Also 0.784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "# <u>Table of Contents</u>\n",
    "1.) [TODO](#todo)  \n",
    "2.) [Imports](#imports)  \n",
    "3.) [Load data](#load)  \n",
    "4.) [Bureau](#bureau)  \n",
    "5.) [Bureau Balance](#bureau_bal)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.1.) [Merge into Bureau](#merge_bureau_bal)  \n",
    "6.) [Previous Application](#prev_app)  \n",
    "7.) [POS CASH balance](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.1.) [Missing values](#pos_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.2.) [Merge into Previous Application](#merge_pos_cash)  \n",
    "8.) [Installment Payments](#install_pay)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.1.) [Missing values](#install_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.2.) [Merge into Previous Application](#merge_install_pay)  \n",
    "9.) [Credit Card Balance](#credit)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 9.1.) [Missing values](#credit_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 9.2.) [Merge into Previous Application](#merge_credit)  \n",
    "10.) [Miscellaneous Clean Up](#clean_up)  \n",
    "11.) [Final Data Prep](#final_merge)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.1.) [Missing values](#final_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.2.) [Merge Previous Application with Full](#merge_prev)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.3.) [Merge Bureau with Full](#merge_bureau)  \n",
    "12.) [Modeling](#models)  \n",
    "13.) [Save file to CSV](#save)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todo\"></a>\n",
    "\n",
    "# [^](#toc) <u>TODO</u>\n",
    "\n",
    "- Fix skew on columns\n",
    "- Instead of col for NaN value increment shared column\n",
    "- Tinker with the best way to replace missing values (dropping cols?)\n",
    "- Look for outliers\n",
    "- Include timeline relatoinships like MONTHS_BALANCE\n",
    "- Tune model parameters\n",
    "- Average 5 folds in submission\n",
    "- Use feature importance to drop or improve features\n",
    "- Address [this](https://www.kaggle.com/c/home-credit-default-risk/discussion/57750)\n",
    "- Address [this](https://www.kaggle.com/c/home-credit-default-risk/discussion/57248) (XNA/XAP and 365243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "# [^](#toc) <u>Imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Time keeper\n",
    "import time\n",
    "\n",
    "# Garbage collector\n",
    "import gc\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Removes warnings from output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to create dummy variables of categorical features\n",
    "def get_dummies(df, cats):\n",
    "    for col in cats:\n",
    "        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "    return df \n",
    "\n",
    "def fillna_num(df):\n",
    "    missing_cols = [col for col in df.columns if any(df[col].isnull()) and df[col].dtype != object]\n",
    "    for col in missing_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    return df\n",
    "\n",
    "def fillna_cat(df):\n",
    "    for col in [col for col in df if df[col].dtype==object]:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df\n",
    "\n",
    "def factorize_df(df, cats):\n",
    "    for col in cats:\n",
    "        df[col], _ = pd.factorize(df[col])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"load\"></a>\n",
    "\n",
    "# [^](#toc) <u>Load data</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/home_default/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"bureau\"></a>\n",
    "\n",
    "# [^](#toc) <u>Bureau</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau: (1716428, 17)\n",
      "\n",
      "Columns of bureau:\n",
      "SK_ID_CURR --- SK_ID_BUREAU --- CREDIT_ACTIVE --- CREDIT_CURRENCY --- DAYS_CREDIT --- CREDIT_DAY_OVERDUE --- DAYS_CREDIT_ENDDATE --- DAYS_ENDDATE_FACT --- AMT_CREDIT_MAX_OVERDUE --- CNT_CREDIT_PROLONG --- AMT_CREDIT_SUM --- AMT_CREDIT_SUM_DEBT --- AMT_CREDIT_SUM_LIMIT --- AMT_CREDIT_SUM_OVERDUE --- CREDIT_TYPE --- DAYS_CREDIT_UPDATE --- AMT_ANNUITY\n"
     ]
    }
   ],
   "source": [
    "bureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\n",
    "print(\"Shape of bureau:\", bureau.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau:\")\n",
    "print(\" --- \".join(bureau.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau   = fillna_num(bureau)\n",
    "bureau   = fillna_cat(bureau)\n",
    "\n",
    "cols = [\"DAYS_CREDIT_ENDDATE\", \"DAYS_ENDDATE_FACT\", \"AMT_CREDIT_MAX_OVERDUE\",\n",
    "        \"AMT_CREDIT_SUM\", \"AMT_CREDIT_SUM_DEBT\", \"AMT_CREDIT_SUM_LIMIT\",\n",
    "        \"AMT_ANNUITY\"]\n",
    "\n",
    "bureau[\"nan\"] = np.zeros(len(bureau)).astype(int)\n",
    "for col in tqdm(cols):\n",
    "    bureau[\"nan\"] += bureau[col].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "    temp           = bureau[bureau[col].notnull()][col].median()\n",
    "    bureau[col]    = bureau[col].fillna(temp)\n",
    "    \n",
    "sum(bureau.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lump together values with low counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREDIT_CURRENCY\n",
    "cols = [\"currency 3\", \"currency 4\"]\n",
    "bureau.CREDIT_CURRENCY = bureau.CREDIT_CURRENCY.map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# CREDIT_TYPE\n",
    "cols = [\"Cash loan (non-earmarked)\", \"Real estate loan\", \"Loan for the purchase of equipment\",\n",
    "        \"Loan for purchase of shares (margin lending)\", \"Interbank credit\", \"Mobile operator loan\"]\n",
    "bureau.CREDIT_TYPE = bureau.CREDIT_TYPE.map(lambda x: \"MISC\" if x in cols else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_bal\"></a>\n",
    "\n",
    "# [^](#toc) <u>Bureau Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau_balance: (27299925, 3)\n",
      "\n",
      "Columns of bureau_balance:\n",
      "SK_ID_BUREAU --- MONTHS_BALANCE --- STATUS\n"
     ]
    }
   ],
   "source": [
    "bureau_balance = pd.read_csv(DATA_PATH + \"bureau_balance.csv\")\n",
    "print(\"Shape of bureau_balance:\",  bureau_balance.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau_balance:\")\n",
    "print(\" --- \".join(bureau_balance.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau_bal\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Bureau</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_df = get_dummies(bureau_balance, [\"STATUS\"])\n",
    "merge_df = merge_df.drop([\"MONTHS_BALANCE\", \"STATUS\"], axis=1)\n",
    "\n",
    "# prep for merge\n",
    "merge_df = merge_df.groupby(\"SK_ID_BUREAU\").sum().reset_index()\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = bureau_balance.groupby(\"SK_ID_BUREAU\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_BUREAU\").set_index(\"SK_ID_BUREAU\")\n",
    "\n",
    "### Remember added columns\n",
    "merged_cols = ['bur_bal_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "bureau = bureau.merge(right=merge_df.reset_index(), how='left', on='SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in new missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau[\"no_bureau_bal\"] = bureau[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "bureau[merged_cols]     = bureau[merged_cols].fillna(0)\n",
    "\n",
    "# Delete old variables\n",
    "del bureau_balance, merge_df, merged_cols, right\n",
    "gc.collect()\n",
    "\n",
    "# Make sure missing values were filled in\n",
    "sum(bureau.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"prev_app\"></a>\n",
    "\n",
    "# [^](#toc) <u>Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prev_app: (1670214, 37)\n",
      "\n",
      "Columns of prev_app:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NAME_CONTRACT_TYPE --- AMT_ANNUITY --- AMT_APPLICATION --- AMT_CREDIT --- AMT_DOWN_PAYMENT --- AMT_GOODS_PRICE --- WEEKDAY_APPR_PROCESS_START --- HOUR_APPR_PROCESS_START --- FLAG_LAST_APPL_PER_CONTRACT --- NFLAG_LAST_APPL_IN_DAY --- RATE_DOWN_PAYMENT --- RATE_INTEREST_PRIMARY --- RATE_INTEREST_PRIVILEGED --- NAME_CASH_LOAN_PURPOSE --- NAME_CONTRACT_STATUS --- DAYS_DECISION --- NAME_PAYMENT_TYPE --- CODE_REJECT_REASON --- NAME_TYPE_SUITE --- NAME_CLIENT_TYPE --- NAME_GOODS_CATEGORY --- NAME_PORTFOLIO --- NAME_PRODUCT_TYPE --- CHANNEL_TYPE --- SELLERPLACE_AREA --- NAME_SELLER_INDUSTRY --- CNT_PAYMENT --- NAME_YIELD_GROUP --- PRODUCT_COMBINATION --- DAYS_FIRST_DRAWING --- DAYS_FIRST_DUE --- DAYS_LAST_DUE_1ST_VERSION --- DAYS_LAST_DUE --- DAYS_TERMINATION --- NFLAG_INSURED_ON_APPROVAL\n"
     ]
    }
   ],
   "source": [
    "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\n",
    "print(\"Shape of prev_app:\",  prev_app.shape)\n",
    "\n",
    "print(\"\\nColumns of prev_app:\")\n",
    "print(\" --- \".join(prev_app.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_app = fillna_num(prev_app)\n",
    "prev_app = fillna_cat(prev_app)\n",
    "\n",
    "# prev_app[\"nan\"] = np.zeros(len(prev_app)).astype(int)\n",
    "\n",
    "# ### Fillin missing values with filler... NaN\n",
    "# for col in (\"NAME_TYPE_SUITE\", \"PRODUCT_COMBINATION\"):\n",
    "#     prev_app[\"nan\"] += prev_app[col].map(lambda x: 1 if type(x) == float else 0)\n",
    "#     prev_app[col]    = prev_app[col].fillna(\"NaN\")\n",
    "\n",
    "# ### Drop outliers\n",
    "# prev_app = prev_app.drop(\n",
    "#                             prev_app[(prev_app.NAME_GOODS_CATEGORY == \"Animals\") |\n",
    "#                                      (prev_app.NAME_GOODS_CATEGORY == \"House Construction\")].index)\n",
    "\n",
    "# ### Light missing values\n",
    "# prev_app[\"AMT_CREDIT\"] = prev_app[\"AMT_CREDIT\"].fillna(prev_app[\"AMT_CREDIT\"].median())\n",
    "\n",
    "# ### Moderate missing values\n",
    "# for col in tqdm((\"AMT_ANNUITY\", \"AMT_GOODS_PRICE\", \"CNT_PAYMENT\", \"AMT_DOWN_PAYMENT\", \"RATE_DOWN_PAYMENT\",\n",
    "#                 \"DAYS_FIRST_DRAWING\", \"DAYS_FIRST_DUE\", \"DAYS_LAST_DUE_1ST_VERSION\", \"DAYS_LAST_DUE\",\n",
    "#                 \"DAYS_TERMINATION\", \"NFLAG_INSURED_ON_APPROVAL\")):\n",
    "#     prev_app[\"nan\"] += prev_app[col].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "#     prev_app[col]    = prev_app[col].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "# ### Severe missing values\n",
    "# for col in (\"RATE_INTEREST_PRIMARY\", \"RATE_INTEREST_PRIVILEGED\"):\n",
    "#     prev_app[\"nan\"] += prev_app[col].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "#     prev_app[col]    = prev_app[col].transform(lambda x: x.fillna(x.median()))\n",
    "# #     prev_app[col] = prev_app[col].map(lambda x: 0 if np.isnan(x) else x)\n",
    "    \n",
    "sum(prev_app.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lump together values with low counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NAME_GOODS_CATEGORY\n",
    "prev_app.NAME_GOODS_CATEGORY = prev_app.NAME_GOODS_CATEGORY.map(\n",
    "    lambda x: \"MISC\" if x in [\"Weapon\", \"Insurance\"] else x)\n",
    "\n",
    "# NAME_CASH_LOAN_PURPOSE\n",
    "prev_app.NAME_CASH_LOAN_PURPOSE = prev_app.NAME_CASH_LOAN_PURPOSE.map(\n",
    "    lambda x: \"MISC\" if x in [\"Hobby\", \"Money for a third person\", \"Refusal to name the goal\"] else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"pos_cash\"></a>\n",
    "\n",
    "# [^](#toc) <u>POS CASH balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pcb: (10001358, 8)\n",
      "\n",
      "Columns of pcb:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- CNT_INSTALMENT --- CNT_INSTALMENT_FUTURE --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "pcb = pd.read_csv(DATA_PATH + \"POS_CASH_balance.csv\")\n",
    "print(\"Shape of pcb:\",  pcb.shape)\n",
    "\n",
    "print(\"\\nColumns of pcb:\")\n",
    "print(\" --- \".join(pcb.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_nan\"></a>\n",
    "\n",
    "### [^](#toc) Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill in missing values\n",
    "for col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n",
    "    pcb[col] = pcb[col].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "# Remove Outliers\n",
    "pcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_df = pcb[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "\n",
    "merge_df = get_dummies(merge_df, [\"NAME_CONTRACT_STATUS\"])\n",
    "merge_df = merge_df.drop(\"NAME_CONTRACT_STATUS\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_pos_cash\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prep for merge\n",
    "count    = merge_df.groupby(\"SK_ID_PREV\").count()\n",
    "merge_df = merge_df.groupby(\"SK_ID_PREV\").sum().reset_index()\n",
    "merge_df[\"N\"] = list(count.iloc[:,0])\n",
    "\n",
    "right    = pcb.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "merged_cols = ['pos_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:06<00:00,  2.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_app[\"no_pcb\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null      = prev_app[col].notnull()\n",
    "    temp          = prev_app[not_null][col].median()\n",
    "    prev_app[col] = prev_app[col].fillna(temp)    \n",
    "    \n",
    "# Delete old variables\n",
    "del pcb, merge_df, merged_cols, right, count\n",
    "gc.collect()\n",
    "\n",
    "# Make sure missing values were filled in\n",
    "sum(prev_app.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"install_pay\"></a>\n",
    "\n",
    "# [^](#toc) <u>Installment Payments</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of install_pay: (13605401, 8)\n",
      "\n",
      "Columns of install_pay:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NUM_INSTALMENT_VERSION --- NUM_INSTALMENT_NUMBER --- DAYS_INSTALMENT --- DAYS_ENTRY_PAYMENT --- AMT_INSTALMENT --- AMT_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "install_pay = pd.read_csv(DATA_PATH + \"installments_payments.csv\")\n",
    "print(\"Shape of install_pay:\",  install_pay.shape)\n",
    "\n",
    "print(\"\\nColumns of install_pay:\")\n",
    "print(\" --- \".join(install_pay.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install_nan\"></a>\n",
    "\n",
    "### [^](#toc) <u>Missing values</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in (\"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"):\n",
    "    install_pay[col + \"_nan\"] = install_pay[col].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "    install_pay[col] = install_pay[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "install_pay[\"AMT_MISSING\"] = install_pay[\"AMT_INSTALMENT\"] - install_pay[\"AMT_PAYMENT\"]\n",
    "temp = install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"]\n",
    "\n",
    "merge_df = pd.DataFrame({\n",
    "    \"INSTALL_missing_max\": temp.max(),\n",
    "    \"INSTALL_missing_min\": temp.min(),\n",
    "    \"INSTALL_payment_nan\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_PAYMENT_nan\"].sum(),\n",
    "    \"INSTALL_N\":           temp.count()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_install_pay\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right = install_pay.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.reset_index()\n",
    "\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "merged_cols = merge_df.columns\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_app[\"no_install\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null      = prev_app[col].notnull()\n",
    "    temp          = prev_app[not_null][col].median()\n",
    "    prev_app[col] = prev_app[col].fillna(temp)    \n",
    "\n",
    "# Delete old variables\n",
    "del install_pay, merge_df, merged_cols, right, temp\n",
    "gc.collect()\n",
    "\n",
    "# Make sure missing values were filled in\n",
    "sum(prev_app.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"credit\"></a>\n",
    "\n",
    "# [^](#toc) <u>Credit Card Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of credit_card: (3840312, 23)\n",
      "\n",
      "Columns of credit_card:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- AMT_BALANCE --- AMT_CREDIT_LIMIT_ACTUAL --- AMT_DRAWINGS_ATM_CURRENT --- AMT_DRAWINGS_CURRENT --- AMT_DRAWINGS_OTHER_CURRENT --- AMT_DRAWINGS_POS_CURRENT --- AMT_INST_MIN_REGULARITY --- AMT_PAYMENT_CURRENT --- AMT_PAYMENT_TOTAL_CURRENT --- AMT_RECEIVABLE_PRINCIPAL --- AMT_RECIVABLE --- AMT_TOTAL_RECEIVABLE --- CNT_DRAWINGS_ATM_CURRENT --- CNT_DRAWINGS_CURRENT --- CNT_DRAWINGS_OTHER_CURRENT --- CNT_DRAWINGS_POS_CURRENT --- CNT_INSTALMENT_MATURE_CUM --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\n",
    "print(\"Shape of credit_card:\",  credit_card.shape)\n",
    "\n",
    "print(\"\\nColumns of credit_card:\")\n",
    "print(\" --- \".join(credit_card.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"credit_nan\"></a>\n",
    "\n",
    "### [^](#toc) <u>Missing Values and Outliers</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:05<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "### Remove outliers\n",
    "# Gets indices with outlier values\n",
    "temp = credit_card[credit_card.NAME_CONTRACT_STATUS.isin([\"Refused\", \"Approved\"])].index\n",
    "\n",
    "# Drops outlier values\n",
    "credit_card = credit_card.drop(temp, axis=0)\n",
    "\n",
    "# ------------------------------\n",
    "#### Fill in missing values\n",
    "cols = [\n",
    "        \"AMT_DRAWINGS_ATM_CURRENT\", \"AMT_DRAWINGS_OTHER_CURRENT\", \"AMT_DRAWINGS_POS_CURRENT\", \n",
    "        \"AMT_INST_MIN_REGULARITY\", \"AMT_PAYMENT_CURRENT\", \"CNT_DRAWINGS_ATM_CURRENT\", \n",
    "        \"CNT_DRAWINGS_OTHER_CURRENT\", \"CNT_DRAWINGS_POS_CURRENT\", \"CNT_INSTALMENT_MATURE_CUM\"\n",
    "]\n",
    "for col in tqdm(cols):\n",
    "    not_null = credit_card[col].notnull()\n",
    "    temp = credit_card[not_null][col].median()\n",
    "    credit_card[col] = credit_card[col].fillna(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_credit\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Select numeric columns\n",
    "merge_df = pd.DataFrame({\n",
    "    \"AMT_BALANCE\": credit_card.groupby(\"SK_ID_PREV\").AMT_BALANCE.mean(),\n",
    "    \"SK_DPD\":      credit_card.groupby(\"SK_ID_PREV\").SK_DPD.max(),\n",
    "    \"SK_DPD_DEF\":  credit_card.groupby(\"SK_ID_PREV\").SK_DPD_DEF.max(),\n",
    "    \"N\":           credit_card.groupby(\"SK_ID_PREV\").count().iloc[:,0]\n",
    "})\n",
    "\n",
    "### Select categoric columns\n",
    "temp = credit_card[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "temp = get_dummies(temp, [\"NAME_CONTRACT_STATUS\"])\n",
    "temp = temp.drop(\"NAME_CONTRACT_STATUS\", axis=1)\n",
    "temp = temp.groupby(\"SK_ID_PREV\").sum()\n",
    "\n",
    "### Merge both\n",
    "merge_df = temp.join(merge_df)\n",
    "\n",
    "### Add the rest of the columns\n",
    "right = credit_card.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Merge\n",
    "merged_cols = ['credit_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in new NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:06<00:00,  4.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_app[\"no_credit\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null = prev_app[col].notnull()\n",
    "    temp = prev_app[not_null][col].median()\n",
    "    prev_app[col] = prev_app[col].fillna(temp)    \n",
    "    \n",
    "# Delete old variables\n",
    "del credit_card, merge_df, merged_cols, right, temp\n",
    "gc.collect()\n",
    "\n",
    "# Make sure missing values were filled in\n",
    "sum(prev_app.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"clean_up\"></a>\n",
    "\n",
    "# [^](#toc) <u>Miscellaneous Clean Up</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null in prev_app: 0\n",
      "Number of null in bureau:   0\n"
     ]
    }
   ],
   "source": [
    "### Drop unneeded ID columns\n",
    "prev_app = prev_app.drop(\"SK_ID_PREV\", axis=1)\n",
    "bureau   = bureau.drop(\"SK_ID_BUREAU\", axis=1)\n",
    "\n",
    "print(\"Number of null in prev_app:\", sum(prev_app.isnull().sum()))\n",
    "print(\"Number of null in bureau:  \", sum(bureau.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"final_merge\"></a>\n",
    "\n",
    "# [^](#toc) <u>Final Data Prep</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (307511, 122)\n",
      "Shape of test: (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "test  = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "\n",
    "print(\"Shape of train:\", train.shape)\n",
    "print(\"Shape of test:\",  test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split into predictors, target, and id\n",
    "train_y = train.TARGET\n",
    "train_x = train.drop([\"TARGET\"], axis=1)\n",
    "test_id = test.SK_ID_CURR\n",
    "test_x  = test\n",
    "\n",
    "### Merge train and test data\n",
    "full    = pd.concat([train_x, test_x])\n",
    "train_N = len(train_x)\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final_nan\"></a>\n",
    "\n",
    "### [^](#toc) <u>Missing values</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full[\"nan\"]  = np.zeros(len(full)).astype(int)\n",
    "\n",
    "# ------------ Categorical columns ------------ #\n",
    "### Severe missing\n",
    "for col in (\"OCCUPATION_TYPE\", \"FONDKAPREMONT_MODE\", \"HOUSETYPE_MODE\", \"WALLSMATERIAL_MODE\", \"EMERGENCYSTATE_MODE\"):\n",
    "    full[\"nan\"] += full[col].map(lambda x: 1 if type(x) == float else 0)\n",
    "    full[col]    = full[col].fillna(\"NaN\")\n",
    "\n",
    "### Low missing\n",
    "full[\"nan\"]            += full[\"NAME_TYPE_SUITE\"].map(lambda x: 1 if type(x) == float else 0)\n",
    "full[\"NAME_TYPE_SUITE\"] = full[\"NAME_TYPE_SUITE\"].fillna(\"Unaccompanied\")\n",
    "\n",
    "### Quasi missing\n",
    "full[\"nan\"]        += full[\"CODE_GENDER\"].map(lambda x: 1   if x == \"XNA\" else 0)\n",
    "full[\"CODE_GENDER\"] = full[\"CODE_GENDER\"].map(lambda x: \"F\" if x == \"XNA\" else x)\n",
    "\n",
    "full[\"nan\"]               += full[\"NAME_FAMILY_STATUS\"].map(lambda x: 1 if x == \"Unknown\" else 0)\n",
    "full[\"NAME_FAMILY_STATUS\"] = full[\"NAME_FAMILY_STATUS\"].map(lambda x: \"Married\" if x == \"Unknown\" else x)\n",
    "\n",
    "# NAME_INCOME_TYPE\n",
    "cols = [\"Unemployed\", \"Student\", \"Businessman\", \"Maternity leave\"]\n",
    "full[\"NAME_INCOME_TYPE\"] = full[\"NAME_INCOME_TYPE\"].map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "### ORGANIZATION_TYPE\n",
    "cols = [\"Trade: type 4\", \"Trade: type 5\"]\n",
    "full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Trade\" if x in cols else x)\n",
    "cols = [\"Industry: type 13\", \"Industry: type 8\"]\n",
    "full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Industry\" if x in cols else x)\n",
    "\n",
    "# -------------- Numeric columns -------------- #\n",
    "cols = [col for col in full.columns if any(full[col].isnull()) and full[col].dtype != object]\n",
    "for col in cols:\n",
    "    full[\"nan\"] += full[col].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "    full[col]    = full[col].fillna(full[col].median())\n",
    "\n",
    "full = fillna_cat(full)\n",
    "full = fillna_num(full)\n",
    "sum(full.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorize\n",
    "\n",
    "Don't factorize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get categorical features\n",
    "data_cats = [col for col in full.columns if full[col].dtype == 'object']\n",
    "\n",
    "# Factorize the dataframe\n",
    "full = factorize_df(full, data_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_prev\"></a>\n",
    "\n",
    "### [^](#toc) Merge Previous Application with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "        \"NAME_CONTRACT_TYPE\", \"WEEKDAY_APPR_PROCESS_START\",\n",
    "        \"FLAG_LAST_APPL_PER_CONTRACT\", \"NAME_CASH_LOAN_PURPOSE\",\n",
    "        \"NAME_CONTRACT_STATUS\", \"NAME_PAYMENT_TYPE\",\n",
    "        \"CODE_REJECT_REASON\", \"NAME_TYPE_SUITE\", \"NAME_CLIENT_TYPE\",\n",
    "        \"NAME_GOODS_CATEGORY\", \"NAME_PORTFOLIO\", \"NAME_PRODUCT_TYPE\",\n",
    "        \"CHANNEL_TYPE\", \"NAME_SELLER_INDUSTRY\", \"NAME_YIELD_GROUP\",\n",
    "        \"PRODUCT_COMBINATION\", \"SK_ID_CURR\"]\n",
    "num_cols = [col for col in prev_app.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "# Numeric columns\n",
    "merge_df      = prev_app[num_cols].groupby('SK_ID_CURR').mean()\n",
    "merge_df[\"N\"] = prev_app.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "\n",
    "# Categorical columns\n",
    "right = prev_app[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "\n",
    "merged_cols   = ['p_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [02:49<00:00,  1.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full[\"no_prev_app\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null  = full[col].notnull()\n",
    "    median    = full[not_null][col].median()\n",
    "    full[col] = full[col].fillna(median)    \n",
    "    \n",
    "sum(full.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau\"></a>\n",
    "\n",
    "### [^](#toc) Merge Bureau with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE', 'SK_ID_CURR']\n",
    "num_cols = [col for col in bureau.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "# Numeric columns\n",
    "merge_df      = bureau[num_cols].groupby('SK_ID_CURR').mean()\n",
    "merge_df[\"N\"] = bureau.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "\n",
    "# Categorical columns\n",
    "right = bureau[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "\n",
    "merged_cols   = ['b_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:38<00:00,  1.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full[\"no_bureau\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null  = full[col].notnull()\n",
    "    median    = full[not_null][col].median()\n",
    "    full[col] = full[col].fillna(median)    \n",
    "\n",
    "sum(full.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = full[['DAYS_EMPLOYED',\n",
    " 'p_INSTALL_missing_max',\n",
    " 'p_pos_CNT_INSTALMENT_FUTURE',\n",
    " 'DAYS_ID_PUBLISH',\n",
    " 'AMT_CREDIT',\n",
    " 'AMT_ANNUITY',\n",
    " 'DAYS_BIRTH',\n",
    " 'EXT_SOURCE_3',\n",
    " 'EXT_SOURCE_2',\n",
    " 'EXT_SOURCE_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split full back into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = full[:train_N]\n",
    "test_x = full[train_N:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed data look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>p_INSTALL_missing_max</th>\n",
       "      <th>p_pos_CNT_INSTALMENT_FUTURE</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-637</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>-2120</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>-9461</td>\n",
       "      <td>0.139376</td>\n",
       "      <td>0.262949</td>\n",
       "      <td>0.083037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1188</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>-291</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>-16765</td>\n",
       "      <td>0.533482</td>\n",
       "      <td>0.622246</td>\n",
       "      <td>0.311267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-225</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>-2531</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>-19046</td>\n",
       "      <td>0.729567</td>\n",
       "      <td>0.555912</td>\n",
       "      <td>0.506155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>-2437</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>-19005</td>\n",
       "      <td>0.533482</td>\n",
       "      <td>0.650442</td>\n",
       "      <td>0.506155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3038</td>\n",
       "      <td>4375.6575</td>\n",
       "      <td>8.416667</td>\n",
       "      <td>-3458</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>-19932</td>\n",
       "      <td>0.533482</td>\n",
       "      <td>0.322738</td>\n",
       "      <td>0.506155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DAYS_EMPLOYED  p_INSTALL_missing_max  p_pos_CNT_INSTALMENT_FUTURE  \\\n",
       "0           -637                 0.0000                    15.000000   \n",
       "1          -1188                 0.0000                     5.833333   \n",
       "2           -225                 0.0000                     2.500000   \n",
       "3          -3039                 0.0000                     5.777778   \n",
       "4          -3038              4375.6575                     8.416667   \n",
       "\n",
       "   DAYS_ID_PUBLISH  AMT_CREDIT  AMT_ANNUITY  DAYS_BIRTH  EXT_SOURCE_3  \\\n",
       "0            -2120    406597.5      24700.5       -9461      0.139376   \n",
       "1             -291   1293502.5      35698.5      -16765      0.533482   \n",
       "2            -2531    135000.0       6750.0      -19046      0.729567   \n",
       "3            -2437    312682.5      29686.5      -19005      0.533482   \n",
       "4            -3458    513000.0      21865.5      -19932      0.533482   \n",
       "\n",
       "   EXT_SOURCE_2  EXT_SOURCE_1  \n",
       "0      0.262949      0.083037  \n",
       "1      0.622246      0.311267  \n",
       "2      0.555912      0.506155  \n",
       "3      0.650442      0.506155  \n",
       "4      0.322738      0.506155  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# [^](#toc) <u>Models </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sban's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[200]\tvalid_0's auc: 0.729293\n",
      "[400]\tvalid_0's auc: 0.736504\n",
      "[600]\tvalid_0's auc: 0.741693\n",
      "[800]\tvalid_0's auc: 0.743779\n",
      "[1000]\tvalid_0's auc: 0.744465\n",
      "[1200]\tvalid_0's auc: 0.744913\n",
      "[1400]\tvalid_0's auc: 0.74504\n",
      "Early stopping, best iteration is:\n",
      "[1364]\tvalid_0's auc: 0.745075\n",
      "Training took 31 seconds\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(data=training_x, label=training_y)\n",
    "lgb_eval  = lgb.Dataset(data=val_x, label=val_y)\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc',\n",
    "          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n",
    "          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n",
    "          'min_split_gain':.01, 'min_child_weight':1}\n",
    "\n",
    "start = time.time()\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DAYS_EMPLOYED',\n",
       " 'p_INSTALL_missing_max',\n",
       " 'p_pos_CNT_INSTALMENT_FUTURE',\n",
       " 'DAYS_ID_PUBLISH',\n",
       " 'AMT_CREDIT',\n",
       " 'AMT_ANNUITY',\n",
       " 'DAYS_BIRTH',\n",
       " 'EXT_SOURCE_3',\n",
       " 'EXT_SOURCE_2',\n",
       " 'EXT_SOURCE_1')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = list(zip(*sorted(zip(model.feature_importance(), train_x.columns))))\n",
    "temp[1][229:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.702659\n",
      "Will train until train-auc hasn't improved in 100 rounds.\n",
      "[20]\ttrain-auc:0.726062\n",
      "[40]\ttrain-auc:0.728114\n",
      "[60]\ttrain-auc:0.730421\n",
      "[80]\ttrain-auc:0.734031\n",
      "[100]\ttrain-auc:0.736945\n",
      "[120]\ttrain-auc:0.738766\n",
      "[140]\ttrain-auc:0.740794\n",
      "[160]\ttrain-auc:0.743496\n",
      "[180]\ttrain-auc:0.745756\n",
      "[200]\ttrain-auc:0.748137\n",
      "[220]\ttrain-auc:0.75043\n",
      "[240]\ttrain-auc:0.752559\n",
      "[260]\ttrain-auc:0.754798\n",
      "[280]\ttrain-auc:0.756673\n",
      "[300]\ttrain-auc:0.758379\n",
      "[320]\ttrain-auc:0.760105\n",
      "[340]\ttrain-auc:0.761675\n",
      "[360]\ttrain-auc:0.763371\n",
      "[380]\ttrain-auc:0.764743\n",
      "[400]\ttrain-auc:0.766121\n",
      "[420]\ttrain-auc:0.767482\n",
      "[440]\ttrain-auc:0.768768\n",
      "[460]\ttrain-auc:0.769746\n",
      "[480]\ttrain-auc:0.770811\n",
      "[500]\ttrain-auc:0.771799\n",
      "[520]\ttrain-auc:0.772769\n",
      "[540]\ttrain-auc:0.773692\n",
      "[560]\ttrain-auc:0.774471\n",
      "[580]\ttrain-auc:0.775231\n",
      "[600]\ttrain-auc:0.77603\n",
      "[620]\ttrain-auc:0.776866\n",
      "[640]\ttrain-auc:0.777608\n",
      "[660]\ttrain-auc:0.778339\n",
      "[680]\ttrain-auc:0.779027\n",
      "[700]\ttrain-auc:0.779727\n",
      "[720]\ttrain-auc:0.780437\n",
      "[740]\ttrain-auc:0.78118\n",
      "[760]\ttrain-auc:0.781813\n",
      "[780]\ttrain-auc:0.782423\n",
      "[800]\ttrain-auc:0.783079\n",
      "[820]\ttrain-auc:0.783697\n",
      "[840]\ttrain-auc:0.784247\n",
      "[860]\ttrain-auc:0.784841\n",
      "[880]\ttrain-auc:0.785421\n",
      "[900]\ttrain-auc:0.785987\n",
      "[920]\ttrain-auc:0.786509\n",
      "[940]\ttrain-auc:0.787016\n",
      "[960]\ttrain-auc:0.787564\n",
      "[980]\ttrain-auc:0.78813\n",
      "[1000]\ttrain-auc:0.788628\n",
      "[1020]\ttrain-auc:0.789121\n",
      "[1040]\ttrain-auc:0.789625\n",
      "[1060]\ttrain-auc:0.790129\n",
      "[1080]\ttrain-auc:0.790602\n",
      "[1100]\ttrain-auc:0.790989\n",
      "[1120]\ttrain-auc:0.791465\n",
      "[1140]\ttrain-auc:0.791954\n",
      "[1160]\ttrain-auc:0.792392\n",
      "[1180]\ttrain-auc:0.792817\n",
      "[1200]\ttrain-auc:0.793294\n",
      "[1220]\ttrain-auc:0.793688\n",
      "[1240]\ttrain-auc:0.79414\n",
      "[1260]\ttrain-auc:0.794536\n",
      "[1280]\ttrain-auc:0.794892\n",
      "[1300]\ttrain-auc:0.795259\n",
      "[1320]\ttrain-auc:0.795676\n",
      "[1340]\ttrain-auc:0.796054\n",
      "[1360]\ttrain-auc:0.796477\n",
      "[1380]\ttrain-auc:0.796869\n",
      "[1400]\ttrain-auc:0.797217\n",
      "[1420]\ttrain-auc:0.797591\n",
      "[1440]\ttrain-auc:0.797965\n",
      "[1460]\ttrain-auc:0.798298\n",
      "[1480]\ttrain-auc:0.798655\n",
      "[1500]\ttrain-auc:0.798962\n",
      "[1520]\ttrain-auc:0.799281\n",
      "[1540]\ttrain-auc:0.799625\n",
      "[1560]\ttrain-auc:0.799917\n",
      "[1580]\ttrain-auc:0.800264\n",
      "[1600]\ttrain-auc:0.800556\n",
      "[1620]\ttrain-auc:0.80084\n",
      "[1640]\ttrain-auc:0.801139\n",
      "[1660]\ttrain-auc:0.801478\n",
      "[1680]\ttrain-auc:0.801753\n",
      "[1700]\ttrain-auc:0.802102\n",
      "[1720]\ttrain-auc:0.802423\n",
      "[1740]\ttrain-auc:0.802775\n",
      "[1760]\ttrain-auc:0.803081\n",
      "[1780]\ttrain-auc:0.80337\n",
      "[1800]\ttrain-auc:0.803698\n",
      "[1820]\ttrain-auc:0.804021\n",
      "[1840]\ttrain-auc:0.804321\n",
      "[1860]\ttrain-auc:0.804561\n",
      "[1880]\ttrain-auc:0.804872\n",
      "[1900]\ttrain-auc:0.805166\n",
      "[1920]\ttrain-auc:0.805407\n",
      "[1940]\ttrain-auc:0.805653\n",
      "[1960]\ttrain-auc:0.805935\n",
      "[1980]\ttrain-auc:0.80621\n",
      "[2000]\ttrain-auc:0.80646\n",
      "[2020]\ttrain-auc:0.806664\n",
      "[2040]\ttrain-auc:0.806913\n",
      "[2060]\ttrain-auc:0.807217\n",
      "[2080]\ttrain-auc:0.807488\n",
      "[2100]\ttrain-auc:0.807726\n",
      "[2120]\ttrain-auc:0.807978\n",
      "[2140]\ttrain-auc:0.808204\n",
      "[2160]\ttrain-auc:0.808484\n",
      "[2180]\ttrain-auc:0.808687\n",
      "[2200]\ttrain-auc:0.808931\n",
      "[2220]\ttrain-auc:0.809148\n",
      "[2240]\ttrain-auc:0.809397\n",
      "[2260]\ttrain-auc:0.809583\n",
      "[2280]\ttrain-auc:0.809807\n",
      "[2300]\ttrain-auc:0.810035\n",
      "[2320]\ttrain-auc:0.810305\n",
      "[2340]\ttrain-auc:0.810543\n",
      "[2360]\ttrain-auc:0.810824\n",
      "[2380]\ttrain-auc:0.81103\n",
      "[2400]\ttrain-auc:0.811323\n",
      "[2420]\ttrain-auc:0.811565\n",
      "[2440]\ttrain-auc:0.811761\n",
      "[2460]\ttrain-auc:0.812024\n",
      "[2480]\ttrain-auc:0.812235\n",
      "[2500]\ttrain-auc:0.812457\n",
      "[2520]\ttrain-auc:0.81268\n",
      "[2540]\ttrain-auc:0.812859\n",
      "[2560]\ttrain-auc:0.813054\n",
      "[2580]\ttrain-auc:0.813269\n",
      "[2600]\ttrain-auc:0.813474\n",
      "[2620]\ttrain-auc:0.813677\n",
      "[2640]\ttrain-auc:0.813904\n",
      "[2660]\ttrain-auc:0.814158\n",
      "[2680]\ttrain-auc:0.814317\n",
      "[2700]\ttrain-auc:0.814579\n",
      "[2720]\ttrain-auc:0.814816\n",
      "[2740]\ttrain-auc:0.815045\n",
      "[2760]\ttrain-auc:0.815291\n",
      "[2780]\ttrain-auc:0.815537\n",
      "[2800]\ttrain-auc:0.815778\n",
      "[2820]\ttrain-auc:0.815936\n",
      "[2840]\ttrain-auc:0.81612\n",
      "[2860]\ttrain-auc:0.816274\n",
      "[2880]\ttrain-auc:0.81649\n",
      "[2900]\ttrain-auc:0.816706\n",
      "[2920]\ttrain-auc:0.816926\n",
      "[2940]\ttrain-auc:0.817164\n",
      "[2960]\ttrain-auc:0.8174\n",
      "[2980]\ttrain-auc:0.81756\n",
      "[3000]\ttrain-auc:0.81774\n",
      "[3020]\ttrain-auc:0.817932\n",
      "[3040]\ttrain-auc:0.818119\n",
      "[3060]\ttrain-auc:0.818311\n",
      "[3080]\ttrain-auc:0.818513\n",
      "[3100]\ttrain-auc:0.818669\n",
      "[3120]\ttrain-auc:0.81884\n",
      "[3140]\ttrain-auc:0.819013\n",
      "[3160]\ttrain-auc:0.819172\n",
      "[3180]\ttrain-auc:0.819398\n",
      "[3200]\ttrain-auc:0.8196\n",
      "[3220]\ttrain-auc:0.819839\n",
      "[3240]\ttrain-auc:0.820059\n",
      "[3260]\ttrain-auc:0.820282\n",
      "[3280]\ttrain-auc:0.820471\n",
      "[3300]\ttrain-auc:0.820673\n",
      "[3320]\ttrain-auc:0.820866\n",
      "[3340]\ttrain-auc:0.821059\n",
      "[3360]\ttrain-auc:0.821292\n",
      "[3380]\ttrain-auc:0.821462\n",
      "[3400]\ttrain-auc:0.821653\n",
      "[3420]\ttrain-auc:0.82184\n",
      "[3440]\ttrain-auc:0.821994\n",
      "[3460]\ttrain-auc:0.82215\n",
      "[3480]\ttrain-auc:0.82233\n",
      "[3500]\ttrain-auc:0.822517\n",
      "[3520]\ttrain-auc:0.822728\n",
      "[3540]\ttrain-auc:0.822944\n",
      "[3560]\ttrain-auc:0.823164\n",
      "[3580]\ttrain-auc:0.823325\n",
      "[3600]\ttrain-auc:0.823515\n",
      "[3620]\ttrain-auc:0.823703\n",
      "[3640]\ttrain-auc:0.823894\n",
      "[3660]\ttrain-auc:0.824086\n",
      "[3680]\ttrain-auc:0.824277\n",
      "[3700]\ttrain-auc:0.824431\n",
      "[3720]\ttrain-auc:0.82462\n",
      "[3740]\ttrain-auc:0.824786\n",
      "[3760]\ttrain-auc:0.824974\n",
      "[3780]\ttrain-auc:0.825146\n",
      "[3800]\ttrain-auc:0.825327\n",
      "[3820]\ttrain-auc:0.825547\n",
      "[3840]\ttrain-auc:0.825786\n",
      "[3860]\ttrain-auc:0.825996\n",
      "[3880]\ttrain-auc:0.826173\n",
      "[3900]\ttrain-auc:0.826361\n",
      "[3920]\ttrain-auc:0.826545\n",
      "[3940]\ttrain-auc:0.826726\n",
      "[3960]\ttrain-auc:0.826913\n",
      "[3980]\ttrain-auc:0.827058\n",
      "[4000]\ttrain-auc:0.827238\n",
      "[4020]\ttrain-auc:0.827399\n",
      "[4040]\ttrain-auc:0.827565\n",
      "[4060]\ttrain-auc:0.827727\n",
      "[4080]\ttrain-auc:0.827904\n",
      "[4100]\ttrain-auc:0.828131\n",
      "[4120]\ttrain-auc:0.828317\n",
      "[4140]\ttrain-auc:0.828497\n",
      "[4160]\ttrain-auc:0.828689\n",
      "[4180]\ttrain-auc:0.828862\n",
      "[4200]\ttrain-auc:0.829013\n",
      "[4220]\ttrain-auc:0.829226\n",
      "[4240]\ttrain-auc:0.829332\n",
      "[4260]\ttrain-auc:0.829498\n",
      "[4280]\ttrain-auc:0.829666\n",
      "[4300]\ttrain-auc:0.829834\n",
      "[4320]\ttrain-auc:0.830005\n",
      "[4340]\ttrain-auc:0.830181\n",
      "[4360]\ttrain-auc:0.830333\n",
      "[4380]\ttrain-auc:0.830514\n",
      "[4400]\ttrain-auc:0.830669\n",
      "[4420]\ttrain-auc:0.830822\n",
      "[4440]\ttrain-auc:0.831025\n",
      "[4460]\ttrain-auc:0.831178\n",
      "[4480]\ttrain-auc:0.831311\n",
      "[4500]\ttrain-auc:0.831484\n",
      "[4520]\ttrain-auc:0.831661\n",
      "[4540]\ttrain-auc:0.831816\n",
      "[4560]\ttrain-auc:0.831985\n",
      "[4580]\ttrain-auc:0.832157\n",
      "[4600]\ttrain-auc:0.832305\n",
      "[4620]\ttrain-auc:0.832457\n",
      "[4640]\ttrain-auc:0.83264\n",
      "[4660]\ttrain-auc:0.832813\n",
      "[4680]\ttrain-auc:0.833009\n",
      "[4700]\ttrain-auc:0.833176\n",
      "[4720]\ttrain-auc:0.833315\n",
      "[4740]\ttrain-auc:0.833476\n",
      "[4760]\ttrain-auc:0.833651\n",
      "[4780]\ttrain-auc:0.833829\n",
      "[4800]\ttrain-auc:0.834041\n",
      "[4820]\ttrain-auc:0.834163\n",
      "[4840]\ttrain-auc:0.834308\n",
      "[4860]\ttrain-auc:0.834493\n",
      "[4880]\ttrain-auc:0.834637\n",
      "[4900]\ttrain-auc:0.834773\n",
      "[4920]\ttrain-auc:0.834915\n",
      "[4940]\ttrain-auc:0.835067\n",
      "[4960]\ttrain-auc:0.835225\n",
      "[4980]\ttrain-auc:0.835379\n",
      "[4999]\ttrain-auc:0.835553\n",
      "Training took 12638 seconds\n"
     ]
    }
   ],
   "source": [
    "xgb_train = xgb.DMatrix(data=train_x, label=train_y)\n",
    "watchlist = [(xgb_train, 'train')]\n",
    "\n",
    "params = {'seed': 0, 'colsample_bytree': 0.8, 'silent': 1,\n",
    "         'subsample': 0.6, 'learning_rate': 0.01, 'objective': 'reg:linear',\n",
    "         'max_depth': 4, 'num_parallel_tree': 1, 'min_child_weight': 1,\n",
    "         'eval_metric': 'auc'}\n",
    "\n",
    "start = time.time()\n",
    "xgb_model = xgb.train(params, xgb_train, 5000, watchlist, early_stopping_rounds=100, verbose_eval=20)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(test_x)\n",
    "predictions = xgb_model.predict(dtest, ntree_limit=xgb_model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = predictions.clip(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"SK_ID_CURR\": test_id,\n",
    "    \"TARGET\": predictions\n",
    "}).to_csv(\"../submissions/240_best_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
