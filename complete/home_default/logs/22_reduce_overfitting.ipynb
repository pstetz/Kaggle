{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score:\n",
    "CV 0.78879859253631179 [3800]\n",
    "\n",
    "LB 0.783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "# <u>Table of Contents</u>\n",
    "1.) [TODO](#todo)  \n",
    "2.) [Imports](#imports)  \n",
    "3.) [Bureau](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.1.) [Data Processing](#bureau_process)  \n",
    "4.) [Bureau Balance](#bureau_bal)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.1.) [Merge into Bureau](#merge_bureau_bal)  \n",
    "5.) [Previous Application](#prev_app)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.1.) [Data Processing](#prev_process)  \n",
    "6.) [POS CASH balance](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.1.) [Data Processing](#pos_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.2.) [Merge into Previous Application](#merge_pos_cash)  \n",
    "7.) [Installment Payments](#install_pay)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.1.) [Merge into Previous Application](#merge_install_pay)  \n",
    "8.) [Credit Card Balance](#credit)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.1.) [Data Processing](#credit_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.2.) [Merge into Previous Application](#merge_credit)  \n",
    "9.) [Miscellaneous clean up](#misc)  \n",
    "10.) [Final Data Prep](#final_merge)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.1.) [Data Processing](#final_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.2.) [Create Features](#train_feat)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.3.) [Categorical values](#train_cat)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.4.) [Merge Previous Application with Full](#merge_prev)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.5.) [Merge Bureau with Full](#merge_bureau)  \n",
    "11.) [Modeling](#models)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.1.) [Feature Reduction](#feat_reduction)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.2.) [Most important features](#important_feats)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.3.) [Parameter tuning](#param_tuning)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 11.4.) [CV Score](#cv)  \n",
    "12.) [Final submission](#final)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 12.1.) [Final predictions](#final_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todo\"></a>\n",
    "\n",
    "# [^](#toc) <u>TODO</u>\n",
    "\n",
    "- Fix skew on columns\n",
    "- Tinker with the best way to replace missing values (dropping cols?)\n",
    "- Look for outliers\n",
    "- Include timeline relatoinships like MONTHS_BALANCE\n",
    "- Added coded value of -1 to missing numerical values\n",
    "- Address [this](https://www.kaggle.com/c/home-credit-default-risk/discussion/57248)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "# [^](#toc) <u>Imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Time keeper\n",
    "import time\n",
    "\n",
    "# Randomize seeds\n",
    "import random\n",
    "\n",
    "# Garbage collector\n",
    "import gc\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Removes warnings from output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dummies(df, cats):\n",
    "    for col in cats:\n",
    "        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "    return df \n",
    "\n",
    "def factorize_df(df, cats):\n",
    "    for col in cats:\n",
    "        df[col], _ = pd.factorize(df[col])\n",
    "    return df \n",
    "\n",
    "DATA_PATH = \"../data/home_default/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"bureau\"></a>\n",
    "\n",
    "# [^](#toc) Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau: (1716428, 17)\n",
      "\n",
      "Columns of bureau:\n",
      "SK_ID_CURR --- SK_ID_BUREAU --- CREDIT_ACTIVE --- CREDIT_CURRENCY --- DAYS_CREDIT --- CREDIT_DAY_OVERDUE --- DAYS_CREDIT_ENDDATE --- DAYS_ENDDATE_FACT --- AMT_CREDIT_MAX_OVERDUE --- CNT_CREDIT_PROLONG --- AMT_CREDIT_SUM --- AMT_CREDIT_SUM_DEBT --- AMT_CREDIT_SUM_LIMIT --- AMT_CREDIT_SUM_OVERDUE --- CREDIT_TYPE --- DAYS_CREDIT_UPDATE --- AMT_ANNUITY\n"
     ]
    }
   ],
   "source": [
    "bureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\n",
    "print(\"Shape of bureau:\", bureau.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau:\")\n",
    "print(\" --- \".join(bureau.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Lump together values with low counts\n",
    "# CREDIT_CURRENCY\n",
    "cols = [\"currency 3\", \"currency 4\"]\n",
    "bureau.CREDIT_CURRENCY = bureau.CREDIT_CURRENCY.map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# # CREDIT_TYPE\n",
    "# cols = [\"Cash loan (non-earmarked)\", \"Real estate loan\", \"Loan for the purchase of equipment\",\n",
    "#         \"Loan for purchase of shares (margin lending)\", \"Interbank credit\", \"Mobile operator loan\"]\n",
    "# bureau.CREDIT_TYPE = bureau.CREDIT_TYPE.map(lambda x: \"MISC\" if x in cols else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_bal\"></a>\n",
    "\n",
    "# [^](#toc) <u>Bureau Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau_balance: (27299925, 3)\n",
      "\n",
      "Columns of bureau_balance:\n",
      "SK_ID_BUREAU --- MONTHS_BALANCE --- STATUS\n"
     ]
    }
   ],
   "source": [
    "bureau_balance = pd.read_csv(DATA_PATH + \"bureau_balance.csv\")\n",
    "print(\"Shape of bureau_balance:\",  bureau_balance.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau_balance:\")\n",
    "print(\" --- \".join(bureau_balance.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau_bal\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Bureau</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get sum of counts in categorical column\n",
    "merge_df = get_dummies(bureau_balance, [\"STATUS\"])\n",
    "cols = ['STATUS_0', 'STATUS_1', 'STATUS_2', 'STATUS_3', 'STATUS_4', 'STATUS_5', 'STATUS_C', 'STATUS_X']\n",
    "for col in cols:\n",
    "    merge_df[col] = merge_df[col] / (merge_df[\"MONTHS_BALANCE\"] - 1)\n",
    "merge_df = merge_df.drop([\"MONTHS_BALANCE\", \"STATUS\"], axis=1)\n",
    "merge_df = merge_df.groupby(\"SK_ID_BUREAU\").sum().reset_index()\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = bureau_balance.groupby(\"SK_ID_BUREAU\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_BUREAU\").set_index(\"SK_ID_BUREAU\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['bur_bal_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "bureau = bureau.merge(right=merge_df.reset_index(), how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "# Mark missing values\n",
    "bureau[\"no_bureau_bal\"] = bureau[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del bureau_balance, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"prev_app\"></a>\n",
    "\n",
    "# [^](#toc) <u>Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prev_app: (1670214, 37)\n",
      "\n",
      "Columns of prev_app:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NAME_CONTRACT_TYPE --- AMT_ANNUITY --- AMT_APPLICATION --- AMT_CREDIT --- AMT_DOWN_PAYMENT --- AMT_GOODS_PRICE --- WEEKDAY_APPR_PROCESS_START --- HOUR_APPR_PROCESS_START --- FLAG_LAST_APPL_PER_CONTRACT --- NFLAG_LAST_APPL_IN_DAY --- RATE_DOWN_PAYMENT --- RATE_INTEREST_PRIMARY --- RATE_INTEREST_PRIVILEGED --- NAME_CASH_LOAN_PURPOSE --- NAME_CONTRACT_STATUS --- DAYS_DECISION --- NAME_PAYMENT_TYPE --- CODE_REJECT_REASON --- NAME_TYPE_SUITE --- NAME_CLIENT_TYPE --- NAME_GOODS_CATEGORY --- NAME_PORTFOLIO --- NAME_PRODUCT_TYPE --- CHANNEL_TYPE --- SELLERPLACE_AREA --- NAME_SELLER_INDUSTRY --- CNT_PAYMENT --- NAME_YIELD_GROUP --- PRODUCT_COMBINATION --- DAYS_FIRST_DRAWING --- DAYS_FIRST_DUE --- DAYS_LAST_DUE_1ST_VERSION --- DAYS_LAST_DUE --- DAYS_TERMINATION --- NFLAG_INSURED_ON_APPROVAL\n"
     ]
    }
   ],
   "source": [
    "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\n",
    "print(\"Shape of prev_app:\",  prev_app.shape)\n",
    "\n",
    "print(\"\\nColumns of prev_app:\")\n",
    "print(\" --- \".join(prev_app.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_app[\"ASKED_ACTUAL_RATIO\"] = prev_app[\"AMT_APPLICATION\"] / prev_app[\"AMT_CREDIT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prev_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fill in values that should be null\n",
    "prev_app['DAYS_FIRST_DRAWING'       ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_FIRST_DUE'           ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_LAST_DUE'            ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_TERMINATION'         ].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "### Lump together values with low counts\n",
    "# NAME_GOODS_CATEGORY\n",
    "prev_app.NAME_GOODS_CATEGORY = prev_app.NAME_GOODS_CATEGORY.map(\n",
    "    lambda x: \"MISC\" if x in [\"Weapon\", \"Insurance\"] else x)\n",
    "\n",
    "# NAME_CASH_LOAN_PURPOSE\n",
    "prev_app.NAME_CASH_LOAN_PURPOSE = prev_app.NAME_CASH_LOAN_PURPOSE.map(\n",
    "    lambda x: \"MISC\" if x in [\"Buying a garage\", \"Misc\"] else x)\n",
    "\n",
    "# Create features\n",
    "prev_app[\"APP_CREDIT_PERC\"] = prev_app['AMT_APPLICATION'] / prev_app['AMT_CREDIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"pos_cash\"></a>\n",
    "\n",
    "# [^](#toc) <u>POS CASH balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pcb: (10001358, 8)\n",
      "\n",
      "Columns of pcb:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- CNT_INSTALMENT --- CNT_INSTALMENT_FUTURE --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "pcb = pd.read_csv(DATA_PATH + \"POS_CASH_balance.csv\")\n",
    "print(\"Shape of pcb:\",  pcb.shape)\n",
    "\n",
    "print(\"\\nColumns of pcb:\")\n",
    "print(\" --- \".join(pcb.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Outliers\n",
    "pcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_pos_cash\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get Dummies\n",
    "merge_df = pcb[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "merge_df = get_dummies(merge_df, [\"NAME_CONTRACT_STATUS\"])\n",
    "merge_df = merge_df.drop(\"NAME_CONTRACT_STATUS\", axis=1)\n",
    "\n",
    "# Prep for merge\n",
    "count    = merge_df.groupby(\"SK_ID_PREV\").count()\n",
    "merge_df = merge_df.groupby(\"SK_ID_PREV\").sum().reset_index()\n",
    "merge_df[\"N\"] = list(count.iloc[:,0])\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = pcb.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['pos_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_pcb\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del pcb, count, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"install_pay\"></a>\n",
    "\n",
    "# [^](#toc) <u>Installment Payments</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of install_pay: (13605401, 8)\n",
      "\n",
      "Columns of install_pay:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NUM_INSTALMENT_VERSION --- NUM_INSTALMENT_NUMBER --- DAYS_INSTALMENT --- DAYS_ENTRY_PAYMENT --- AMT_INSTALMENT --- AMT_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "install_pay = pd.read_csv(DATA_PATH + \"installments_payments.csv\")\n",
    "print(\"Shape of install_pay:\",  install_pay.shape)\n",
    "\n",
    "print(\"\\nColumns of install_pay:\")\n",
    "print(\" --- \".join(install_pay.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_install_pay\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create new feature\n",
    "install_pay[\"AMT_MISSING\"]  = install_pay[\"AMT_INSTALMENT\"]     - install_pay[\"AMT_PAYMENT\"]\n",
    "install_pay['PAYMENT_PERC'] = install_pay['AMT_PAYMENT']        / install_pay['AMT_INSTALMENT']\n",
    "\n",
    "# Days past due and days before due (no negative values)\n",
    "install_pay['DPD']          = install_pay['DAYS_ENTRY_PAYMENT'] - install_pay['DAYS_INSTALMENT']\n",
    "install_pay['DBD']          = install_pay['DAYS_INSTALMENT']    - install_pay['DAYS_ENTRY_PAYMENT']\n",
    "install_pay['DPD']          = install_pay['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "install_pay['DBD']          = install_pay['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "# Amount of values missing in AMT_PAYMENT\n",
    "install_pay[\"temp\"]         = install_pay[\"AMT_PAYMENT\"].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Select important features\n",
    "merge_df = pd.DataFrame({\n",
    "    \"missing_max\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].max(),\n",
    "    \"missing_min\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].min(),\n",
    "    \"payment_max\": install_pay.groupby(\"SK_ID_PREV\")['PAYMENT_PERC'].max(),\n",
    "    \"payment_min\": install_pay.groupby(\"SK_ID_PREV\")['PAYMENT_PERC'].min(),\n",
    "    \n",
    "    \"payment_nan\": install_pay.groupby(\"SK_ID_PREV\")[\"temp\"].sum(),\n",
    "    \"N\":           install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].count(),\n",
    "    \n",
    "    \"unique_ver\":  install_pay.groupby(\"SK_ID_PREV\")[\"NUM_INSTALMENT_VERSION\"].nunique()\n",
    "})\n",
    "\n",
    "# Delete temp column\n",
    "install_pay = install_pay.drop(\"temp\", axis=1)\n",
    "\n",
    "# Select median of everything\n",
    "right = install_pay.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "\n",
    "### Merge the two\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['install_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_install\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del install_pay, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"credit\"></a>\n",
    "\n",
    "# [^](#toc) <u>Credit Card Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of credit_card: (3840312, 23)\n",
      "\n",
      "Columns of credit_card:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- AMT_BALANCE --- AMT_CREDIT_LIMIT_ACTUAL --- AMT_DRAWINGS_ATM_CURRENT --- AMT_DRAWINGS_CURRENT --- AMT_DRAWINGS_OTHER_CURRENT --- AMT_DRAWINGS_POS_CURRENT --- AMT_INST_MIN_REGULARITY --- AMT_PAYMENT_CURRENT --- AMT_PAYMENT_TOTAL_CURRENT --- AMT_RECEIVABLE_PRINCIPAL --- AMT_RECIVABLE --- AMT_TOTAL_RECEIVABLE --- CNT_DRAWINGS_ATM_CURRENT --- CNT_DRAWINGS_CURRENT --- CNT_DRAWINGS_OTHER_CURRENT --- CNT_DRAWINGS_POS_CURRENT --- CNT_INSTALMENT_MATURE_CUM --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\n",
    "print(\"Shape of credit_card:\",  credit_card.shape)\n",
    "\n",
    "print(\"\\nColumns of credit_card:\")\n",
    "print(\" --- \".join(credit_card.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"credit_process\"></a>\n",
    "\n",
    "### [^](#toc) <u>Data Processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gets indices with outlier values\n",
    "temp = credit_card[credit_card.NAME_CONTRACT_STATUS.isin([\"Refused\", \"Approved\"])].index\n",
    "\n",
    "# Drops outlier values\n",
    "credit_card = credit_card.drop(temp, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit_card[\"AVG_DRAWINGS_ATM_CURRENT\"]   = (credit_card[\"AMT_DRAWINGS_ATM_CURRENT\"] /\n",
    "                                             credit_card[\"CNT_DRAWINGS_ATM_CURRENT\"])\n",
    "\n",
    "credit_card[\"AVG_DRAWINGS_CURRENT\"]       = (credit_card[\"AMT_DRAWINGS_CURRENT\"] /\n",
    "                                             credit_card[\"CNT_DRAWINGS_CURRENT\"])\n",
    "\n",
    "credit_card[\"AVG_DRAWINGS_OTHER_CURRENT\"] = (credit_card[\"AMT_DRAWINGS_OTHER_CURRENT\"] /\n",
    "                                             credit_card[\"CNT_DRAWINGS_OTHER_CURRENT\"])\n",
    "\n",
    "credit_card[\"AVG_DRAWINGS_POS_CURRENT\"]   = (credit_card[\"AMT_DRAWINGS_POS_CURRENT\"] /\n",
    "                                             credit_card[\"CNT_DRAWINGS_POS_CURRENT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_credit\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create features\n",
    "merge_df = pd.DataFrame({\n",
    "    \"AMT_BALANCE\": credit_card.groupby(\"SK_ID_PREV\").AMT_BALANCE.mean(),\n",
    "    \"SK_DPD\":      credit_card.groupby(\"SK_ID_PREV\").SK_DPD.max(),\n",
    "    \"SK_DPD_DEF\":  credit_card.groupby(\"SK_ID_PREV\").SK_DPD_DEF.max(),\n",
    "    \"N\":           credit_card.groupby(\"SK_ID_PREV\").count().iloc[:,0]\n",
    "})\n",
    "\n",
    "### Categorical column\n",
    "temp = get_dummies(credit_card, [\"NAME_CONTRACT_STATUS\"])\n",
    "cols = ['NAME_CONTRACT_STATUS_Active',\n",
    "       'NAME_CONTRACT_STATUS_Completed', 'NAME_CONTRACT_STATUS_Demand',\n",
    "       'NAME_CONTRACT_STATUS_Sent proposal', 'NAME_CONTRACT_STATUS_Signed']\n",
    "for col in cols:\n",
    "    temp[col] = temp[col] / (temp[\"MONTHS_BALANCE\"] - 1)\n",
    "cols.extend([\"SK_ID_PREV\"])\n",
    "temp = temp[cols]\n",
    "temp = temp.groupby(\"SK_ID_PREV\").sum()\n",
    "\n",
    "# Merge categorical and numerical df\n",
    "merge_df = temp.join(merge_df)\n",
    "\n",
    "### Add the rest of the columns\n",
    "right = credit_card.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['credit_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_credit\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del credit_card, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"misc\"></a>\n",
    "\n",
    "# [^](#toc) <u>Miscellaneous clean up</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Drop unneeded ID columns\n",
    "prev_app = prev_app.drop(\"SK_ID_PREV\", axis=1)\n",
    "bureau   = bureau.drop(\"SK_ID_BUREAU\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"final_merge\"></a>\n",
    "\n",
    "# [^](#toc) <u>Final Data Prep</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (307511, 122)\n",
      "Shape of test: (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "test  = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "\n",
    "print(\"Shape of train:\", train.shape)\n",
    "print(\"Shape of test:\",  test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into predictors, target, and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train.TARGET\n",
    "train_x = train.drop([\"TARGET\"], axis=1)\n",
    "\n",
    "test_id = test.SK_ID_CURR\n",
    "test_x  = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full    = pd.concat([train_x, test_x])\n",
    "train_N = len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final_process\"></a>\n",
    "\n",
    "### [^](#toc) <u>Data Processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Replace maxed values with NaN\n",
    "full['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "# ### Fill in outlier values\n",
    "# full[\"CODE_GENDER\"]        = full[\"CODE_GENDER\"].map(lambda x: \"F\" if x == \"XNA\" else x)\n",
    "# full[\"NAME_FAMILY_STATUS\"] = full[\"NAME_FAMILY_STATUS\"].map(lambda x: \"Married\" if x == \"Unknown\" else x)\n",
    "\n",
    "# # NAME_INCOME_TYPE\n",
    "# cols = [\"Unemployed\", \"Student\", \"Businessman\", \"Maternity leave\"]\n",
    "# full[\"NAME_INCOME_TYPE\"] = full[\"NAME_INCOME_TYPE\"].map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# # ORGANIZATION_TYPE\n",
    "# cols = [\"Trade: type 4\", \"Trade: type 5\"]\n",
    "# full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Trade\" if x in cols else x)\n",
    "# cols = [\"Industry: type 13\", \"Industry: type 8\"]\n",
    "# full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Industry\" if x in cols else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_cat\"></a>\n",
    "\n",
    "### [^](#toc) Categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Get dummies\n",
    "# cols  = [\"WALLSMATERIAL_MODE\", \"NAME_TYPE_SUITE\", \"NAME_INCOME_TYPE\", \"NAME_FAMILY_STATUS\",\n",
    "#                \"NAME_HOUSING_TYPE\", \"OCCUPATION_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"ORGANIZATION_TYPE\",\n",
    "#                \"FONDKAPREMONT_MODE\", \"NAME_EDUCATION_TYPE\"]\n",
    "# full = get_dummies(full, cols)\n",
    "# full = full.drop(cols, axis=1)\n",
    "\n",
    "# ### Factorize the dataframe\n",
    "# cols = [\"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"FLAG_OWN_CAR\",\n",
    "#                \"FLAG_OWN_REALTY\", \"HOUSETYPE_MODE\", \"EMERGENCYSTATE_MODE\"]\n",
    "# full = factorize_df(full, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_feat\"></a>\n",
    "\n",
    "### [^](#toc) Create Features\n",
    "\n",
    "FIXME:\n",
    "\n",
    "Scale age and create age in year feature\n",
    "\n",
    "Monthly debt payments, alimony,living costs divided by monthy gross income\n",
    "\n",
    "Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 187.38it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 245.62it/s]\n"
     ]
    }
   ],
   "source": [
    "full['DAYS_EMPLOYED_PERC']  = full['DAYS_EMPLOYED']    / full['DAYS_BIRTH']\n",
    "full['INCOME_CREDIT_PERC']  = full['AMT_INCOME_TOTAL'] / full['AMT_CREDIT']\n",
    "full['INCOME_PER_PERSON']   = full['AMT_INCOME_TOTAL'] / full['CNT_FAM_MEMBERS']\n",
    "full['ANNUITY_INCOME_PERC'] = full['AMT_ANNUITY']      / full['AMT_INCOME_TOTAL']\n",
    "full['NUM_PROPERTY']        = full['FLAG_OWN_CAR']     + full['FLAG_OWN_REALTY']\n",
    "\n",
    "### Create feature marking number of enquires\n",
    "full[\"NUM_ENQUIRIES\"]       = np.zeros(len(full))\n",
    "for enquiry in tqdm((\"AMT_REQ_CREDIT_BUREAU_HOUR\", \"AMT_REQ_CREDIT_BUREAU_DAY\",\n",
    "                     \"AMT_REQ_CREDIT_BUREAU_WEEK\", \"AMT_REQ_CREDIT_BUREAU_MON\",\n",
    "                     \"AMT_REQ_CREDIT_BUREAU_QRT\",  \"AMT_REQ_CREDIT_BUREAU_YEAR\")):\n",
    "    full[\"NUM_ENQUIRIES\"] += full[enquiry]\n",
    "    \n",
    "### Create feature marking number of discrepancies\n",
    "full[\"DISCREPANCIES\"]       = np.zeros(len(full))\n",
    "for discrepancy in (\"REG_REGION_NOT_LIVE_REGION\", \"REG_REGION_NOT_WORK_REGION\",\n",
    "                    \"LIVE_REGION_NOT_WORK_REGION\", \"REG_CITY_NOT_LIVE_CITY\",\n",
    "                    \"REG_CITY_NOT_WORK_CITY\", \"LIVE_CITY_NOT_WORK_CITY\"):\n",
    "    full[\"DISCREPANCIES\"] += full[discrepancy]\n",
    "    \n",
    "### Create feature marking number of info provided\n",
    "full[\"PROVIDE_INFO\"]       = np.zeros(len(full))\n",
    "for info in (\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\", \"FLAG_WORK_PHONE\",\n",
    "             \"FLAG_PHONE\", \"FLAG_EMAIL\"):\n",
    "    full[\"PROVIDE_INFO\"] += full[info]\n",
    "\n",
    "### Create feature marking number of flags\n",
    "full[\"NUM_FLAGS\"]           = np.zeros(len(full))\n",
    "for flag in tqdm(('FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4',\n",
    "                 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7',\n",
    "                 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10',\n",
    "                 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13',\n",
    "                 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16',\n",
    "                 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19',\n",
    "                 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21')):\n",
    "    full[\"NUM_FLAGS\"] += full[flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_prev\"></a>\n",
    "\n",
    "### [^](#toc) Merge Previous Application with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected median of numerical columns\n",
      "Selected max of numerical columns\n",
      "Merged median and max\n",
      "Selected min of numerical columns\n",
      "Merged min with median and max\n",
      "Selected categorical columns\n",
      "Merged categorical and numerical\n",
      "Merged into full\n",
      "Marked missing values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = [\n",
    "        \"NAME_CONTRACT_TYPE\", \"WEEKDAY_APPR_PROCESS_START\",\n",
    "        \"FLAG_LAST_APPL_PER_CONTRACT\", \"NAME_CASH_LOAN_PURPOSE\",\n",
    "        \"NAME_CONTRACT_STATUS\", \"NAME_PAYMENT_TYPE\",\n",
    "        \"CODE_REJECT_REASON\", \"NAME_TYPE_SUITE\", \"NAME_CLIENT_TYPE\",\n",
    "        \"NAME_GOODS_CATEGORY\", \"NAME_PORTFOLIO\", \"NAME_PRODUCT_TYPE\",\n",
    "        \"CHANNEL_TYPE\", \"NAME_SELLER_INDUSTRY\", \"NAME_YIELD_GROUP\",\n",
    "        \"PRODUCT_COMBINATION\", \"SK_ID_CURR\"]\n",
    "min_max_cols = ['AMT_ANNUITY', 'AMT_APPLICATION', 'AMT_CREDIT',\n",
    "                 'AMT_DOWN_PAYMENT', 'AMT_GOODS_PRICE', 'HOUR_APPR_PROCESS_START',\n",
    "                 'NFLAG_LAST_APPL_IN_DAY', 'RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY',\n",
    "                 'RATE_INTEREST_PRIVILEGED', 'DAYS_DECISION', 'SELLERPLACE_AREA',\n",
    "                 'CNT_PAYMENT', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE',\n",
    "                 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION',\n",
    "                 'NFLAG_INSURED_ON_APPROVAL', 'APP_CREDIT_PERC', \"SK_ID_CURR\"]\n",
    "num_cols = [col for col in prev_app.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "### numerical columns - median\n",
    "merge_df         = prev_app[num_cols].groupby('SK_ID_CURR').median()\n",
    "merge_df.columns = [\"MED_\" + col for col in merge_df.columns]\n",
    "print(\"Selected median of numerical columns\")\n",
    "\n",
    "### numerical columns - max\n",
    "right         = prev_app[min_max_cols].groupby(\"SK_ID_CURR\").max()\n",
    "right.columns = [\"MAX_\" + col for col in right.columns]\n",
    "print(\"Selected max of numerical columns\")\n",
    "\n",
    "### Merge median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged median and max\")\n",
    "\n",
    "### numerical columns - min\n",
    "right = prev_app[min_max_cols].groupby(\"SK_ID_CURR\").min()\n",
    "right.columns = [\"MIN_\" + col for col in right.columns]\n",
    "print(\"Selected min of numerical columns\")\n",
    "\n",
    "### Merge min with median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged min with median and max\")\n",
    "\n",
    "### Categorical columns\n",
    "right = prev_app[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "print(\"Selected categorical columns\")\n",
    "\n",
    "### Merge categorical and numerical\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged categorical and numerical\")\n",
    "\n",
    "### Prefix column names\n",
    "merge_df[\"N\"]    = prev_app.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "merged_cols      = ['p_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')\n",
    "print(\"Merged into full\")\n",
    "\n",
    "# Mark missing values\n",
    "full[\"no_prev_app\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "print(\"Marked missing values\")\n",
    "\n",
    "### Delete old variables\n",
    "del merge_df, merged_cols, right, cat_cols, num_cols\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau\"></a>\n",
    "\n",
    "### [^](#toc) Merge Bureau with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected median of numerical columns\n",
      "Selected max of numerical columns\n",
      "Merged median and max\n",
      "Selected min of numerical columns\n",
      "Merged min with median and max\n",
      "Selected categorical columns\n",
      "Merged categorical and numerical\n",
      "Merged into full\n",
      "Marked missing values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE', 'SK_ID_CURR']\n",
    "num_cols = [col for col in bureau.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "### Numeric columns - median\n",
    "merge_df         = bureau[num_cols].groupby('SK_ID_CURR').median()\n",
    "merge_df.columns = [\"MED_\" + col for col in merge_df.columns]\n",
    "print(\"Selected median of numerical columns\")\n",
    "\n",
    "### Numeric columns - max\n",
    "right         = bureau[num_cols].groupby(\"SK_ID_CURR\").max()\n",
    "right.columns = [\"MAX_\" + col for col in right.columns]\n",
    "print(\"Selected max of numerical columns\")\n",
    "\n",
    "### Merge median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged median and max\")\n",
    "\n",
    "### Numeric columns - min\n",
    "right = bureau[num_cols].groupby(\"SK_ID_CURR\").min()\n",
    "right.columns = [\"MIN_\" + col for col in right.columns]\n",
    "print(\"Selected min of numerical columns\")\n",
    "\n",
    "### Merge min with median and max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged min with median and max\")\n",
    "\n",
    "### Categorical columns\n",
    "right = bureau[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum()\n",
    "right[\"NUM_UNQ_CREDIT\"] = bureau.groupby(\"SK_ID_CURR\")[\"CREDIT_TYPE\"].nunique()\n",
    "print(\"Selected categorical columns\")\n",
    "\n",
    "### Merge categorical and numeric\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "print(\"Merged categorical and numerical\")\n",
    "\n",
    "### Prefix column names\n",
    "merge_df[\"N\"] = bureau.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "merged_cols      = ['b_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')\n",
    "print(\"Merged into full\")\n",
    "\n",
    "# Mark missing values\n",
    "full[\"no_bureau\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "print(\"Marked missing values\")\n",
    "\n",
    "### Delete old variables\n",
    "del merge_df, merged_cols, right, cat_cols, num_cols\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = full.drop(\"SK_ID_CURR\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorize and save Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = [col for col in full.columns if full[col].dtype == object]\n",
    "full     = factorize_df(full, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split full back into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE</th>\n",
       "      <th>...</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for the purchase of equipment</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for working capital replenishment</th>\n",
       "      <th>b_CREDIT_TYPE_Microloan</th>\n",
       "      <th>b_CREDIT_TYPE_Mobile operator loan</th>\n",
       "      <th>b_CREDIT_TYPE_Mortgage</th>\n",
       "      <th>b_CREDIT_TYPE_Real estate loan</th>\n",
       "      <th>b_CREDIT_TYPE_Unknown type of loan</th>\n",
       "      <th>b_NUM_UNQ_CREDIT</th>\n",
       "      <th>b_N</th>\n",
       "      <th>no_bureau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0                   0            0             0                0   \n",
       "1                   0            1             0                1   \n",
       "2                   1            0             1                0   \n",
       "3                   0            1             0                0   \n",
       "4                   0            0             0                0   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "0             0          202500.0    406597.5      24700.5         351000.0   \n",
       "1             0          270000.0   1293502.5      35698.5        1129500.0   \n",
       "2             0           67500.0    135000.0       6750.0         135000.0   \n",
       "3             0          135000.0    312682.5      29686.5         297000.0   \n",
       "4             0          121500.0    513000.0      21865.5         513000.0   \n",
       "\n",
       "   NAME_TYPE_SUITE    ...      \\\n",
       "0                0    ...       \n",
       "1                1    ...       \n",
       "2                0    ...       \n",
       "3                0    ...       \n",
       "4                0    ...       \n",
       "\n",
       "   b_CREDIT_TYPE_Loan for the purchase of equipment  \\\n",
       "0                                               0.0   \n",
       "1                                               0.0   \n",
       "2                                               0.0   \n",
       "3                                               NaN   \n",
       "4                                               0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Loan for working capital replenishment  \\\n",
       "0                                                0.0      \n",
       "1                                                0.0      \n",
       "2                                                0.0      \n",
       "3                                                NaN      \n",
       "4                                                0.0      \n",
       "\n",
       "   b_CREDIT_TYPE_Microloan  b_CREDIT_TYPE_Mobile operator loan  \\\n",
       "0                      0.0                                 0.0   \n",
       "1                      0.0                                 0.0   \n",
       "2                      0.0                                 0.0   \n",
       "3                      NaN                                 NaN   \n",
       "4                      0.0                                 0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Mortgage  b_CREDIT_TYPE_Real estate loan  \\\n",
       "0                     0.0                             0.0   \n",
       "1                     0.0                             0.0   \n",
       "2                     0.0                             0.0   \n",
       "3                     NaN                             NaN   \n",
       "4                     0.0                             0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Unknown type of loan  b_NUM_UNQ_CREDIT  b_N  no_bureau  \n",
       "0                                 0.0               2.0  8.0          0  \n",
       "1                                 0.0               2.0  4.0          0  \n",
       "2                                 0.0               1.0  2.0          0  \n",
       "3                                 NaN               NaN  NaN          1  \n",
       "4                                 0.0               1.0  1.0          0  \n",
       "\n",
       "[5 rows x 491 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = full[:train_N]\n",
    "test_x = full[train_N:]\n",
    "\n",
    "### Processed data look\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# [^](#toc) <u>Modeling</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics         import roc_auc_score, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm                import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat_reduction\"></a>\n",
    "\n",
    "### [^](#toc) Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[200]\tvalid_0's auc: 0.740518\n",
      "[400]\tvalid_0's auc: 0.758108\n",
      "[600]\tvalid_0's auc: 0.7682\n",
      "[800]\tvalid_0's auc: 0.774756\n",
      "[1000]\tvalid_0's auc: 0.778598\n",
      "[1200]\tvalid_0's auc: 0.780772\n",
      "[1400]\tvalid_0's auc: 0.782256\n",
      "[1600]\tvalid_0's auc: 0.783186\n",
      "[1800]\tvalid_0's auc: 0.783862\n",
      "[2000]\tvalid_0's auc: 0.784419\n",
      "[2200]\tvalid_0's auc: 0.784848\n",
      "[2400]\tvalid_0's auc: 0.785169\n",
      "[2600]\tvalid_0's auc: 0.78544\n",
      "[2800]\tvalid_0's auc: 0.785672\n",
      "[3000]\tvalid_0's auc: 0.785815\n",
      "[3200]\tvalid_0's auc: 0.785878\n",
      "[3400]\tvalid_0's auc: 0.785963\n",
      "[3600]\tvalid_0's auc: 0.786049\n",
      "[3800]\tvalid_0's auc: 0.786095\n",
      "Early stopping, best iteration is:\n",
      "[3773]\tvalid_0's auc: 0.786124\n",
      "Training took 1753 seconds\n"
     ]
    }
   ],
   "source": [
    "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)\n",
    "\n",
    "lgb_train = lgb.Dataset(data=training_x, label=training_y, categorical_feature=cat_cols)\n",
    "lgb_eval  = lgb.Dataset(data=val_x, label=val_y, categorical_feature=cat_cols)\n",
    "\n",
    "# try feature_fraction\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.03, 'num_leaves': 55, 'num_iteration': 2000, 'verbose': 0 ,\n",
    "          'subsample':.9, 'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "          'min_split_gain':.05, 'min_child_weight':1, \"min_data_in_leaf\": 40,\n",
    "          \"feature_fraction\":0.5}\n",
    "\n",
    "start = time.time()\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"important_feats\"></a>\n",
    "\n",
    "### [^](#toc) Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATS = 350\n",
    "\n",
    "feats = sorted(list(zip(model.feature_importance(), train_x.columns)))\n",
    "feats = list(list(zip(*feats[-NUM_FEATS:]))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"param_tuning\"></a>\n",
    "\n",
    "### [^](#toc) Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD \n",
    "\n",
    "lambda tuning (L2 regularization)\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "lambda = 10, num_iter = 2500\n",
    "- [2500] train: 0.871737, test: 0.78456\n",
    "\n",
    "lambda = 20, num_iter = 5000\n",
    "- [3270] train: 0.882077, test: 0.785751\n",
    "\n",
    "lambda = 40, num_iter = 3000\n",
    "- [3000] train: 0.868533, test: 0.78583\n",
    "\n",
    "##### Implemented random_state=17\n",
    "\n",
    "lambda = 80, num_iter = 3000\n",
    "- [3000] train: 0.859416, test: 0.785235\n",
    "\n",
    "lambda = 160, num_iter = 4000\n",
    "- [3789] train: 0.86189, test: 0.785736\n",
    "\n",
    "lambda = 0.1, num_iter = 4000\n",
    "- [2603] train: 0.895891, test: 0.784205\n",
    "\n",
    "##### change max_depth from 6 to 7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Other variables\n",
    "--------------\n",
    "\n",
    "learning_rate = 0.01,\n",
    "num_leaves = 48,\n",
    "colsample_bytree = 0.8,\n",
    "subsample = 0.9,\n",
    "max_depth = 6,\n",
    "reg_alpha = 0.1,\n",
    "min_split_gain = 0.01,\n",
    "min_child_weight = 1,\n",
    "\n",
    "</div>\n",
    "\n",
    "max_depth tuning\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "max_depth = 6, num_iter = 3000\n",
    "- [3000] train: 0.859416, test: 0.785235\n",
    "\n",
    "max_depth = 7, num_iter = 4000\n",
    "- [3415] train: 0.878744, test: 0.786247\n",
    "\n",
    "max_depth = 8, num_iter = 4000\n",
    "- [2965] train: 0.875784, test: 0.786024\n",
    "\n",
    "##### Change reg_alpha from 0.1 to 40\n",
    "\n",
    "max_depth = 8, num_iter = 4000\n",
    "- [3420] train: 0.858222, test: 0.785642\n",
    "\n",
    "max_depth = 9, num_iter = 4000\n",
    "- [] train: , test: \n",
    "\n",
    "max_depth = 10, num_iter = 4000\n",
    "- [] train: , test: \n",
    "\n",
    "Other variables\n",
    "--------------\n",
    "\n",
    "learning_rate = 0.01,\n",
    "num_leaves = 48,\n",
    "colsample_bytree = 0.8,\n",
    "subsample = 0.9,\n",
    "reg_alpha = 0.1,\n",
    "reg_lambda = 80,\n",
    "min_split_gain = 0.01,\n",
    "min_child_weight = 1,\n",
    "random_state=17\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16062018 - Reduce Overfitting 1\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "Starting params\n",
    "\n",
    "'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "'subsample':.9, 'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "'min_split_gain':.05, 'min_child_weight':1, \"min_data_in_leaf\": 40,\n",
    "\"bagging_freq\": 4, \"bagging_fraction\":0.5\n",
    "\n",
    "___Initial___\n",
    "\n",
    " - Validation: 0.786\n",
    " - Test:       0.779\n",
    "\n",
    "Remove bagging_freq (=4), bagging_fraction (=0.5), add feature_fraction (=0.5).\n",
    "\n",
    " - Validation: 0.789602\n",
    " - Test:       0.785184954089\n",
    " \n",
    "2nd run\n",
    "\n",
    " - Validation: 0.789004\n",
    " - Test:       0.784571125086\n",
    " \n",
    "Change min_data_in_leaf (40 --> 60)\n",
    "\n",
    " - Validation: 0.789002\n",
    " - Test:       0.784891310112\n",
    " \n",
    "Change min_data_in_leaf (60 --> 20)\n",
    "\n",
    " - Validation: 0.788626\n",
    " - Test:       0.78465452505\n",
    " \n",
    "Change min_data_in_leaf (20 --> 40) and feature_fraction (0.5 --> 0.6)\n",
    "\n",
    " - Validation: 0.788533\n",
    " - Test:       0.784679323774\n",
    " \n",
    "Change max_depth (7 --> 6) and feature_fraction (0.6 --> 0.5)\n",
    "\n",
    " - Validation: 0.788849\n",
    " - Test:       0.784880849041\n",
    " \n",
    "Change learning_rate (0.01 --> 0.03) and max_depth (6 --> 7)\n",
    "\n",
    " - Validation: 0.788195\n",
    " - Test:       0.784589015086\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16062018 - Reduce Overfitting 2\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "Starting params\n",
    "\n",
    "'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "'subsample':0.9, 'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "'min_split_gain':0.05, 'min_child_weight':1, \"min_data_in_leaf\": 40,\n",
    "\"feature_fraction\":0.5\n",
    "\n",
    "___Initial___\n",
    "\n",
    " - Validation: 0.788926\n",
    " - Test:       0.784734949879\n",
    " \n",
    "Change subsample (0.9 --> 0.7)\n",
    "\n",
    " - Validation: 0.788926\n",
    " - Test:       0.784734949879\n",
    " \n",
    "Change min_child_weight (1 --> 4) and remove subsample\n",
    "\n",
    " - Validation: 0.78953\n",
    " - Test:       0.785232093044\n",
    " \n",
    "Change min_split_gain (0.05 --> 0.1)\n",
    " \n",
    " - Validation: 0.789338\n",
    " - Test:       0.784930068808\n",
    " \n",
    "Change min_child_weight (4 --> 10) and min_split_gain (0.1 --> 0.05)\n",
    " \n",
    " - Validation: 0.788998\n",
    " - Test:       0.784947160688\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.735437\n",
      "[200]\tvalid_0's auc: 0.747542\n",
      "[300]\tvalid_0's auc: 0.756961\n",
      "[400]\tvalid_0's auc: 0.763622\n",
      "[500]\tvalid_0's auc: 0.769285\n",
      "[600]\tvalid_0's auc: 0.773791\n",
      "[700]\tvalid_0's auc: 0.776977\n",
      "[800]\tvalid_0's auc: 0.77961\n",
      "[900]\tvalid_0's auc: 0.781509\n",
      "[1000]\tvalid_0's auc: 0.782992\n",
      "[1100]\tvalid_0's auc: 0.784065\n",
      "[1200]\tvalid_0's auc: 0.785\n",
      "[1300]\tvalid_0's auc: 0.785791\n",
      "[1400]\tvalid_0's auc: 0.786315\n",
      "[1500]\tvalid_0's auc: 0.786691\n",
      "[1600]\tvalid_0's auc: 0.787025\n",
      "[1700]\tvalid_0's auc: 0.787238\n",
      "[1800]\tvalid_0's auc: 0.787481\n",
      "[1900]\tvalid_0's auc: 0.787766\n",
      "[2000]\tvalid_0's auc: 0.787977\n",
      "[2100]\tvalid_0's auc: 0.788174\n",
      "[2200]\tvalid_0's auc: 0.788328\n",
      "[2300]\tvalid_0's auc: 0.788517\n",
      "[2400]\tvalid_0's auc: 0.788673\n",
      "[2500]\tvalid_0's auc: 0.788729\n",
      "[2600]\tvalid_0's auc: 0.788802\n",
      "[2700]\tvalid_0's auc: 0.788826\n",
      "[2800]\tvalid_0's auc: 0.788905\n",
      "[2900]\tvalid_0's auc: 0.788972\n",
      "[3000]\tvalid_0's auc: 0.788943\n",
      "[3100]\tvalid_0's auc: 0.788922\n",
      "Early stopping, best iteration is:\n",
      "[2952]\tvalid_0's auc: 0.788998\n",
      "Training took 859 seconds\n",
      "Testing score: 0.784947160688\n"
     ]
    }
   ],
   "source": [
    "training_x, testing_x, training_y, testing_y = train_test_split(train_x[feats], train_y, test_size=0.2, random_state=17)\n",
    "training_x, val_x, training_y, val_y = train_test_split(training_x, training_y, test_size=0.25, random_state=17)\n",
    "\n",
    "lgb_train = lgb.Dataset(data=training_x,\n",
    "                        label=training_y,\n",
    "                        categorical_feature=cat_cols)\n",
    "lgb_eval  = lgb.Dataset(data=val_x,\n",
    "                        label=val_y,\n",
    "                        categorical_feature=cat_cols)\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "          'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "          'min_split_gain':0.05, 'min_child_weight':4, \"min_data_in_leaf\": 40,\n",
    "          \"feature_fraction\":0.5}\n",
    "\n",
    "start = time.time()\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=100)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))\n",
    "\n",
    "prediction = model.predict(testing_x)\n",
    "\n",
    "score = roc_auc_score(testing_y, prediction)\n",
    "\n",
    "print(\"Testing score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cv\"></a>\n",
    "\n",
    "### [^](#toc) CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's auc: 0.734262 + 0.0013434\n",
      "[200]\tcv_agg's auc: 0.746387 + 0.00153842\n",
      "[300]\tcv_agg's auc: 0.755584 + 0.00145228\n",
      "[400]\tcv_agg's auc: 0.762399 + 0.00152256\n",
      "[500]\tcv_agg's auc: 0.767869 + 0.00156887\n",
      "[600]\tcv_agg's auc: 0.771965 + 0.00153819\n",
      "[700]\tcv_agg's auc: 0.775268 + 0.00145693\n",
      "[800]\tcv_agg's auc: 0.777931 + 0.00136613\n",
      "[900]\tcv_agg's auc: 0.779935 + 0.00133051\n",
      "[1000]\tcv_agg's auc: 0.781432 + 0.00134249\n",
      "[1100]\tcv_agg's auc: 0.7826 + 0.00137212\n",
      "[1200]\tcv_agg's auc: 0.783581 + 0.0013923\n",
      "[1300]\tcv_agg's auc: 0.784306 + 0.0013814\n",
      "[1400]\tcv_agg's auc: 0.784915 + 0.00140205\n",
      "[1500]\tcv_agg's auc: 0.785411 + 0.00139166\n",
      "[1600]\tcv_agg's auc: 0.785825 + 0.00137735\n",
      "[1700]\tcv_agg's auc: 0.786204 + 0.00137201\n",
      "[1800]\tcv_agg's auc: 0.786534 + 0.00133517\n",
      "[1900]\tcv_agg's auc: 0.786841 + 0.00131041\n",
      "[2000]\tcv_agg's auc: 0.78711 + 0.00128903\n",
      "[2100]\tcv_agg's auc: 0.787321 + 0.00125792\n",
      "[2200]\tcv_agg's auc: 0.7875 + 0.00123477\n",
      "[2300]\tcv_agg's auc: 0.787682 + 0.00120538\n",
      "[2400]\tcv_agg's auc: 0.787835 + 0.00119471\n",
      "[2500]\tcv_agg's auc: 0.788002 + 0.00119654\n",
      "[2600]\tcv_agg's auc: 0.78813 + 0.00116669\n",
      "[2700]\tcv_agg's auc: 0.78825 + 0.00116981\n",
      "[2800]\tcv_agg's auc: 0.788325 + 0.00119421\n",
      "[2900]\tcv_agg's auc: 0.788384 + 0.00119514\n",
      "[3000]\tcv_agg's auc: 0.788516 + 0.00119209\n",
      "[3100]\tcv_agg's auc: 0.788581 + 0.00117378\n",
      "[3200]\tcv_agg's auc: 0.788634 + 0.00116462\n",
      "[3300]\tcv_agg's auc: 0.788659 + 0.00117742\n",
      "[3400]\tcv_agg's auc: 0.788696 + 0.0011914\n",
      "[3500]\tcv_agg's auc: 0.788732 + 0.00115333\n",
      "[3600]\tcv_agg's auc: 0.788784 + 0.00115868\n",
      "[3700]\tcv_agg's auc: 0.788786 + 0.00118997\n",
      "[3800]\tcv_agg's auc: 0.788767 + 0.00116952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78879859253631179"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(train_x, train_y, usecols, params, dropcols=[]):  \n",
    "    dtrain = lgb.Dataset(train_x[usecols].drop(dropcols, axis=1),\n",
    "                         train_y,\n",
    "                         categorical_feature=cat_cols)\n",
    "    \n",
    "    eval = lgb.cv(params,\n",
    "             dtrain,\n",
    "             nfold=5,\n",
    "             stratified=True,\n",
    "             num_boost_round=20000,\n",
    "             early_stopping_rounds=200,\n",
    "             verbose_eval=100,\n",
    "             seed = 5,\n",
    "             show_stdv=True)\n",
    "    return max(eval['auc-mean'])\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "          'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "          'min_split_gain':0.05, 'min_child_weight':4, \"min_data_in_leaf\": 40,\n",
    "          \"feature_fraction\":0.5,}\n",
    "    \n",
    "get_score(train_x, train_y, feats, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"final\"></a>\n",
    "\n",
    "# [^](#toc) <u>Final submission</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.728036\n",
      "[200]\tvalid_0's auc: 0.739861\n",
      "[300]\tvalid_0's auc: 0.749731\n",
      "[400]\tvalid_0's auc: 0.757379\n",
      "[500]\tvalid_0's auc: 0.763477\n",
      "[600]\tvalid_0's auc: 0.768177\n",
      "[700]\tvalid_0's auc: 0.77187\n",
      "[800]\tvalid_0's auc: 0.77496\n",
      "[900]\tvalid_0's auc: 0.777106\n",
      "[1000]\tvalid_0's auc: 0.778788\n",
      "[1100]\tvalid_0's auc: 0.77996\n",
      "[1200]\tvalid_0's auc: 0.780998\n",
      "[1300]\tvalid_0's auc: 0.781803\n",
      "[1400]\tvalid_0's auc: 0.782387\n",
      "[1500]\tvalid_0's auc: 0.782914\n",
      "[1600]\tvalid_0's auc: 0.783389\n",
      "[1700]\tvalid_0's auc: 0.783822\n",
      "[1800]\tvalid_0's auc: 0.784147\n",
      "[1900]\tvalid_0's auc: 0.784533\n",
      "[2000]\tvalid_0's auc: 0.784751\n",
      "[2100]\tvalid_0's auc: 0.784945\n",
      "[2200]\tvalid_0's auc: 0.785191\n",
      "[2300]\tvalid_0's auc: 0.785439\n",
      "[2400]\tvalid_0's auc: 0.785655\n",
      "[2500]\tvalid_0's auc: 0.785796\n",
      "[2600]\tvalid_0's auc: 0.78597\n",
      "[2700]\tvalid_0's auc: 0.786039\n",
      "[2800]\tvalid_0's auc: 0.786111\n",
      "[2900]\tvalid_0's auc: 0.786182\n",
      "[3000]\tvalid_0's auc: 0.78627\n",
      "[3100]\tvalid_0's auc: 0.786268\n",
      "[3200]\tvalid_0's auc: 0.786272\n",
      "[3300]\tvalid_0's auc: 0.786329\n",
      "[3400]\tvalid_0's auc: 0.786357\n",
      "[3500]\tvalid_0's auc: 0.78636\n",
      "Early stopping, best iteration is:\n",
      "[3435]\tvalid_0's auc: 0.786392\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.737014\n",
      "[200]\tvalid_0's auc: 0.749196\n",
      "[300]\tvalid_0's auc: 0.757985\n",
      "[400]\tvalid_0's auc: 0.764605\n",
      "[500]\tvalid_0's auc: 0.769387\n",
      "[600]\tvalid_0's auc: 0.772893\n",
      "[700]\tvalid_0's auc: 0.775779\n",
      "[800]\tvalid_0's auc: 0.77817\n",
      "[900]\tvalid_0's auc: 0.779728\n",
      "[1000]\tvalid_0's auc: 0.781007\n",
      "[1100]\tvalid_0's auc: 0.782099\n",
      "[1200]\tvalid_0's auc: 0.783027\n",
      "[1300]\tvalid_0's auc: 0.783736\n",
      "[1400]\tvalid_0's auc: 0.784314\n",
      "[1500]\tvalid_0's auc: 0.784739\n",
      "[1600]\tvalid_0's auc: 0.785174\n",
      "[1700]\tvalid_0's auc: 0.785593\n",
      "[1800]\tvalid_0's auc: 0.785976\n",
      "[1900]\tvalid_0's auc: 0.786243\n",
      "[2000]\tvalid_0's auc: 0.786445\n",
      "[2100]\tvalid_0's auc: 0.786617\n",
      "[2200]\tvalid_0's auc: 0.786746\n",
      "[2300]\tvalid_0's auc: 0.78687\n",
      "[2400]\tvalid_0's auc: 0.787039\n",
      "[2500]\tvalid_0's auc: 0.787151\n",
      "[2600]\tvalid_0's auc: 0.787263\n",
      "[2700]\tvalid_0's auc: 0.787403\n",
      "[2800]\tvalid_0's auc: 0.787472\n",
      "[2900]\tvalid_0's auc: 0.787562\n",
      "[3000]\tvalid_0's auc: 0.787556\n",
      "[3100]\tvalid_0's auc: 0.787625\n",
      "[3200]\tvalid_0's auc: 0.787728\n",
      "[3300]\tvalid_0's auc: 0.787784\n",
      "[3400]\tvalid_0's auc: 0.787786\n",
      "[3500]\tvalid_0's auc: 0.787793\n",
      "[3600]\tvalid_0's auc: 0.787853\n",
      "[3700]\tvalid_0's auc: 0.787843\n",
      "[3800]\tvalid_0's auc: 0.787859\n",
      "Early stopping, best iteration is:\n",
      "[3725]\tvalid_0's auc: 0.78789\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.734875\n",
      "[200]\tvalid_0's auc: 0.746338\n",
      "[300]\tvalid_0's auc: 0.755202\n",
      "[400]\tvalid_0's auc: 0.762084\n",
      "[500]\tvalid_0's auc: 0.767857\n",
      "[600]\tvalid_0's auc: 0.77206\n",
      "[700]\tvalid_0's auc: 0.775288\n",
      "[800]\tvalid_0's auc: 0.777997\n",
      "[900]\tvalid_0's auc: 0.779928\n",
      "[1000]\tvalid_0's auc: 0.781459\n",
      "[1100]\tvalid_0's auc: 0.782598\n",
      "[1200]\tvalid_0's auc: 0.783634\n",
      "[1300]\tvalid_0's auc: 0.784383\n",
      "[1400]\tvalid_0's auc: 0.785057\n",
      "[1500]\tvalid_0's auc: 0.785576\n",
      "[1600]\tvalid_0's auc: 0.786013\n",
      "[1700]\tvalid_0's auc: 0.786428\n",
      "[1800]\tvalid_0's auc: 0.786801\n",
      "[1900]\tvalid_0's auc: 0.787145\n",
      "[2000]\tvalid_0's auc: 0.78743\n",
      "[2100]\tvalid_0's auc: 0.787672\n",
      "[2200]\tvalid_0's auc: 0.787935\n",
      "[2300]\tvalid_0's auc: 0.788102\n",
      "[2400]\tvalid_0's auc: 0.788276\n",
      "[2500]\tvalid_0's auc: 0.788365\n",
      "[2600]\tvalid_0's auc: 0.788474\n",
      "[2700]\tvalid_0's auc: 0.788577\n",
      "[2800]\tvalid_0's auc: 0.78872\n",
      "[2900]\tvalid_0's auc: 0.788802\n",
      "[3000]\tvalid_0's auc: 0.78888\n",
      "[3100]\tvalid_0's auc: 0.788989\n",
      "[3200]\tvalid_0's auc: 0.789102\n",
      "[3300]\tvalid_0's auc: 0.789142\n",
      "[3400]\tvalid_0's auc: 0.78923\n",
      "[3500]\tvalid_0's auc: 0.78926\n",
      "[3600]\tvalid_0's auc: 0.789279\n",
      "[3700]\tvalid_0's auc: 0.789298\n",
      "[3800]\tvalid_0's auc: 0.789363\n",
      "[3900]\tvalid_0's auc: 0.789356\n",
      "Early stopping, best iteration is:\n",
      "[3829]\tvalid_0's auc: 0.78937\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.736687\n",
      "[200]\tvalid_0's auc: 0.748429\n",
      "[300]\tvalid_0's auc: 0.757692\n",
      "[400]\tvalid_0's auc: 0.764143\n",
      "[500]\tvalid_0's auc: 0.769098\n",
      "[600]\tvalid_0's auc: 0.773249\n",
      "[700]\tvalid_0's auc: 0.776606\n",
      "[800]\tvalid_0's auc: 0.779423\n",
      "[900]\tvalid_0's auc: 0.781282\n",
      "[1000]\tvalid_0's auc: 0.782798\n",
      "[1100]\tvalid_0's auc: 0.783984\n",
      "[1200]\tvalid_0's auc: 0.784796\n",
      "[1300]\tvalid_0's auc: 0.78548\n",
      "[1400]\tvalid_0's auc: 0.786005\n",
      "[1500]\tvalid_0's auc: 0.786448\n",
      "[1600]\tvalid_0's auc: 0.786825\n",
      "[1700]\tvalid_0's auc: 0.787215\n",
      "[1800]\tvalid_0's auc: 0.787553\n",
      "[1900]\tvalid_0's auc: 0.787807\n",
      "[2000]\tvalid_0's auc: 0.788054\n",
      "[2100]\tvalid_0's auc: 0.788355\n",
      "[2200]\tvalid_0's auc: 0.788546\n",
      "[2300]\tvalid_0's auc: 0.788703\n",
      "[2400]\tvalid_0's auc: 0.78877\n",
      "[2500]\tvalid_0's auc: 0.788918\n",
      "[2600]\tvalid_0's auc: 0.789041\n",
      "[2700]\tvalid_0's auc: 0.789191\n",
      "[2800]\tvalid_0's auc: 0.789259\n",
      "[2900]\tvalid_0's auc: 0.78934\n",
      "[3000]\tvalid_0's auc: 0.789414\n",
      "[3100]\tvalid_0's auc: 0.789471\n",
      "[3200]\tvalid_0's auc: 0.789501\n",
      "[3300]\tvalid_0's auc: 0.789546\n",
      "[3400]\tvalid_0's auc: 0.789552\n",
      "[3500]\tvalid_0's auc: 0.789587\n",
      "[3600]\tvalid_0's auc: 0.789566\n",
      "Early stopping, best iteration is:\n",
      "[3459]\tvalid_0's auc: 0.789608\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\tvalid_0's auc: 0.736003\n",
      "[200]\tvalid_0's auc: 0.747963\n",
      "[300]\tvalid_0's auc: 0.757162\n",
      "[400]\tvalid_0's auc: 0.763972\n",
      "[500]\tvalid_0's auc: 0.7696\n",
      "[600]\tvalid_0's auc: 0.773966\n",
      "[700]\tvalid_0's auc: 0.77735\n",
      "[800]\tvalid_0's auc: 0.780231\n",
      "[900]\tvalid_0's auc: 0.782305\n",
      "[1000]\tvalid_0's auc: 0.783848\n",
      "[1100]\tvalid_0's auc: 0.784945\n",
      "[1200]\tvalid_0's auc: 0.785965\n",
      "[1300]\tvalid_0's auc: 0.786753\n",
      "[1400]\tvalid_0's auc: 0.787381\n",
      "[1500]\tvalid_0's auc: 0.787869\n",
      "[1600]\tvalid_0's auc: 0.788293\n",
      "[1700]\tvalid_0's auc: 0.788625\n",
      "[1800]\tvalid_0's auc: 0.789036\n",
      "[1900]\tvalid_0's auc: 0.789445\n",
      "[2000]\tvalid_0's auc: 0.789767\n",
      "[2100]\tvalid_0's auc: 0.789901\n",
      "[2200]\tvalid_0's auc: 0.79015\n",
      "[2300]\tvalid_0's auc: 0.790331\n",
      "[2400]\tvalid_0's auc: 0.790503\n",
      "[2500]\tvalid_0's auc: 0.790644\n",
      "[2600]\tvalid_0's auc: 0.790711\n",
      "[2700]\tvalid_0's auc: 0.790778\n",
      "[2800]\tvalid_0's auc: 0.790895\n",
      "[2900]\tvalid_0's auc: 0.790993\n",
      "[3000]\tvalid_0's auc: 0.791048\n",
      "[3100]\tvalid_0's auc: 0.791097\n",
      "[3200]\tvalid_0's auc: 0.791155\n",
      "[3300]\tvalid_0's auc: 0.791146\n",
      "Early stopping, best iteration is:\n",
      "[3225]\tvalid_0's auc: 0.791173\n"
     ]
    }
   ],
   "source": [
    "folds       = KFold(n_splits=5, shuffle=True, random_state=17)\n",
    "predictions = np.zeros(test_x.shape[0])\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x)):\n",
    "    trn_x, trn_y = train_x[feats].iloc[trn_idx], train_y.iloc[trn_idx]\n",
    "    val_x, val_y = train_x[feats].iloc[val_idx], train_y.iloc[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(data=trn_x, label=trn_y, categorical_feature=cat_cols)\n",
    "    lgb_eval  = lgb.Dataset(data=val_x, label=val_y, categorical_feature=cat_cols)\n",
    "\n",
    "    params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "              'learning_rate': 0.01, 'num_leaves': 55, 'num_iteration': 4000, 'verbose': 0 ,\n",
    "              'max_depth':7, 'reg_alpha':20, 'reg_lambda':20, \n",
    "              'min_split_gain':0.05, 'min_child_weight':4, \"min_data_in_leaf\": 40,\n",
    "              \"feature_fraction\":0.5,\n",
    "              \"random_state\": random.randint(1, 100)}# Recommended to make the seed random\n",
    "\n",
    "\n",
    "    model = lgb.train(params, lgb_train, valid_sets=lgb_eval,\n",
    "                      early_stopping_rounds=150, verbose_eval=100) \n",
    "    \n",
    "    predictions += model.predict(test_x[feats]) / folds.n_splits\n",
    "    \n",
    "    print(\"\\n Done with {} fold\".format(n_fold + 1))\n",
    "    \n",
    "    del model, trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final_pred\"></a>\n",
    "\n",
    "### [^](#toc) Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"SK_ID_CURR\": test_id,\n",
    "    \"TARGET\": predictions\n",
    "}).to_csv(\"../submissions/more_tuning.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
