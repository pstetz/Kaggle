{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score: 0.785"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "# <u>Table of Contents</u>\n",
    "1.) [TODO](#todo)  \n",
    "2.) [Imports](#imports)  \n",
    "3.) [Bureau](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.1.) [Data Processing](#bureau_process)  \n",
    "4.) [Bureau Balance](#bureau_bal)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.1.) [Merge into Bureau](#merge_bureau_bal)  \n",
    "5.) [Previous Application](#prev_app)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.1.) [Data Processing](#prev_process)  \n",
    "6.) [POS CASH balance](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.1.) [Data Processing](#pos_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.2.) [Merge into Previous Application](#merge_pos_cash)  \n",
    "7.) [Installment Payments](#install_pay)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.1.) [Merge into Previous Application](#merge_install_pay)  \n",
    "8.) [Credit Card Balance](#credit)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.1.) [Data Processing](#credit_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.2.) [Merge into Previous Application](#merge_credit)  \n",
    "9.) [Miscellaneous clean up](#misc)  \n",
    "10.) [Final Data Prep](#final_merge)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.1.) [Data Processing](#final_process)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.2.) [Merge Previous Application with Full](#merge_prev)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 10.3.) [Merge Bureau with Full](#merge_bureau)  \n",
    "11.) [Modeling](#models)  \n",
    "12.) [Save file to CSV](#save)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todo\"></a>\n",
    "\n",
    "# [^](#toc) <u>TODO</u>\n",
    "\n",
    "- Fix skew on columns\n",
    "- Tinker with the best way to replace missing values (dropping cols?)\n",
    "- Look for outliers\n",
    "- Merge db together\n",
    "- Include timeline relatoinships like MONTHS_BALANCE\n",
    "- Tune model parameters\n",
    "- Address [this](https://www.kaggle.com/c/home-credit-default-risk/discussion/57248)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "# [^](#toc) <u>Imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Time keeper\n",
    "import time\n",
    "\n",
    "# Garbage collector\n",
    "import gc\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Removes warnings from output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dummies(df, cats):\n",
    "    for col in cats:\n",
    "        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "    return df \n",
    "\n",
    "def factorize_df(df, cats):\n",
    "    for col in cats:\n",
    "        df[col], _ = pd.factorize(df[col])\n",
    "    return df \n",
    "\n",
    "DATA_PATH = \"../data/home_default/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"bureau\"></a>\n",
    "\n",
    "# [^](#toc) Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau: (1716428, 17)\n",
      "\n",
      "Columns of bureau:\n",
      "SK_ID_CURR --- SK_ID_BUREAU --- CREDIT_ACTIVE --- CREDIT_CURRENCY --- DAYS_CREDIT --- CREDIT_DAY_OVERDUE --- DAYS_CREDIT_ENDDATE --- DAYS_ENDDATE_FACT --- AMT_CREDIT_MAX_OVERDUE --- CNT_CREDIT_PROLONG --- AMT_CREDIT_SUM --- AMT_CREDIT_SUM_DEBT --- AMT_CREDIT_SUM_LIMIT --- AMT_CREDIT_SUM_OVERDUE --- CREDIT_TYPE --- DAYS_CREDIT_UPDATE --- AMT_ANNUITY\n"
     ]
    }
   ],
   "source": [
    "bureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\n",
    "print(\"Shape of bureau:\", bureau.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau:\")\n",
    "print(\" --- \".join(bureau.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Lump together values with low counts\n",
    "# CREDIT_CURRENCY\n",
    "cols = [\"currency 3\", \"currency 4\"]\n",
    "bureau.CREDIT_CURRENCY = bureau.CREDIT_CURRENCY.map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# CREDIT_TYPE\n",
    "cols = [\"Cash loan (non-earmarked)\", \"Real estate loan\", \"Loan for the purchase of equipment\",\n",
    "        \"Loan for purchase of shares (margin lending)\", \"Interbank credit\", \"Mobile operator loan\"]\n",
    "bureau.CREDIT_TYPE = bureau.CREDIT_TYPE.map(lambda x: \"MISC\" if x in cols else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_bal\"></a>\n",
    "\n",
    "# [^](#toc) <u>Bureau Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau_balance: (27299925, 3)\n",
      "\n",
      "Columns of bureau_balance:\n",
      "SK_ID_BUREAU --- MONTHS_BALANCE --- STATUS\n"
     ]
    }
   ],
   "source": [
    "bureau_balance = pd.read_csv(DATA_PATH + \"bureau_balance.csv\")\n",
    "print(\"Shape of bureau_balance:\",  bureau_balance.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau_balance:\")\n",
    "print(\" --- \".join(bureau_balance.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau_bal\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Bureau</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get sum of counts in categorical column\n",
    "merge_df = get_dummies(bureau_balance, [\"STATUS\"])\n",
    "cols = ['STATUS_0', 'STATUS_1', 'STATUS_2', 'STATUS_3', 'STATUS_4', 'STATUS_5', 'STATUS_C', 'STATUS_X']\n",
    "for col in cols:\n",
    "    merge_df[col] = merge_df[col] / (merge_df[\"MONTHS_BALANCE\"] - 1)\n",
    "merge_df = merge_df.drop([\"MONTHS_BALANCE\", \"STATUS\"], axis=1)\n",
    "merge_df = merge_df.groupby(\"SK_ID_BUREAU\").sum().reset_index()\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = bureau_balance.groupby(\"SK_ID_BUREAU\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_BUREAU\").set_index(\"SK_ID_BUREAU\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['bur_bal_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "bureau = bureau.merge(right=merge_df.reset_index(), how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "# Mark missing values\n",
    "bureau[\"no_bureau_bal\"] = bureau[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del bureau_balance, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"prev_app\"></a>\n",
    "\n",
    "# [^](#toc) <u>Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prev_app: (1670214, 37)\n",
      "\n",
      "Columns of prev_app:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NAME_CONTRACT_TYPE --- AMT_ANNUITY --- AMT_APPLICATION --- AMT_CREDIT --- AMT_DOWN_PAYMENT --- AMT_GOODS_PRICE --- WEEKDAY_APPR_PROCESS_START --- HOUR_APPR_PROCESS_START --- FLAG_LAST_APPL_PER_CONTRACT --- NFLAG_LAST_APPL_IN_DAY --- RATE_DOWN_PAYMENT --- RATE_INTEREST_PRIMARY --- RATE_INTEREST_PRIVILEGED --- NAME_CASH_LOAN_PURPOSE --- NAME_CONTRACT_STATUS --- DAYS_DECISION --- NAME_PAYMENT_TYPE --- CODE_REJECT_REASON --- NAME_TYPE_SUITE --- NAME_CLIENT_TYPE --- NAME_GOODS_CATEGORY --- NAME_PORTFOLIO --- NAME_PRODUCT_TYPE --- CHANNEL_TYPE --- SELLERPLACE_AREA --- NAME_SELLER_INDUSTRY --- CNT_PAYMENT --- NAME_YIELD_GROUP --- PRODUCT_COMBINATION --- DAYS_FIRST_DRAWING --- DAYS_FIRST_DUE --- DAYS_LAST_DUE_1ST_VERSION --- DAYS_LAST_DUE --- DAYS_TERMINATION --- NFLAG_INSURED_ON_APPROVAL\n"
     ]
    }
   ],
   "source": [
    "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\n",
    "print(\"Shape of prev_app:\",  prev_app.shape)\n",
    "\n",
    "print(\"\\nColumns of prev_app:\")\n",
    "print(\" --- \".join(prev_app.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prev_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fill in values that should be null\n",
    "prev_app['DAYS_FIRST_DRAWING'       ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_FIRST_DUE'           ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_LAST_DUE'            ].replace(365243, np.nan, inplace= True)\n",
    "prev_app['DAYS_TERMINATION'         ].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "### Lump together values with low counts\n",
    "# NAME_GOODS_CATEGORY\n",
    "prev_app.NAME_GOODS_CATEGORY = prev_app.NAME_GOODS_CATEGORY.map(\n",
    "    lambda x: \"MISC\" if x in [\"Weapon\", \"Insurance\"] else x)\n",
    "\n",
    "# NAME_CASH_LOAN_PURPOSE\n",
    "prev_app.NAME_CASH_LOAN_PURPOSE = prev_app.NAME_CASH_LOAN_PURPOSE.map(\n",
    "    lambda x: \"MISC\" if x in [\"Buying a garage\", \"Misc\"] else x)\n",
    "\n",
    "# Create features\n",
    "prev_app[\"APP_CREDIT_PERC\"] = prev_app['AMT_APPLICATION'] / prev_app['AMT_CREDIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"pos_cash\"></a>\n",
    "\n",
    "# [^](#toc) <u>POS CASH balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pcb: (10001358, 8)\n",
      "\n",
      "Columns of pcb:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- CNT_INSTALMENT --- CNT_INSTALMENT_FUTURE --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "pcb = pd.read_csv(DATA_PATH + \"POS_CASH_balance.csv\")\n",
    "print(\"Shape of pcb:\",  pcb.shape)\n",
    "\n",
    "print(\"\\nColumns of pcb:\")\n",
    "print(\" --- \".join(pcb.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_process\"></a>\n",
    "\n",
    "### [^](#toc) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Outliers\n",
    "pcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_pos_cash\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get Dummies\n",
    "merge_df = pcb[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "merge_df = get_dummies(merge_df, [\"NAME_CONTRACT_STATUS\"])\n",
    "merge_df = merge_df.drop(\"NAME_CONTRACT_STATUS\", axis=1)\n",
    "\n",
    "# Prep for merge\n",
    "count    = merge_df.groupby(\"SK_ID_PREV\").count()\n",
    "merge_df = merge_df.groupby(\"SK_ID_PREV\").sum().reset_index()\n",
    "merge_df[\"N\"] = list(count.iloc[:,0])\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = pcb.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['pos_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_pcb\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del pcb, count, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"install_pay\"></a>\n",
    "\n",
    "# [^](#toc) <u>Installment Payments</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of install_pay: (13605401, 8)\n",
      "\n",
      "Columns of install_pay:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NUM_INSTALMENT_VERSION --- NUM_INSTALMENT_NUMBER --- DAYS_INSTALMENT --- DAYS_ENTRY_PAYMENT --- AMT_INSTALMENT --- AMT_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "install_pay = pd.read_csv(DATA_PATH + \"installments_payments.csv\")\n",
    "print(\"Shape of install_pay:\",  install_pay.shape)\n",
    "\n",
    "print(\"\\nColumns of install_pay:\")\n",
    "print(\" --- \".join(install_pay.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_install_pay\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create new feature\n",
    "install_pay[\"AMT_MISSING\"]  = install_pay[\"AMT_INSTALMENT\"]     - install_pay[\"AMT_PAYMENT\"]\n",
    "install_pay['PAYMENT_PERC'] = install_pay['AMT_PAYMENT']        / install_pay['AMT_INSTALMENT']\n",
    "\n",
    "# Days past due and days before due (no negative values)\n",
    "install_pay['DPD']          = install_pay['DAYS_ENTRY_PAYMENT'] - install_pay['DAYS_INSTALMENT']\n",
    "install_pay['DBD']          = install_pay['DAYS_INSTALMENT']    - install_pay['DAYS_ENTRY_PAYMENT']\n",
    "install_pay['DPD']          = install_pay['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "install_pay['DBD']          = install_pay['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "# Amount of values missing in AMT_PAYMENT\n",
    "install_pay[\"temp\"]         = install_pay[\"AMT_PAYMENT\"].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Select important features\n",
    "merge_df = pd.DataFrame({\n",
    "    \"missing_max\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].max(),\n",
    "    \"missing_min\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].min(),\n",
    "    \"payment_max\": install_pay.groupby(\"SK_ID_PREV\")['PAYMENT_PERC'].max(),\n",
    "    \"payment_min\": install_pay.groupby(\"SK_ID_PREV\")['PAYMENT_PERC'].min(),\n",
    "    \n",
    "    \"payment_nan\": install_pay.groupby(\"SK_ID_PREV\")[\"temp\"].sum(),\n",
    "    \"N\":           install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"].count(),\n",
    "    \"unique_ver\":  install_pay.groupby(\"SK_ID_PREV\")[\"NUM_INSTALMENT_VERSION\"].unique()\n",
    "})\n",
    "\n",
    "# Delete temp column\n",
    "install_pay = install_pay.drop(\"temp\", axis=1)\n",
    "\n",
    "# Select median of everything\n",
    "right = install_pay.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "\n",
    "### Merge the two\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['install_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_install\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del install_pay, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"credit\"></a>\n",
    "\n",
    "# [^](#toc) <u>Credit Card Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of credit_card: (3840312, 23)\n",
      "\n",
      "Columns of credit_card:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- AMT_BALANCE --- AMT_CREDIT_LIMIT_ACTUAL --- AMT_DRAWINGS_ATM_CURRENT --- AMT_DRAWINGS_CURRENT --- AMT_DRAWINGS_OTHER_CURRENT --- AMT_DRAWINGS_POS_CURRENT --- AMT_INST_MIN_REGULARITY --- AMT_PAYMENT_CURRENT --- AMT_PAYMENT_TOTAL_CURRENT --- AMT_RECEIVABLE_PRINCIPAL --- AMT_RECIVABLE --- AMT_TOTAL_RECEIVABLE --- CNT_DRAWINGS_ATM_CURRENT --- CNT_DRAWINGS_CURRENT --- CNT_DRAWINGS_OTHER_CURRENT --- CNT_DRAWINGS_POS_CURRENT --- CNT_INSTALMENT_MATURE_CUM --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\n",
    "print(\"Shape of credit_card:\",  credit_card.shape)\n",
    "\n",
    "print(\"\\nColumns of credit_card:\")\n",
    "print(\" --- \".join(credit_card.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"credit_process\"></a>\n",
    "\n",
    "### [^](#toc) <u>Data Processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gets indices with outlier values\n",
    "temp = credit_card[credit_card.NAME_CONTRACT_STATUS.isin([\"Refused\", \"Approved\"])].index\n",
    "\n",
    "# Drops outlier values\n",
    "credit_card = credit_card.drop(temp, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_credit\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3696"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create features\n",
    "merge_df = pd.DataFrame({\n",
    "    \"AMT_BALANCE\": credit_card.groupby(\"SK_ID_PREV\").AMT_BALANCE.mean(),\n",
    "    \"SK_DPD\":      credit_card.groupby(\"SK_ID_PREV\").SK_DPD.max(),\n",
    "    \"SK_DPD_DEF\":  credit_card.groupby(\"SK_ID_PREV\").SK_DPD_DEF.max(),\n",
    "    \"N\":           credit_card.groupby(\"SK_ID_PREV\").count().iloc[:,0]\n",
    "})\n",
    "\n",
    "### Categorical column\n",
    "temp = get_dummies(credit_card, [\"NAME_CONTRACT_STATUS\"])\n",
    "cols = ['NAME_CONTRACT_STATUS_Active',\n",
    "       'NAME_CONTRACT_STATUS_Completed', 'NAME_CONTRACT_STATUS_Demand',\n",
    "       'NAME_CONTRACT_STATUS_Sent proposal', 'NAME_CONTRACT_STATUS_Signed']\n",
    "for col in cols:\n",
    "    temp[col] = temp[col] / (temp[\"MONTHS_BALANCE\"] - 1)\n",
    "cols.extend([\"SK_ID_PREV\"])\n",
    "temp = temp[cols]\n",
    "temp = temp.groupby(\"SK_ID_PREV\").sum()\n",
    "\n",
    "# Merge categorical and numerical df\n",
    "merge_df = temp.join(merge_df)\n",
    "\n",
    "### Add the rest of the columns\n",
    "right = credit_card.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols = ['credit_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')\n",
    "\n",
    "# Mark missing values\n",
    "prev_app[\"no_credit\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "### Delete old variables\n",
    "del credit_card, merge_df, merged_cols, right\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"misc\"></a>\n",
    "\n",
    "# [^](#toc) <u>Miscellaneous clean up</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Drop unneeded ID columns\n",
    "prev_app = prev_app.drop(\"SK_ID_PREV\", axis=1)\n",
    "bureau   = bureau.drop(\"SK_ID_BUREAU\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"final_merge\"></a>\n",
    "\n",
    "# [^](#toc) <u>Final Data Prep</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (307511, 122)\n",
      "Shape of test: (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "test  = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "\n",
    "print(\"Shape of train:\", train.shape)\n",
    "print(\"Shape of test:\",  test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into predictors, target, and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train.TARGET\n",
    "train_x = train.drop([\"TARGET\"], axis=1)\n",
    "\n",
    "test_id = test.SK_ID_CURR\n",
    "test_x  = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full    = pd.concat([train_x, test_x])\n",
    "train_N = len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final_process\"></a>\n",
    "\n",
    "### [^](#toc) <u>Data Processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Replace maxed values with NaN\n",
    "full['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "### Fill in outlier values\n",
    "full[\"CODE_GENDER\"]        = full[\"CODE_GENDER\"].map(lambda x: \"F\" if x == \"XNA\" else x)\n",
    "full[\"NAME_FAMILY_STATUS\"] = full[\"NAME_FAMILY_STATUS\"].map(lambda x: \"Married\" if x == \"Unknown\" else x)\n",
    "\n",
    "# NAME_INCOME_TYPE\n",
    "cols = [\"Unemployed\", \"Student\", \"Businessman\", \"Maternity leave\"]\n",
    "full[\"NAME_INCOME_TYPE\"] = full[\"NAME_INCOME_TYPE\"].map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# ORGANIZATION_TYPE\n",
    "cols = [\"Trade: type 4\", \"Trade: type 5\"]\n",
    "full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Trade\" if x in cols else x)\n",
    "cols = [\"Industry: type 13\", \"Industry: type 8\"]\n",
    "full[\"ORGANIZATION_TYPE\"] = full[\"ORGANIZATION_TYPE\"].map(lambda x: \"MISC Industry\" if x in cols else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get dummies\n",
    "cols  = [\"WALLSMATERIAL_MODE\", \"NAME_TYPE_SUITE\", \"NAME_INCOME_TYPE\", \"NAME_FAMILY_STATUS\",\n",
    "               \"NAME_HOUSING_TYPE\", \"OCCUPATION_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"ORGANIZATION_TYPE\",\n",
    "               \"FONDKAPREMONT_MODE\", \"NAME_EDUCATION_TYPE\"]\n",
    "full = get_dummies(full, cols)\n",
    "full = full.drop(cols, axis=1)\n",
    "\n",
    "### Factorize the dataframe\n",
    "cols = [\"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"FLAG_OWN_CAR\",\n",
    "               \"FLAG_OWN_REALTY\", \"HOUSETYPE_MODE\", \"EMERGENCYSTATE_MODE\"]\n",
    "full = factorize_df(full, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full['DAYS_EMPLOYED_PERC']  = full['DAYS_EMPLOYED']    / full['DAYS_BIRTH']\n",
    "full['INCOME_CREDIT_PERC']  = full['AMT_INCOME_TOTAL'] / full['AMT_CREDIT']\n",
    "full['INCOME_PER_PERSON']   = full['AMT_INCOME_TOTAL'] / full['CNT_FAM_MEMBERS']\n",
    "full['ANNUITY_INCOME_PERC'] = full['AMT_ANNUITY']      / full['AMT_INCOME_TOTAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_prev\"></a>\n",
    "\n",
    "### [^](#toc) Merge Previous Application with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "        \"NAME_CONTRACT_TYPE\", \"WEEKDAY_APPR_PROCESS_START\",\n",
    "        \"FLAG_LAST_APPL_PER_CONTRACT\", \"NAME_CASH_LOAN_PURPOSE\",\n",
    "        \"NAME_CONTRACT_STATUS\", \"NAME_PAYMENT_TYPE\",\n",
    "        \"CODE_REJECT_REASON\", \"NAME_TYPE_SUITE\", \"NAME_CLIENT_TYPE\",\n",
    "        \"NAME_GOODS_CATEGORY\", \"NAME_PORTFOLIO\", \"NAME_PRODUCT_TYPE\",\n",
    "        \"CHANNEL_TYPE\", \"NAME_SELLER_INDUSTRY\", \"NAME_YIELD_GROUP\",\n",
    "        \"PRODUCT_COMBINATION\", \"SK_ID_CURR\"]\n",
    "num_cols = [col for col in prev_app.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "### Numeric columns\n",
    "merge_df      = prev_app[num_cols].groupby('SK_ID_CURR').mean()\n",
    "merge_df[\"N\"] = prev_app.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "\n",
    "### Categorical columns\n",
    "right = prev_app[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "\n",
    "### Merge categorical and numeric\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols   = ['p_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "# Mark missing values\n",
    "full[\"no_prev_app\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau\"></a>\n",
    "\n",
    "### [^](#toc) Merge Bureau with Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE', 'SK_ID_CURR']\n",
    "num_cols = [col for col in bureau.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "### Numeric columns\n",
    "merge_df      = bureau[num_cols].groupby('SK_ID_CURR').mean()\n",
    "merge_df[\"N\"] = bureau.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "\n",
    "### Categorical columns\n",
    "right = bureau[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "\n",
    "### Merge categorical and numeric\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "\n",
    "### Prefix column names\n",
    "merged_cols   = ['b_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "# Mark missing values\n",
    "full[\"no_bureau\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full = full.drop(['APARTMENTS_MODE', 'BASEMENTAREA_MODE',\n",
    "#        'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE',\n",
    "#        'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE',\n",
    "#        'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE',\n",
    "#        'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n",
    "#        'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI',\n",
    "#        'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI',\n",
    "#        'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI',\n",
    "#        'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI',\n",
    "#        'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI',\n",
    "#        'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI'], axis=1)\n",
    "\n",
    "full = full.drop(\"SK_ID_CURR\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split full back into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>...</th>\n",
       "      <th>b_CREDIT_TYPE_Car loan</th>\n",
       "      <th>b_CREDIT_TYPE_Consumer credit</th>\n",
       "      <th>b_CREDIT_TYPE_Credit card</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for business development</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for working capital replenishment</th>\n",
       "      <th>b_CREDIT_TYPE_MISC</th>\n",
       "      <th>b_CREDIT_TYPE_Microloan</th>\n",
       "      <th>b_CREDIT_TYPE_Mortgage</th>\n",
       "      <th>b_CREDIT_TYPE_Unknown type of loan</th>\n",
       "      <th>no_bureau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0                   0            0             0                0   \n",
       "1                   0            1             0                1   \n",
       "2                   1            0             1                0   \n",
       "3                   0            1             0                0   \n",
       "4                   0            0             0                0   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "0             0          202500.0    406597.5      24700.5         351000.0   \n",
       "1             0          270000.0   1293502.5      35698.5        1129500.0   \n",
       "2             0           67500.0    135000.0       6750.0         135000.0   \n",
       "3             0          135000.0    312682.5      29686.5         297000.0   \n",
       "4             0          121500.0    513000.0      21865.5         513000.0   \n",
       "\n",
       "   REGION_POPULATION_RELATIVE    ...      b_CREDIT_TYPE_Car loan  \\\n",
       "0                    0.018801    ...                         0.0   \n",
       "1                    0.003541    ...                         0.0   \n",
       "2                    0.010032    ...                         0.0   \n",
       "3                    0.008019    ...                         NaN   \n",
       "4                    0.028663    ...                         0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Consumer credit  b_CREDIT_TYPE_Credit card  \\\n",
       "0                            4.0                        4.0   \n",
       "1                            2.0                        2.0   \n",
       "2                            2.0                        0.0   \n",
       "3                            NaN                        NaN   \n",
       "4                            1.0                        0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Loan for business development  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          NaN   \n",
       "4                                          0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Loan for working capital replenishment  b_CREDIT_TYPE_MISC  \\\n",
       "0                                                0.0                    0.0   \n",
       "1                                                0.0                    0.0   \n",
       "2                                                0.0                    0.0   \n",
       "3                                                NaN                    NaN   \n",
       "4                                                0.0                    0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Microloan  b_CREDIT_TYPE_Mortgage  \\\n",
       "0                      0.0                     0.0   \n",
       "1                      0.0                     0.0   \n",
       "2                      0.0                     0.0   \n",
       "3                      NaN                     NaN   \n",
       "4                      0.0                     0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Unknown type of loan  no_bureau  \n",
       "0                                 0.0          0  \n",
       "1                                 0.0          0  \n",
       "2                                 0.0          0  \n",
       "3                                 NaN          1  \n",
       "4                                 0.0          0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = full[:train_N]\n",
    "test_x = full[train_N:]\n",
    "\n",
    "### Processed data look\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# [^](#toc) <u>Models </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[200]\tvalid_0's auc: 0.740902\n",
      "[400]\tvalid_0's auc: 0.756678\n",
      "[600]\tvalid_0's auc: 0.768199\n",
      "[800]\tvalid_0's auc: 0.774027\n",
      "[1000]\tvalid_0's auc: 0.776892\n",
      "[1200]\tvalid_0's auc: 0.778745\n",
      "[1400]\tvalid_0's auc: 0.779959\n",
      "[1600]\tvalid_0's auc: 0.780851\n",
      "[1800]\tvalid_0's auc: 0.78126\n",
      "[2000]\tvalid_0's auc: 0.781587\n",
      "[2200]\tvalid_0's auc: 0.781805\n",
      "[2400]\tvalid_0's auc: 0.781905\n",
      "[2600]\tvalid_0's auc: 0.782068\n",
      "[2800]\tvalid_0's auc: 0.782201\n",
      "[3000]\tvalid_0's auc: 0.782213\n",
      "Early stopping, best iteration is:\n",
      "[2902]\tvalid_0's auc: 0.782258\n",
      "Training took 858 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import lightgbm as lgb\n",
    "\n",
    "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)\n",
    "lgb_train = lgb.Dataset(data=training_x, label=training_y)\n",
    "lgb_eval  = lgb.Dataset(data=val_x, label=val_y)\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n",
    "          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n",
    "          'min_split_gain':.01, 'min_child_weight':1}\n",
    "\n",
    "start = time.time()\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help from [Dmitriy Kisil](https://www.kaggle.com/oysiyl) and [his kernel](https://www.kaggle.com/oysiyl/good-fun-with-ligthgbm/code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\ttraining's auc: 0.755308\tvalid_1's auc: 0.735649\n",
      "[200]\ttraining's auc: 0.764373\tvalid_1's auc: 0.742282\n",
      "[300]\ttraining's auc: 0.77609\tvalid_1's auc: 0.749285\n",
      "[400]\ttraining's auc: 0.788913\tvalid_1's auc: 0.757173\n",
      "[500]\ttraining's auc: 0.800729\tvalid_1's auc: 0.764146\n",
      "[600]\ttraining's auc: 0.810725\tvalid_1's auc: 0.769251\n",
      "[700]\ttraining's auc: 0.819225\tvalid_1's auc: 0.772753\n",
      "[800]\ttraining's auc: 0.826612\tvalid_1's auc: 0.775125\n",
      "[900]\ttraining's auc: 0.833353\tvalid_1's auc: 0.776832\n",
      "[1000]\ttraining's auc: 0.839434\tvalid_1's auc: 0.77814\n",
      "[1100]\ttraining's auc: 0.84516\tvalid_1's auc: 0.779099\n",
      "[1200]\ttraining's auc: 0.850539\tvalid_1's auc: 0.779794\n",
      "[1300]\ttraining's auc: 0.855494\tvalid_1's auc: 0.780428\n",
      "[1400]\ttraining's auc: 0.86034\tvalid_1's auc: 0.781067\n",
      "[1500]\ttraining's auc: 0.864871\tvalid_1's auc: 0.78157\n",
      "[1600]\ttraining's auc: 0.869159\tvalid_1's auc: 0.781959\n",
      "[1700]\ttraining's auc: 0.873233\tvalid_1's auc: 0.782414\n",
      "[1800]\ttraining's auc: 0.877069\tvalid_1's auc: 0.782709\n",
      "[1900]\ttraining's auc: 0.880696\tvalid_1's auc: 0.782835\n",
      "[2000]\ttraining's auc: 0.884279\tvalid_1's auc: 0.783068\n",
      "[2100]\ttraining's auc: 0.88788\tvalid_1's auc: 0.783346\n",
      "[2200]\ttraining's auc: 0.89127\tvalid_1's auc: 0.783492\n",
      "[2300]\ttraining's auc: 0.894698\tvalid_1's auc: 0.783661\n",
      "[2400]\ttraining's auc: 0.897892\tvalid_1's auc: 0.783746\n",
      "[2500]\ttraining's auc: 0.900738\tvalid_1's auc: 0.783916\n",
      "[2600]\ttraining's auc: 0.903663\tvalid_1's auc: 0.784043\n",
      "[2700]\ttraining's auc: 0.906267\tvalid_1's auc: 0.784076\n",
      "[2800]\ttraining's auc: 0.909012\tvalid_1's auc: 0.784054\n",
      "[2900]\ttraining's auc: 0.911907\tvalid_1's auc: 0.78413\n",
      "[3000]\ttraining's auc: 0.914461\tvalid_1's auc: 0.784147\n",
      "Early stopping, best iteration is:\n",
      "[2925]\ttraining's auc: 0.912567\tvalid_1's auc: 0.784177\n",
      "Fold  1 AUC : 0.784177\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\ttraining's auc: 0.753239\tvalid_1's auc: 0.743536\n",
      "[200]\ttraining's auc: 0.762941\tvalid_1's auc: 0.749262\n",
      "[300]\ttraining's auc: 0.77552\tvalid_1's auc: 0.756122\n",
      "[400]\ttraining's auc: 0.789074\tvalid_1's auc: 0.762911\n",
      "[500]\ttraining's auc: 0.800605\tvalid_1's auc: 0.768444\n",
      "[600]\ttraining's auc: 0.810517\tvalid_1's auc: 0.772935\n",
      "[700]\ttraining's auc: 0.818915\tvalid_1's auc: 0.775988\n",
      "[800]\ttraining's auc: 0.826242\tvalid_1's auc: 0.778238\n",
      "[900]\ttraining's auc: 0.832871\tvalid_1's auc: 0.779937\n",
      "[1000]\ttraining's auc: 0.838797\tvalid_1's auc: 0.781067\n",
      "[1100]\ttraining's auc: 0.844222\tvalid_1's auc: 0.781955\n",
      "[1200]\ttraining's auc: 0.849371\tvalid_1's auc: 0.782719\n",
      "[1300]\ttraining's auc: 0.854389\tvalid_1's auc: 0.783386\n",
      "[1400]\ttraining's auc: 0.859021\tvalid_1's auc: 0.783751\n",
      "[1500]\ttraining's auc: 0.863672\tvalid_1's auc: 0.784068\n",
      "[1600]\ttraining's auc: 0.8682\tvalid_1's auc: 0.784438\n",
      "[1700]\ttraining's auc: 0.872526\tvalid_1's auc: 0.784657\n",
      "[1800]\ttraining's auc: 0.876341\tvalid_1's auc: 0.784833\n",
      "[1900]\ttraining's auc: 0.879821\tvalid_1's auc: 0.784951\n",
      "[2000]\ttraining's auc: 0.883452\tvalid_1's auc: 0.785082\n",
      "[2100]\ttraining's auc: 0.886885\tvalid_1's auc: 0.785206\n",
      "[2200]\ttraining's auc: 0.890456\tvalid_1's auc: 0.785279\n",
      "[2300]\ttraining's auc: 0.893845\tvalid_1's auc: 0.785377\n",
      "[2400]\ttraining's auc: 0.89718\tvalid_1's auc: 0.785548\n",
      "[2500]\ttraining's auc: 0.900045\tvalid_1's auc: 0.785652\n",
      "[2600]\ttraining's auc: 0.902945\tvalid_1's auc: 0.785696\n",
      "[2700]\ttraining's auc: 0.905683\tvalid_1's auc: 0.785738\n",
      "[2800]\ttraining's auc: 0.908474\tvalid_1's auc: 0.785809\n",
      "[2900]\ttraining's auc: 0.911224\tvalid_1's auc: 0.785788\n",
      "Early stopping, best iteration is:\n",
      "[2845]\ttraining's auc: 0.909724\tvalid_1's auc: 0.785844\n",
      "Fold  2 AUC : 0.785844\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\ttraining's auc: 0.753386\tvalid_1's auc: 0.74136\n",
      "[200]\ttraining's auc: 0.762412\tvalid_1's auc: 0.747484\n",
      "[300]\ttraining's auc: 0.775383\tvalid_1's auc: 0.755633\n",
      "[400]\ttraining's auc: 0.788806\tvalid_1's auc: 0.76384\n",
      "[500]\ttraining's auc: 0.800686\tvalid_1's auc: 0.769998\n",
      "[600]\ttraining's auc: 0.810418\tvalid_1's auc: 0.773847\n",
      "[700]\ttraining's auc: 0.818902\tvalid_1's auc: 0.776636\n",
      "[800]\ttraining's auc: 0.826208\tvalid_1's auc: 0.778468\n",
      "[900]\ttraining's auc: 0.832772\tvalid_1's auc: 0.779825\n",
      "[1000]\ttraining's auc: 0.838884\tvalid_1's auc: 0.780998\n",
      "[1100]\ttraining's auc: 0.844574\tvalid_1's auc: 0.78164\n",
      "[1200]\ttraining's auc: 0.849874\tvalid_1's auc: 0.782386\n",
      "[1300]\ttraining's auc: 0.854797\tvalid_1's auc: 0.782994\n",
      "[1400]\ttraining's auc: 0.859789\tvalid_1's auc: 0.783458\n",
      "[1500]\ttraining's auc: 0.864474\tvalid_1's auc: 0.783777\n",
      "[1600]\ttraining's auc: 0.868814\tvalid_1's auc: 0.784067\n",
      "[1700]\ttraining's auc: 0.873006\tvalid_1's auc: 0.784326\n",
      "[1800]\ttraining's auc: 0.877045\tvalid_1's auc: 0.784612\n",
      "[1900]\ttraining's auc: 0.880796\tvalid_1's auc: 0.784813\n",
      "[2000]\ttraining's auc: 0.884331\tvalid_1's auc: 0.784934\n",
      "[2100]\ttraining's auc: 0.888067\tvalid_1's auc: 0.784987\n",
      "[2200]\ttraining's auc: 0.891384\tvalid_1's auc: 0.78504\n",
      "Early stopping, best iteration is:\n",
      "[2126]\ttraining's auc: 0.888916\tvalid_1's auc: 0.785073\n",
      "Fold  3 AUC : 0.785073\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\ttraining's auc: 0.753513\tvalid_1's auc: 0.742134\n",
      "[200]\ttraining's auc: 0.762904\tvalid_1's auc: 0.748\n",
      "[300]\ttraining's auc: 0.775397\tvalid_1's auc: 0.755574\n",
      "[400]\ttraining's auc: 0.788499\tvalid_1's auc: 0.762947\n",
      "[500]\ttraining's auc: 0.800205\tvalid_1's auc: 0.769321\n",
      "[600]\ttraining's auc: 0.810165\tvalid_1's auc: 0.773859\n",
      "[700]\ttraining's auc: 0.818458\tvalid_1's auc: 0.776917\n",
      "[800]\ttraining's auc: 0.825866\tvalid_1's auc: 0.779086\n",
      "[900]\ttraining's auc: 0.832445\tvalid_1's auc: 0.780611\n",
      "[1000]\ttraining's auc: 0.838374\tvalid_1's auc: 0.781683\n",
      "[1100]\ttraining's auc: 0.843913\tvalid_1's auc: 0.782585\n",
      "[1200]\ttraining's auc: 0.849171\tvalid_1's auc: 0.783254\n",
      "[1300]\ttraining's auc: 0.854147\tvalid_1's auc: 0.783826\n",
      "[1400]\ttraining's auc: 0.858867\tvalid_1's auc: 0.784284\n",
      "[1500]\ttraining's auc: 0.863385\tvalid_1's auc: 0.784664\n",
      "[1600]\ttraining's auc: 0.867691\tvalid_1's auc: 0.784931\n",
      "[1700]\ttraining's auc: 0.871941\tvalid_1's auc: 0.785274\n",
      "[1800]\ttraining's auc: 0.875787\tvalid_1's auc: 0.78546\n",
      "[1900]\ttraining's auc: 0.879621\tvalid_1's auc: 0.785645\n",
      "[2000]\ttraining's auc: 0.88326\tvalid_1's auc: 0.785807\n",
      "[2100]\ttraining's auc: 0.886756\tvalid_1's auc: 0.785983\n",
      "[2200]\ttraining's auc: 0.889951\tvalid_1's auc: 0.78611\n",
      "[2300]\ttraining's auc: 0.893096\tvalid_1's auc: 0.786224\n",
      "[2400]\ttraining's auc: 0.896286\tvalid_1's auc: 0.786187\n",
      "Early stopping, best iteration is:\n",
      "[2347]\ttraining's auc: 0.894734\tvalid_1's auc: 0.786296\n",
      "Fold  4 AUC : 0.786296\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[100]\ttraining's auc: 0.753357\tvalid_1's auc: 0.742605\n",
      "[200]\ttraining's auc: 0.762875\tvalid_1's auc: 0.748502\n",
      "[300]\ttraining's auc: 0.775286\tvalid_1's auc: 0.755853\n",
      "[400]\ttraining's auc: 0.788482\tvalid_1's auc: 0.763674\n",
      "[500]\ttraining's auc: 0.80028\tvalid_1's auc: 0.770104\n",
      "[600]\ttraining's auc: 0.810016\tvalid_1's auc: 0.774455\n",
      "[700]\ttraining's auc: 0.818412\tvalid_1's auc: 0.777573\n",
      "[800]\ttraining's auc: 0.825586\tvalid_1's auc: 0.779782\n",
      "[900]\ttraining's auc: 0.83228\tvalid_1's auc: 0.781346\n",
      "[1000]\ttraining's auc: 0.838399\tvalid_1's auc: 0.7825\n",
      "[1100]\ttraining's auc: 0.844087\tvalid_1's auc: 0.783447\n",
      "[1200]\ttraining's auc: 0.849291\tvalid_1's auc: 0.784175\n",
      "[1300]\ttraining's auc: 0.854235\tvalid_1's auc: 0.784799\n",
      "[1400]\ttraining's auc: 0.859181\tvalid_1's auc: 0.785353\n",
      "[1500]\ttraining's auc: 0.863515\tvalid_1's auc: 0.785725\n",
      "[1600]\ttraining's auc: 0.867885\tvalid_1's auc: 0.786158\n",
      "[1700]\ttraining's auc: 0.871905\tvalid_1's auc: 0.786474\n",
      "[1800]\ttraining's auc: 0.875892\tvalid_1's auc: 0.786819\n",
      "[1900]\ttraining's auc: 0.879696\tvalid_1's auc: 0.787061\n",
      "[2000]\ttraining's auc: 0.883226\tvalid_1's auc: 0.787283\n",
      "[2100]\ttraining's auc: 0.886595\tvalid_1's auc: 0.787399\n",
      "[2200]\ttraining's auc: 0.889948\tvalid_1's auc: 0.787657\n",
      "[2300]\ttraining's auc: 0.893226\tvalid_1's auc: 0.787641\n",
      "[2400]\ttraining's auc: 0.896352\tvalid_1's auc: 0.787725\n",
      "[2500]\ttraining's auc: 0.899318\tvalid_1's auc: 0.78787\n",
      "[2600]\ttraining's auc: 0.902342\tvalid_1's auc: 0.787915\n",
      "[2700]\ttraining's auc: 0.905332\tvalid_1's auc: 0.787991\n",
      "[2800]\ttraining's auc: 0.908177\tvalid_1's auc: 0.788041\n",
      "[2900]\ttraining's auc: 0.91107\tvalid_1's auc: 0.788062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\ttraining's auc: 0.913632\tvalid_1's auc: 0.788125\n",
      "[3100]\ttraining's auc: 0.916123\tvalid_1's auc: 0.788208\n",
      "[3200]\ttraining's auc: 0.918495\tvalid_1's auc: 0.788255\n",
      "[3300]\ttraining's auc: 0.920817\tvalid_1's auc: 0.788263\n",
      "[3400]\ttraining's auc: 0.92307\tvalid_1's auc: 0.788365\n",
      "[3500]\ttraining's auc: 0.925361\tvalid_1's auc: 0.788244\n",
      "Early stopping, best iteration is:\n",
      "[3374]\ttraining's auc: 0.92252\tvalid_1's auc: 0.788384\n",
      "Fold  5 AUC : 0.788384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics         import roc_auc_score, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm                import LGBMClassifier\n",
    "import random\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=17)\n",
    "oof_preds = np.zeros(train_x.shape[0])\n",
    "sub_preds = np.zeros(test_x.shape[0])\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x)):\n",
    "    trn_x, trn_y = train_x.iloc[trn_idx], train_y.iloc[trn_idx]\n",
    "    val_x, val_y = train_x.iloc[val_idx], train_y.iloc[val_idx]\n",
    "  \n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        learning_rate=0.01,\n",
    "        num_leaves = 48,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.9,\n",
    "        max_depth=7,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        min_split_gain=0.01,\n",
    "        min_child_weight=1,\n",
    "        num_iteration= 5000,\n",
    "        random_state=random.randint(1, 100) # Recommended to make the seed random\n",
    "    )\n",
    "        \n",
    "    model.fit(trn_x, trn_y, \n",
    "            eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "            eval_metric='auc', verbose=100, early_stopping_rounds=150\n",
    "           )\n",
    "    \n",
    "    oof_preds[val_idx] = model.predict_proba(val_x, num_iteration=model.best_iteration_)[:, 1]\n",
    "    sub_preds         += model.predict_proba(\n",
    "                                             test_x, \n",
    "                                             num_iteration=model.best_iteration_\n",
    "                                            )[:, 1] / folds.n_splits\n",
    "    \n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "    del model, trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[200]\tvalid_0's auc: 0.740902\n",
      "[400]\tvalid_0's auc: 0.756678\n",
      "[600]\tvalid_0's auc: 0.768199\n",
      "[800]\tvalid_0's auc: 0.774027\n",
      "[1000]\tvalid_0's auc: 0.776892\n",
      "[1200]\tvalid_0's auc: 0.778745\n",
      "[1400]\tvalid_0's auc: 0.779959\n",
      "[1600]\tvalid_0's auc: 0.780851\n",
      "[1800]\tvalid_0's auc: 0.78126\n",
      "[2000]\tvalid_0's auc: 0.781587\n",
      "[2200]\tvalid_0's auc: 0.781805\n",
      "[2400]\tvalid_0's auc: 0.781905\n",
      "[2600]\tvalid_0's auc: 0.782068\n",
      "[2800]\tvalid_0's auc: 0.782201\n",
      "[3000]\tvalid_0's auc: 0.782213\n",
      "Early stopping, best iteration is:\n",
      "[2902]\tvalid_0's auc: 0.782258\n",
      "Training took 808 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import lightgbm as lgb\n",
    "    \n",
    "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)\n",
    "lgb_train = lgb.Dataset(data=training_x, label=training_y)\n",
    "lgb_eval  = lgb.Dataset(data=val_x, label=val_y)\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n",
    "          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n",
    "          'min_split_gain':.01, 'min_child_weight':1}\n",
    "\n",
    "start = time.time()\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATS = 250\n",
    "\n",
    "feats = sorted(list(zip(model.feature_importance(), train_x.columns)))\n",
    "feats = list(list(zip(*feats[-NUM_FEATS:]))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model on less features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[200]\tvalid_0's auc: 0.740866\n",
      "[400]\tvalid_0's auc: 0.757416\n",
      "[600]\tvalid_0's auc: 0.769089\n",
      "[800]\tvalid_0's auc: 0.774897\n",
      "[1000]\tvalid_0's auc: 0.777586\n",
      "[1200]\tvalid_0's auc: 0.77924\n",
      "[1400]\tvalid_0's auc: 0.780196\n",
      "[1600]\tvalid_0's auc: 0.780802\n",
      "[1800]\tvalid_0's auc: 0.781381\n",
      "[2000]\tvalid_0's auc: 0.781659\n",
      "[2200]\tvalid_0's auc: 0.781893\n",
      "[2400]\tvalid_0's auc: 0.782054\n",
      "[2600]\tvalid_0's auc: 0.78206\n",
      "Early stopping, best iteration is:\n",
      "[2517]\tvalid_0's auc: 0.782139\n",
      "Training took 538 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import lightgbm as lgb\n",
    "    \n",
    "training_x, val_x, training_y, val_y = train_test_split(train_x[feats], train_y, test_size=0.2, random_state=17)\n",
    "lgb_train = lgb.Dataset(data=training_x, label=training_y)\n",
    "lgb_eval  = lgb.Dataset(data=val_x, label=val_y)\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n",
    "          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':1, \n",
    "          'min_split_gain':.01, 'min_child_weight':1}\n",
    "\n",
    "start = time.time()\n",
    "light_model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"SK_ID_CURR\": test_id,\n",
    "    \"TARGET\": sub_preds\n",
    "}).to_csv(\"../submissions/250_feats_single_run.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
