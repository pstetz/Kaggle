{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score: 0.784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "# <u>Table of Contents</u>\n",
    "1.) [TODO](#todo)  \n",
    "2.) [Imports](#imports)  \n",
    "3.) [Load data](#load)  \n",
    "4.) [Bureau Balance](#bureau_bal)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.1.) [Merge into Bureau](#merge_bureau_bal)  \n",
    "5.) [POS CASH balance](#pos_cash)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.1.) [Missing values](#pos_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 5.2.) [Merge into Previous Application](#merge_pos_cash)  \n",
    "6.) [Installment Payments](#install_pay)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.1.) [Missing values](#install_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.2.) [Merge into Previous Application](#merge_install_pay)  \n",
    "7.) [Credit Card Balance](#credit)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.1.) [Missing values](#credit_nan)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 7.2.) [Merge into Previous Application](#merge_credit)  \n",
    "8.) [Final Data Prep](#final_merge)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 8.1.) [Missing values](#final_nan)  \n",
    "9.) [Modeling](#models)  \n",
    "10.) [Save file to CSV](#save)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todo\"></a>\n",
    "\n",
    "# [^](#toc) <u>TODO</u>\n",
    "\n",
    "- Fix skew on columns\n",
    "- Tinker with the best way to replace missing values (dropping cols?)\n",
    "- Look for outliers\n",
    "- Merge db together\n",
    "- Include timeline relatoinships like MONTHS_BALANCE\n",
    "- Tune model parameters\n",
    "- Address [this](https://www.kaggle.com/c/home-credit-default-risk/discussion/57248)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "# [^](#toc) <u>Imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Time keeper\n",
    "import time\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Removes warnings from output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to create dummy variables of categorical features\n",
    "def get_dummies(df, cats):\n",
    "    for col in cats:\n",
    "        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "    return df \n",
    "\n",
    "def fillna_num(df):\n",
    "    missing_cols = [col for col in df.columns if any(df[col].isnull()) and df[col].dtype != object]\n",
    "    for col in missing_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    return df\n",
    "\n",
    "def fillna_cat(df):\n",
    "    for col in [col for col in df if df[col].dtype==object]:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df\n",
    "\n",
    "def factorize_df(df, cats):\n",
    "    for col in cats:\n",
    "        df[col], _ = pd.factorize(df[col])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"load\"></a>\n",
    "\n",
    "# [^](#toc) <u>Load data</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau: (1716428, 17)\n",
      "Shape of prev_app: (1670214, 37)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data/home_default/\"\n",
    "\n",
    "bureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\n",
    "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\n",
    "\n",
    "print(\"Shape of bureau:\",    bureau.shape)\n",
    "print(\"Shape of prev_app:\",  prev_app.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau   = fillna_num(bureau)\n",
    "bureau   = fillna_cat(bureau)\n",
    "\n",
    "prev_app = fillna_num(prev_app)\n",
    "prev_app = fillna_cat(prev_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bureau_bal\"></a>\n",
    "\n",
    "# [^](#toc) <u>Bureau Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bureau_balance: (27299925, 3)\n",
      "\n",
      "Columns of bureau_balance:\n",
      "SK_ID_BUREAU --- MONTHS_BALANCE --- STATUS\n"
     ]
    }
   ],
   "source": [
    "bureau_balance = pd.read_csv(DATA_PATH + \"bureau_balance.csv\")\n",
    "print(\"Shape of bureau_balance:\",  bureau_balance.shape)\n",
    "\n",
    "print(\"\\nColumns of bureau_balance:\")\n",
    "print(\" --- \".join(bureau_balance.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_bureau_bal\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Bureau</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_df = get_dummies(bureau_balance, [\"STATUS\"])\n",
    "merge_df = merge_df.drop([\"MONTHS_BALANCE\", \"STATUS\"], axis=1)\n",
    "\n",
    "# prep for merge\n",
    "merge_df = merge_df.groupby(\"SK_ID_BUREAU\").sum().reset_index()\n",
    "\n",
    "### Add the median of the rest of the columns\n",
    "right    = bureau_balance.groupby(\"SK_ID_BUREAU\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_BUREAU\").set_index(\"SK_ID_BUREAU\")\n",
    "\n",
    "### Remember added columns\n",
    "merged_cols = ['bur_bal_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "bureau = bureau.merge(right=merge_df.reset_index(), how='left', on='SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in new missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau[\"no_bureau_bal\"] = bureau[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "bureau[merged_cols]     = bureau[merged_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"pos_cash\"></a>\n",
    "\n",
    "# [^](#toc) <u>POS CASH balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pcb: (10001358, 8)\n",
      "\n",
      "Columns of pcb:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- CNT_INSTALMENT --- CNT_INSTALMENT_FUTURE --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "pcb = pd.read_csv(DATA_PATH + \"POS_CASH_balance.csv\")\n",
    "print(\"Shape of pcb:\",  pcb.shape)\n",
    "\n",
    "print(\"\\nColumns of pcb:\")\n",
    "print(\" --- \".join(pcb.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_nan\"></a>\n",
    "\n",
    "### [^](#toc) Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n",
    "    pcb[col] = pcb[col].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_df = pcb[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "\n",
    "merge_df = get_dummies(merge_df, [\"NAME_CONTRACT_STATUS\"])\n",
    "merge_df = merge_df.drop(\"NAME_CONTRACT_STATUS\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_pos_cash\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prep for merge\n",
    "count    = merge_df.groupby(\"SK_ID_PREV\").count()\n",
    "merge_df = merge_df.groupby(\"SK_ID_PREV\").sum().reset_index()\n",
    "merge_df[\"N\"] = list(count.iloc[:,0])\n",
    "\n",
    "right    = pcb.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "\n",
    "merged_cols = ['pos_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:08<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "prev_app[\"no_pcb\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null      = prev_app[col].notnull()\n",
    "    mode          = prev_app[not_null][col].mode()\n",
    "    prev_app[col] = prev_app[col].fillna(mode)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"install_pay\"></a>\n",
    "\n",
    "# [^](#toc) <u>Installment Payments</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of install_pay: (13605401, 8)\n",
      "\n",
      "Columns of install_pay:\n",
      "SK_ID_PREV --- SK_ID_CURR --- NUM_INSTALMENT_VERSION --- NUM_INSTALMENT_NUMBER --- DAYS_INSTALMENT --- DAYS_ENTRY_PAYMENT --- AMT_INSTALMENT --- AMT_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "install_pay = pd.read_csv(DATA_PATH + \"installments_payments.csv\")\n",
    "print(\"Shape of install_pay:\",  install_pay.shape)\n",
    "\n",
    "print(\"\\nColumns of install_pay:\")\n",
    "print(\" --- \".join(install_pay.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install_nan\"></a>\n",
    "\n",
    "### [^](#toc) <u>Missing values</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in (\"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"):\n",
    "    install_pay[col + \"_nan\"] = install_pay[col].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "    install_pay[col] = install_pay[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "install_pay[\"AMT_MISSING\"] = install_pay[\"AMT_INSTALMENT\"] - install_pay[\"AMT_PAYMENT\"]\n",
    "temp = install_pay.groupby(\"SK_ID_PREV\")[\"AMT_MISSING\"]\n",
    "\n",
    "merge_df = pd.DataFrame({\n",
    "    \"INSTALL_missing_max\": temp.max(),\n",
    "    \"INSTALL_missing_min\": temp.min(),\n",
    "    \"INSTALL_missing_med\": temp.median(),\n",
    "    \"INSTALL_payment_nan\": install_pay.groupby(\"SK_ID_PREV\")[\"AMT_PAYMENT_nan\"].sum(),\n",
    "    \"INSTALL_N\":           temp.count()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right = install_pay.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.reset_index()\n",
    "\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")\n",
    "merged_cols = merge_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_install_pay\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:13<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "prev_app[\"no_install\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null      = prev_app[col].notnull()\n",
    "    mode          = prev_app[not_null][col].mode()\n",
    "    prev_app[col] = prev_app[col].fillna(mode)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"credit\"></a>\n",
    "\n",
    "# [^](#toc) <u>Credit Card Balance</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of credit_card: (3840312, 23)\n",
      "\n",
      "Columns of credit_card:\n",
      "SK_ID_PREV --- SK_ID_CURR --- MONTHS_BALANCE --- AMT_BALANCE --- AMT_CREDIT_LIMIT_ACTUAL --- AMT_DRAWINGS_ATM_CURRENT --- AMT_DRAWINGS_CURRENT --- AMT_DRAWINGS_OTHER_CURRENT --- AMT_DRAWINGS_POS_CURRENT --- AMT_INST_MIN_REGULARITY --- AMT_PAYMENT_CURRENT --- AMT_PAYMENT_TOTAL_CURRENT --- AMT_RECEIVABLE_PRINCIPAL --- AMT_RECIVABLE --- AMT_TOTAL_RECEIVABLE --- CNT_DRAWINGS_ATM_CURRENT --- CNT_DRAWINGS_CURRENT --- CNT_DRAWINGS_OTHER_CURRENT --- CNT_DRAWINGS_POS_CURRENT --- CNT_INSTALMENT_MATURE_CUM --- NAME_CONTRACT_STATUS --- SK_DPD --- SK_DPD_DEF\n"
     ]
    }
   ],
   "source": [
    "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\n",
    "print(\"Shape of credit_card:\",  credit_card.shape)\n",
    "\n",
    "print(\"\\nColumns of credit_card:\")\n",
    "print(\" --- \".join(credit_card.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"credit_nan\"></a>\n",
    "\n",
    "### [^](#toc) <u>Missing Values and Outliers</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:09<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "### Remove outliers\n",
    "# Gets indices with outlier values\n",
    "temp = credit_card[credit_card.NAME_CONTRACT_STATUS.isin([\"Refused\", \"Approved\"])].index\n",
    "\n",
    "# Drops outlier values\n",
    "credit_card = credit_card.drop(temp, axis=0)\n",
    "\n",
    "# ------------------------------\n",
    "#### Fill in missing values\n",
    "cols = [\n",
    "        \"AMT_DRAWINGS_ATM_CURRENT\", \"AMT_DRAWINGS_OTHER_CURRENT\", \"AMT_DRAWINGS_POS_CURRENT\", \n",
    "        \"AMT_INST_MIN_REGULARITY\", \"AMT_PAYMENT_CURRENT\", \"CNT_DRAWINGS_ATM_CURRENT\", \n",
    "        \"CNT_DRAWINGS_OTHER_CURRENT\", \"CNT_DRAWINGS_POS_CURRENT\", \"CNT_INSTALMENT_MATURE_CUM\"\n",
    "]\n",
    "for col in tqdm(cols):\n",
    "    not_null = credit_card[col].notnull()\n",
    "    mode = float(credit_card[not_null][col].mode())\n",
    "    credit_card[col] = credit_card[col].fillna(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = credit_card[[\"SK_ID_PREV\", \"NAME_CONTRACT_STATUS\"]]\n",
    "\n",
    "temp = get_dummies(temp, [\"NAME_CONTRACT_STATUS\"])\n",
    "temp = temp.drop(\"NAME_CONTRACT_STATUS\", axis=1)\n",
    "temp = temp.groupby(\"SK_ID_PREV\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_df = pd.DataFrame({\n",
    "    \"AMT_BALANCE\": credit_card.groupby(\"SK_ID_PREV\").AMT_BALANCE.mean(),\n",
    "    \"SK_DPD\":      credit_card.groupby(\"SK_ID_PREV\").SK_DPD.max(),\n",
    "    \"SK_DPD_DEF\":  credit_card.groupby(\"SK_ID_PREV\").SK_DPD_DEF.max(),\n",
    "    \"N\":           credit_card.groupby(\"SK_ID_PREV\").count().iloc[:,0]\n",
    "})\n",
    "\n",
    "merge_df = temp.join(merge_df)\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right = credit_card.drop(\"SK_ID_CURR\", axis=1).groupby(\"SK_ID_PREV\").median().reset_index()\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_PREV\").set_index(\"SK_ID_PREV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge_credit\"></a>\n",
    "\n",
    "### [^](#toc) <u>Merge into Previous Application</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_cols = ['credit_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "prev_app = prev_app.merge(right=merge_df.reset_index(), how='left', on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in new NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:07<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "prev_app[\"no_credit\"] = prev_app[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null = prev_app[col].notnull()\n",
    "    median = prev_app[not_null][col].median()\n",
    "    prev_app[col] = prev_app[col].fillna(median)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Misc clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null in prev_app: 19985382\n",
      "Number of null in bureau:   0\n"
     ]
    }
   ],
   "source": [
    "### Drop unneeded ID columns\n",
    "prev_app = prev_app.drop(\"SK_ID_PREV\", axis=1)\n",
    "bureau   = bureau.drop(\"SK_ID_BUREAU\", axis=1)\n",
    "\n",
    "print(\"Number of null in prev_app:\", sum(prev_app.isnull().sum()))\n",
    "print(\"Number of null in bureau:  \", sum(bureau.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"final_merge\"></a>\n",
    "\n",
    "# [^](#toc) <u>Final Data Prep</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (307511, 122)\n",
      "Shape of test: (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "test  = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "\n",
    "print(\"Shape of train:\", train.shape)\n",
    "print(\"Shape of test:\",  test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into predictors, target, and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train.TARGET\n",
    "train_x = train.drop([\"TARGET\"], axis=1)\n",
    "\n",
    "test_id = test.SK_ID_CURR\n",
    "test_x  = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full    = pd.concat([train_x, test_x])\n",
    "train_N = len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final_nan\"></a>\n",
    "\n",
    "### [^](#toc) <u>Missing values</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = fillna_cat(full)\n",
    "full = fillna_num(full)\n",
    "sum(full.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lump together values with low counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Bureau\n",
    "\n",
    "# CREDIT_CURRENCY\n",
    "cols = [\"currency 3\", \"currency 4\"]\n",
    "bureau.CREDIT_CURRENCY = bureau.CREDIT_CURRENCY.map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "# CREDIT_TYPE\n",
    "cols = [\"Cash loan (non-earmarked)\", \"Real estate loan\", \"Loan for the purchase of equipment\",\n",
    "        \"Loan for purchase of shares (margin lending)\", \"Interbank credit\", \"Mobile operator loan\"]\n",
    "bureau.CREDIT_TYPE = bureau.CREDIT_TYPE.map(lambda x: \"MISC\" if x in cols else x)\n",
    "\n",
    "##### Previous Application\n",
    "\n",
    "# NAME_GOODS_CATEGORY\n",
    "prev_app.NAME_GOODS_CATEGORY = prev_app.NAME_GOODS_CATEGORY.map(\n",
    "    lambda x: \"MISC\" if x in [\"Weapon\", \"Insurance\"] else x)\n",
    "\n",
    "# NAME_CASH_LOAN_PURPOSE\n",
    "prev_app.NAME_CASH_LOAN_PURPOSE = prev_app.NAME_CASH_LOAN_PURPOSE.map(\n",
    "    lambda x: \"MISC\" if x in [\"Buying a garage\", \"Misc\"] else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get categorical features\n",
    "data_cats = [col for col in full.columns if full[col].dtype == 'object']\n",
    "\n",
    "# Factorize the dataframe\n",
    "full = factorize_df(full, data_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Previous Application with full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cat_cols = [\n",
    "#         \"NAME_CONTRACT_TYPE\", \"WEEKDAY_APPR_PROCESS_START\",\n",
    "#         \"FLAG_LAST_APPL_PER_CONTRACT\", \"NAME_CASH_LOAN_PURPOSE\",\n",
    "#         \"NAME_CONTRACT_STATUS\", \"NAME_PAYMENT_TYPE\",\n",
    "#         \"CODE_REJECT_REASON\", \"NAME_TYPE_SUITE\", \"NAME_CLIENT_TYPE\",\n",
    "#         \"NAME_GOODS_CATEGORY\", \"NAME_PORTFOLIO\", \"NAME_PRODUCT_TYPE\",\n",
    "#         \"CHANNEL_TYPE\", \"NAME_SELLER_INDUSTRY\", \"NAME_YIELD_GROUP\",\n",
    "#         \"PRODUCT_COMBINATION\", \"SK_ID_CURR\"]\n",
    "# num_cols = [col for col in prev_app.columns if col not in cat_cols]\n",
    "# num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "# # Numeric columns\n",
    "# merge_df      = prev_app[num_cols].groupby('SK_ID_CURR').mean()\n",
    "# merge_df[\"N\"] = prev_app.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "\n",
    "# # Categorical columns\n",
    "# right = prev_app[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "# right = pd.get_dummies(right).reset_index()\n",
    "# right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "\n",
    "# merge_df = merge_df.reset_index()\n",
    "# merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "\n",
    "# merged_cols   = ['p_' + col for col in merge_df.columns]\n",
    "# merge_df.columns = merged_cols\n",
    "\n",
    "# full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW!  Adding way more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "        \"NAME_CONTRACT_TYPE\", \"WEEKDAY_APPR_PROCESS_START\",\n",
    "        \"FLAG_LAST_APPL_PER_CONTRACT\", \"NAME_CASH_LOAN_PURPOSE\",\n",
    "        \"NAME_CONTRACT_STATUS\", \"NAME_PAYMENT_TYPE\",\n",
    "        \"CODE_REJECT_REASON\", \"NAME_TYPE_SUITE\", \"NAME_CLIENT_TYPE\",\n",
    "        \"NAME_GOODS_CATEGORY\", \"NAME_PORTFOLIO\", \"NAME_PRODUCT_TYPE\",\n",
    "        \"CHANNEL_TYPE\", \"NAME_SELLER_INDUSTRY\", \"NAME_YIELD_GROUP\",\n",
    "        \"PRODUCT_COMBINATION\", \"SK_ID_CURR\"]\n",
    "num_cols = [col for col in prev_app.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "############## Numeric columns ##############\n",
    "### Mean\n",
    "merge_df         = prev_app[num_cols].groupby('SK_ID_CURR').mean()\n",
    "merged_cols      = ['avg_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "### Max\n",
    "right         = prev_app[num_cols].groupby('SK_ID_CURR').max()\n",
    "right_cols    = ['max_' + col for col in right.columns]\n",
    "right.columns = right_cols\n",
    "\n",
    "### Merge Mean and Max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "\n",
    "############# Categorical columns #############\n",
    "right = prev_app[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "\n",
    "merge_df[\"N\"] = prev_app.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "merged_cols   = ['p_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [04:54<00:00,  1.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full[\"no_prev_app\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null  = full[col].notnull()\n",
    "    median    = full[not_null][col].median()\n",
    "    full[col] = full[col].fillna(median)    \n",
    "    \n",
    "sum(full.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Bureau with full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE', 'SK_ID_CURR']\n",
    "num_cols = [col for col in bureau.columns if col not in cat_cols]\n",
    "num_cols.append(\"SK_ID_CURR\")\n",
    "\n",
    "############## Numeric columns ##############\n",
    "### Mean\n",
    "merge_df         = bureau[num_cols].groupby('SK_ID_CURR').mean()\n",
    "merged_cols      = ['avg_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "### Max\n",
    "right         = bureau[num_cols].groupby('SK_ID_CURR').max()\n",
    "right_cols    = ['max_' + col for col in right.columns]\n",
    "right.columns = right_cols\n",
    "\n",
    "### Merge Mean and Max\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right.reset_index(), how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "\n",
    "############# Categorical columns #############\n",
    "right = bureau[cat_cols].set_index(\"SK_ID_CURR\")\n",
    "right = pd.get_dummies(right).reset_index()\n",
    "right = right.groupby(\"SK_ID_CURR\").sum().reset_index()\n",
    "\n",
    "merge_df = merge_df.reset_index()\n",
    "merge_df = merge_df.merge(right=right, how=\"left\", on=\"SK_ID_CURR\").set_index(\"SK_ID_CURR\")\n",
    "\n",
    "merge_df[\"N\"] = bureau.groupby('SK_ID_CURR').count().iloc[:,0]\n",
    "merged_cols   = ['b_' + col for col in merge_df.columns]\n",
    "merge_df.columns = merged_cols\n",
    "\n",
    "full = full.merge(right=merge_df.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:06<00:00,  1.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full[\"no_bureau\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\n",
    "\n",
    "for col in tqdm(merged_cols):\n",
    "    not_null  = full[col].notnull()\n",
    "    median    = full[not_null][col].median()\n",
    "    full[col] = full[col].fillna(median)    \n",
    "\n",
    "sum(full.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full = full.drop(['APARTMENTS_MODE', 'BASEMENTAREA_MODE',\n",
    "#        'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE',\n",
    "#        'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE',\n",
    "#        'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE',\n",
    "#        'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n",
    "#        'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI',\n",
    "#        'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI',\n",
    "#        'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI',\n",
    "#        'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI',\n",
    "#        'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI',\n",
    "#        'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI'], axis=1)\n",
    "\n",
    "full = full.drop(\"SK_ID_CURR\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split full back into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = full[:train_N]\n",
    "test_x = full[train_N:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed data look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE</th>\n",
       "      <th>...</th>\n",
       "      <th>b_CREDIT_TYPE_Consumer credit</th>\n",
       "      <th>b_CREDIT_TYPE_Credit card</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for business development</th>\n",
       "      <th>b_CREDIT_TYPE_Loan for working capital replenishment</th>\n",
       "      <th>b_CREDIT_TYPE_MISC</th>\n",
       "      <th>b_CREDIT_TYPE_Microloan</th>\n",
       "      <th>b_CREDIT_TYPE_Mortgage</th>\n",
       "      <th>b_CREDIT_TYPE_Unknown type of loan</th>\n",
       "      <th>b_N</th>\n",
       "      <th>no_bureau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 483 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0                   0            0             0                0   \n",
       "1                   0            1             0                1   \n",
       "2                   1            0             1                0   \n",
       "3                   0            1             0                0   \n",
       "4                   0            0             0                0   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "0             0          202500.0    406597.5      24700.5         351000.0   \n",
       "1             0          270000.0   1293502.5      35698.5        1129500.0   \n",
       "2             0           67500.0    135000.0       6750.0         135000.0   \n",
       "3             0          135000.0    312682.5      29686.5         297000.0   \n",
       "4             0          121500.0    513000.0      21865.5         513000.0   \n",
       "\n",
       "   NAME_TYPE_SUITE    ...      b_CREDIT_TYPE_Consumer credit  \\\n",
       "0                0    ...                                4.0   \n",
       "1                1    ...                                2.0   \n",
       "2                0    ...                                2.0   \n",
       "3                0    ...                                3.0   \n",
       "4                0    ...                                1.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Credit card  b_CREDIT_TYPE_Loan for business development  \\\n",
       "0                        4.0                                          0.0   \n",
       "1                        2.0                                          0.0   \n",
       "2                        0.0                                          0.0   \n",
       "3                        1.0                                          0.0   \n",
       "4                        0.0                                          0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Loan for working capital replenishment  b_CREDIT_TYPE_MISC  \\\n",
       "0                                                0.0                    0.0   \n",
       "1                                                0.0                    0.0   \n",
       "2                                                0.0                    0.0   \n",
       "3                                                0.0                    0.0   \n",
       "4                                                0.0                    0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Microloan  b_CREDIT_TYPE_Mortgage  \\\n",
       "0                      0.0                     0.0   \n",
       "1                      0.0                     0.0   \n",
       "2                      0.0                     0.0   \n",
       "3                      0.0                     0.0   \n",
       "4                      0.0                     0.0   \n",
       "\n",
       "   b_CREDIT_TYPE_Unknown type of loan  b_N  no_bureau  \n",
       "0                                 0.0  8.0          0  \n",
       "1                                 0.0  4.0          0  \n",
       "2                                 0.0  2.0          0  \n",
       "3                                 0.0  4.0          1  \n",
       "4                                 0.0  1.0          0  \n",
       "\n",
       "[5 rows x 483 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# [^](#toc) <u>Models </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sban's method\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "tried\n",
    "\n",
    "## learning rate\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[200]\tvalid_0's auc: 0.741669\n",
      "[400]\tvalid_0's auc: 0.756773\n",
      "[600]\tvalid_0's auc: 0.76861\n",
      "[800]\tvalid_0's auc: 0.774518\n",
      "[1000]\tvalid_0's auc: 0.777448\n",
      "[1200]\tvalid_0's auc: 0.779481\n",
      "[1400]\tvalid_0's auc: 0.780592\n",
      "[1600]\tvalid_0's auc: 0.781474\n",
      "[1800]\tvalid_0's auc: 0.782084\n",
      "[2000]\tvalid_0's auc: 0.782351\n",
      "[2200]\tvalid_0's auc: 0.782535\n",
      "[2400]\tvalid_0's auc: 0.78272\n",
      "Early stopping, best iteration is:\n",
      "[2381]\tvalid_0's auc: 0.78276\n",
      "Training took 634 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import lightgbm as lgb\n",
    "    \n",
    "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)\n",
    "lgb_train = lgb.Dataset(data=training_x, label=training_y)\n",
    "lgb_eval  = lgb.Dataset(data=val_x, label=val_y)\n",
    "\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n",
    "          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n",
    "          'min_split_gain':.01, 'min_child_weight':1}\n",
    "\n",
    "start = time.time()\n",
    "model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\n",
    "print(\"Training took {} seconds\".format(round(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW!  More iterations\n",
    "\n",
    "add a clear output before each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "train size: 246008\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's auc: 0.739765\n",
      "[200]\tvalid_0's auc: 0.745511\n",
      "[300]\tvalid_0's auc: 0.753766\n",
      "[400]\tvalid_0's auc: 0.761816\n",
      "[500]\tvalid_0's auc: 0.768521\n",
      "[600]\tvalid_0's auc: 0.773005\n",
      "[700]\tvalid_0's auc: 0.775818\n",
      "[800]\tvalid_0's auc: 0.777982\n",
      "[900]\tvalid_0's auc: 0.779624\n",
      "[1000]\tvalid_0's auc: 0.780744\n",
      "[1100]\tvalid_0's auc: 0.781763\n",
      "[1200]\tvalid_0's auc: 0.782467\n",
      "[1300]\tvalid_0's auc: 0.783046\n",
      "[1400]\tvalid_0's auc: 0.7835\n",
      "[1500]\tvalid_0's auc: 0.783864\n",
      "[1600]\tvalid_0's auc: 0.7842\n",
      "[1700]\tvalid_0's auc: 0.784374\n",
      "[1800]\tvalid_0's auc: 0.784502\n",
      "[1900]\tvalid_0's auc: 0.78462\n",
      "[2000]\tvalid_0's auc: 0.784706\n",
      "[2100]\tvalid_0's auc: 0.784671\n",
      "[2200]\tvalid_0's auc: 0.784717\n",
      "[2300]\tvalid_0's auc: 0.784727\n",
      "[2400]\tvalid_0's auc: 0.784796\n",
      "[2500]\tvalid_0's auc: 0.784857\n",
      "[2600]\tvalid_0's auc: 0.784883\n",
      "[2700]\tvalid_0's auc: 0.784813\n",
      "[2800]\tvalid_0's auc: 0.784839\n",
      "[2900]\tvalid_0's auc: 0.784895\n",
      "[3000]\tvalid_0's auc: 0.784831\n",
      "[3100]\tvalid_0's auc: 0.784813\n",
      "[3200]\tvalid_0's auc: 0.784782\n",
      "[3300]\tvalid_0's auc: 0.784806\n",
      "Early stopping, best iteration is:\n",
      "[2864]\tvalid_0's auc: 0.784913\n",
      "Training took 754 seconds\n",
      "Predicting took 18 seconds\n",
      "\n",
      "\n",
      "train size: 246008\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's auc: 0.735821\n",
      "[200]\tvalid_0's auc: 0.742414\n",
      "[300]\tvalid_0's auc: 0.750684\n",
      "[400]\tvalid_0's auc: 0.758725\n",
      "[500]\tvalid_0's auc: 0.765382\n",
      "[600]\tvalid_0's auc: 0.770151\n",
      "[700]\tvalid_0's auc: 0.773438\n",
      "[800]\tvalid_0's auc: 0.775693\n",
      "[900]\tvalid_0's auc: 0.777222\n",
      "[1000]\tvalid_0's auc: 0.778406\n",
      "[1100]\tvalid_0's auc: 0.779331\n",
      "[1200]\tvalid_0's auc: 0.780145\n",
      "[1300]\tvalid_0's auc: 0.78068\n",
      "[1400]\tvalid_0's auc: 0.781214\n",
      "[1500]\tvalid_0's auc: 0.781754\n",
      "[1600]\tvalid_0's auc: 0.782221\n",
      "[1700]\tvalid_0's auc: 0.782656\n",
      "[1800]\tvalid_0's auc: 0.782887\n",
      "[1900]\tvalid_0's auc: 0.783212\n",
      "[2000]\tvalid_0's auc: 0.783399\n",
      "[2100]\tvalid_0's auc: 0.783502\n",
      "[2200]\tvalid_0's auc: 0.783679\n",
      "[2300]\tvalid_0's auc: 0.783858\n",
      "[2400]\tvalid_0's auc: 0.783871\n",
      "[2500]\tvalid_0's auc: 0.783876\n",
      "[2600]\tvalid_0's auc: 0.783908\n",
      "[2700]\tvalid_0's auc: 0.784004\n",
      "[2800]\tvalid_0's auc: 0.784073\n",
      "[2900]\tvalid_0's auc: 0.78406\n",
      "[3000]\tvalid_0's auc: 0.784107\n",
      "[3100]\tvalid_0's auc: 0.784105\n",
      "[3200]\tvalid_0's auc: 0.784118\n",
      "[3300]\tvalid_0's auc: 0.784102\n",
      "[3400]\tvalid_0's auc: 0.784067\n",
      "[3500]\tvalid_0's auc: 0.784088\n",
      "Early stopping, best iteration is:\n",
      "[3065]\tvalid_0's auc: 0.78414\n",
      "Training took 795 seconds\n",
      "Predicting took 23 seconds\n",
      "\n",
      "\n",
      "train size: 246008\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's auc: 0.735675\n",
      "[200]\tvalid_0's auc: 0.741235\n",
      "[300]\tvalid_0's auc: 0.749314\n",
      "[400]\tvalid_0's auc: 0.756901\n",
      "[500]\tvalid_0's auc: 0.763394\n",
      "[600]\tvalid_0's auc: 0.767881\n",
      "[700]\tvalid_0's auc: 0.770953\n",
      "[800]\tvalid_0's auc: 0.773143\n",
      "[900]\tvalid_0's auc: 0.774968\n",
      "[1000]\tvalid_0's auc: 0.7762\n",
      "[1100]\tvalid_0's auc: 0.777126\n",
      "[1200]\tvalid_0's auc: 0.777811\n",
      "[1300]\tvalid_0's auc: 0.778396\n",
      "[1400]\tvalid_0's auc: 0.778753\n",
      "[1500]\tvalid_0's auc: 0.779125\n",
      "[1600]\tvalid_0's auc: 0.779354\n",
      "[1700]\tvalid_0's auc: 0.779597\n",
      "[1800]\tvalid_0's auc: 0.779899\n",
      "[1900]\tvalid_0's auc: 0.779978\n",
      "[2000]\tvalid_0's auc: 0.780091\n",
      "[2100]\tvalid_0's auc: 0.780216\n",
      "[2200]\tvalid_0's auc: 0.780312\n",
      "[2300]\tvalid_0's auc: 0.780376\n",
      "[2400]\tvalid_0's auc: 0.780596\n",
      "[2500]\tvalid_0's auc: 0.780605\n",
      "[2600]\tvalid_0's auc: 0.780685\n",
      "[2700]\tvalid_0's auc: 0.780764\n",
      "[2800]\tvalid_0's auc: 0.78071\n",
      "[2900]\tvalid_0's auc: 0.780677\n",
      "[3000]\tvalid_0's auc: 0.780663\n",
      "[3100]\tvalid_0's auc: 0.780658\n",
      "[3200]\tvalid_0's auc: 0.780617\n",
      "Early stopping, best iteration is:\n",
      "[2734]\tvalid_0's auc: 0.780795\n",
      "Training took 877 seconds\n",
      "Predicting took 20 seconds\n",
      "\n",
      "\n",
      "train size: 246008\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's auc: 0.740184\n",
      "[200]\tvalid_0's auc: 0.747346\n",
      "[300]\tvalid_0's auc: 0.754778\n",
      "[400]\tvalid_0's auc: 0.761551\n",
      "[500]\tvalid_0's auc: 0.767003\n",
      "[600]\tvalid_0's auc: 0.771069\n",
      "[700]\tvalid_0's auc: 0.774246\n",
      "[800]\tvalid_0's auc: 0.776333\n",
      "[900]\tvalid_0's auc: 0.777641\n",
      "[1000]\tvalid_0's auc: 0.778758\n",
      "[1100]\tvalid_0's auc: 0.779559\n",
      "[1200]\tvalid_0's auc: 0.780135\n",
      "[1300]\tvalid_0's auc: 0.780684\n",
      "[1400]\tvalid_0's auc: 0.78118\n",
      "[1500]\tvalid_0's auc: 0.781473\n",
      "[1600]\tvalid_0's auc: 0.781819\n",
      "[1700]\tvalid_0's auc: 0.782121\n",
      "[1800]\tvalid_0's auc: 0.782416\n",
      "[1900]\tvalid_0's auc: 0.78257\n",
      "[2000]\tvalid_0's auc: 0.782661\n",
      "[2100]\tvalid_0's auc: 0.782801\n",
      "[2200]\tvalid_0's auc: 0.782925\n",
      "[2300]\tvalid_0's auc: 0.783\n",
      "[2400]\tvalid_0's auc: 0.783176\n",
      "[2500]\tvalid_0's auc: 0.783197\n",
      "[2600]\tvalid_0's auc: 0.783306\n",
      "[2700]\tvalid_0's auc: 0.783367\n",
      "[2800]\tvalid_0's auc: 0.783327\n",
      "[2900]\tvalid_0's auc: 0.783327\n",
      "[3000]\tvalid_0's auc: 0.783352\n",
      "[3100]\tvalid_0's auc: 0.783339\n",
      "[3200]\tvalid_0's auc: 0.783351\n",
      "[3300]\tvalid_0's auc: 0.783306\n",
      "[3400]\tvalid_0's auc: 0.783276\n",
      "[3500]\tvalid_0's auc: 0.783141\n",
      "Early stopping, best iteration is:\n",
      "[3079]\tvalid_0's auc: 0.783383\n",
      "Training took 889 seconds\n",
      "Predicting took 20 seconds\n",
      "\n",
      "\n",
      "train size: 246008\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's auc: 0.740804\n",
      "[200]\tvalid_0's auc: 0.746966\n",
      "[300]\tvalid_0's auc: 0.755511\n",
      "[400]\tvalid_0's auc: 0.763779\n",
      "[500]\tvalid_0's auc: 0.770387\n",
      "[600]\tvalid_0's auc: 0.774998\n",
      "[700]\tvalid_0's auc: 0.778165\n",
      "[800]\tvalid_0's auc: 0.780276\n",
      "[900]\tvalid_0's auc: 0.78181\n",
      "[1000]\tvalid_0's auc: 0.782805\n",
      "[1100]\tvalid_0's auc: 0.783746\n",
      "[1200]\tvalid_0's auc: 0.78441\n",
      "[1300]\tvalid_0's auc: 0.784902\n",
      "[1400]\tvalid_0's auc: 0.785344\n",
      "[1500]\tvalid_0's auc: 0.785735\n",
      "[1600]\tvalid_0's auc: 0.786118\n",
      "[1700]\tvalid_0's auc: 0.786419\n",
      "[1800]\tvalid_0's auc: 0.786654\n",
      "[1900]\tvalid_0's auc: 0.786822\n",
      "[2000]\tvalid_0's auc: 0.787009\n",
      "[2100]\tvalid_0's auc: 0.787092\n",
      "[2200]\tvalid_0's auc: 0.787204\n",
      "[2300]\tvalid_0's auc: 0.787288\n",
      "[2400]\tvalid_0's auc: 0.787386\n",
      "[2500]\tvalid_0's auc: 0.787428\n",
      "[2600]\tvalid_0's auc: 0.787483\n",
      "[2700]\tvalid_0's auc: 0.787603\n",
      "[2800]\tvalid_0's auc: 0.787655\n",
      "[2900]\tvalid_0's auc: 0.787625\n",
      "[3000]\tvalid_0's auc: 0.787548\n",
      "[3100]\tvalid_0's auc: 0.787522\n",
      "[3200]\tvalid_0's auc: 0.787465\n",
      "[3300]\tvalid_0's auc: 0.787432\n",
      "Early stopping, best iteration is:\n",
      "[2829]\tvalid_0's auc: 0.787668\n",
      "Training took 716 seconds\n",
      "Predicting took 19 seconds\n",
      "Total training took 19 seconds\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "NUM_ITER = 5\n",
    "\n",
    "preds = pd.DataFrame()\n",
    "\n",
    "start1 = time.time()\n",
    "for i in range(NUM_ITER):\n",
    "    training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2)\n",
    "    lgb_train = lgb.Dataset(data=training_x, label=training_y)\n",
    "    lgb_eval  = lgb.Dataset(data=val_x, label=val_y)\n",
    "    \n",
    "    ### Delete old variables\n",
    "    del training_x, val_x, training_y, val_y\n",
    "    gc.collect()\n",
    "\n",
    "    ### Training\n",
    "    start2 = time.time()\n",
    "    params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n",
    "          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n",
    "          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n",
    "          'min_split_gain':.01, 'min_child_weight':1}\n",
    "    model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=500, verbose_eval=100)\n",
    "    print(\"Training took {} seconds\".format(round(time.time() - start2)))\n",
    "    \n",
    "    ### Predicting\n",
    "    start2   = time.time()\n",
    "    pred     = model.predict(test_x)\n",
    "    preds[i] = pred\n",
    "    print(\"Predicting took {} seconds\".format(round(time.time() - start2)))\n",
    "    \n",
    "    ### Delete old variables\n",
    "    del lgb_train, lgb_eval, pred, model\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"Total training took {} seconds\".format(round(time.time() - start2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.027977\n",
       "1        0.129554\n",
       "2        0.025091\n",
       "3        0.033974\n",
       "4        0.136601\n",
       "5        0.047445\n",
       "6        0.006938\n",
       "7        0.022963\n",
       "8        0.013601\n",
       "9        0.090261\n",
       "10       0.053548\n",
       "11       0.027306\n",
       "12       0.169998\n",
       "13       0.043243\n",
       "14       0.047789\n",
       "15       0.235612\n",
       "16       0.052854\n",
       "17       0.017143\n",
       "18       0.060678\n",
       "19       0.034247\n",
       "20       0.028206\n",
       "21       0.008363\n",
       "22       0.042622\n",
       "23       0.101389\n",
       "24       0.036326\n",
       "25       0.094908\n",
       "26       0.087442\n",
       "27       0.062852\n",
       "28       0.034161\n",
       "29       0.046265\n",
       "           ...   \n",
       "48714    0.027262\n",
       "48715    0.011388\n",
       "48716    0.242871\n",
       "48717    0.010625\n",
       "48718    0.036675\n",
       "48719    0.132723\n",
       "48720    0.027010\n",
       "48721    0.106396\n",
       "48722    0.131091\n",
       "48723    0.096111\n",
       "48724    0.107426\n",
       "48725    0.078470\n",
       "48726    0.042967\n",
       "48727    0.013109\n",
       "48728    0.008085\n",
       "48729    0.108638\n",
       "48730    0.070992\n",
       "48731    0.025013\n",
       "48732    0.112732\n",
       "48733    0.030133\n",
       "48734    0.035333\n",
       "48735    0.057536\n",
       "48736    0.020606\n",
       "48737    0.164455\n",
       "48738    0.091523\n",
       "48739    0.059526\n",
       "48740    0.069112\n",
       "48741    0.013417\n",
       "48742    0.034840\n",
       "48743    0.187153\n",
       "Length: 48744, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Found on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(data)):\n",
    "    trn_x, trn_y = data[feats].iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_x, val_y = data[feats].iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    clf = LGBMClassifier(\n",
    "        # n_estimators=1000,\n",
    "        # num_leaves=20,\n",
    "        # colsample_bytree=.8,\n",
    "        # subsample=.8,\n",
    "        # max_depth=7,\n",
    "        # reg_alpha=.1,\n",
    "        # reg_lambda=.1,\n",
    "        # min_split_gain=.01\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves = 40,\n",
    "        colsample_bytree=0.9497036,\n",
    "        subsample=0.8715623,\n",
    "        max_depth=10,\n",
    "        reg_alpha=0.041545473,\n",
    "        reg_lambda=0.0735294,\n",
    "        min_split_gain=0.0222415,\n",
    "        min_child_weight=39.3259775,\n",
    "        silent=-1,\n",
    "        verbose=-1,\n",
    "    )\n",
    "    \n",
    "    clf.fit(trn_x, trn_y, \n",
    "            eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "            eval_metric='auc', verbose=100, early_stopping_rounds=300  #30\n",
    "           )\n",
    "    \n",
    "    oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "    sub_preds += clf.predict_proba(test[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = feats\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "    del clf, trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions = model.predict(test_x)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"SK_ID_CURR\": test_id,\n",
    "    \"TARGET\": preds.mean(axis=1)\n",
    "}).to_csv(\"../submissions/5_runs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
