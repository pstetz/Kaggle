{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns:\n",
      "\n",
      "Numerical columns:\n",
      "Id --- Elevation --- Aspect --- Slope --- Horizontal_Distance_To_Hydrology --- Vertical_Distance_To_Hydrology --- Horizontal_Distance_To_Roadways --- Hillshade_9am --- Hillshade_Noon --- Hillshade_3pm --- Horizontal_Distance_To_Fire_Points --- Wilderness_Area1 --- Wilderness_Area2 --- Wilderness_Area3 --- Wilderness_Area4 --- Soil_Type1 --- Soil_Type2 --- Soil_Type3 --- Soil_Type4 --- Soil_Type5 --- Soil_Type6 --- Soil_Type7 --- Soil_Type8 --- Soil_Type9 --- Soil_Type10 --- Soil_Type11 --- Soil_Type12 --- Soil_Type13 --- Soil_Type14 --- Soil_Type15 --- Soil_Type16 --- Soil_Type17 --- Soil_Type18 --- Soil_Type19 --- Soil_Type20 --- Soil_Type21 --- Soil_Type22 --- Soil_Type23 --- Soil_Type24 --- Soil_Type25 --- Soil_Type26 --- Soil_Type27 --- Soil_Type28 --- Soil_Type29 --- Soil_Type30 --- Soil_Type31 --- Soil_Type32 --- Soil_Type33 --- Soil_Type34 --- Soil_Type35 --- Soil_Type36 --- Soil_Type37 --- Soil_Type38 --- Soil_Type39 --- Soil_Type40 --- Cover_Type\n",
      "\n",
      "Shape of train: (15120, 56)\n",
      "Shape of test: (565892, 55)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../../data/Forest-Cover/\"\n",
    "\n",
    "train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "test  = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "\n",
    "cat_columns = [col for col in train.columns if train[col].dtype == object]\n",
    "print(\"Categorical columns:\")\n",
    "print(\" --- \".join(cat_columns))\n",
    "\n",
    "### Numerical columns\n",
    "num_columns = [col for col in train.columns if train[col].dtype != object]\n",
    "print(\"Numerical columns:\")\n",
    "print(\" --- \".join(num_columns))\n",
    "print()\n",
    "print(\"Shape of train:\", train.shape)\n",
    "print(\"Shape of test:\",  test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into id, target, and predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train[\"Cover_Type\"]\n",
    "train_id = train[\"Id\"]\n",
    "train_x = train.drop([\"Cover_Type\", \"Id\"], axis=1)\n",
    "\n",
    "test_id = test[\"Id\"]\n",
    "test_x  = test.drop(\"Id\", axis=1)\n",
    "\n",
    "full    = pd.concat([train_x, test_x])\n",
    "train_N = len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full['HF1'] = abs(full['Horizontal_Distance_To_Hydrology']   + full['Horizontal_Distance_To_Fire_Points'])\n",
    "full['HF2'] = abs(full['Horizontal_Distance_To_Hydrology']   - full['Horizontal_Distance_To_Fire_Points'])\n",
    "full['HR1'] = abs(full['Horizontal_Distance_To_Hydrology']   + full['Horizontal_Distance_To_Roadways'])\n",
    "full['HR2'] = abs(full['Horizontal_Distance_To_Hydrology']   - full['Horizontal_Distance_To_Roadways'])\n",
    "full['FR1'] = abs(full['Horizontal_Distance_To_Fire_Points'] + full['Horizontal_Distance_To_Roadways'])\n",
    "full['FR2'] = abs(full['Horizontal_Distance_To_Fire_Points'] - full['Horizontal_Distance_To_Roadways'])\n",
    "full['ele_vert'] = full.Elevation-full.Vertical_Distance_To_Hydrology\n",
    "\n",
    "full['slope_hyd'] = (full['Horizontal_Distance_To_Hydrology']**2 + full['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "full[\"slope_hyd\"] = full.slope_hyd.map(lambda x: 0 if np.isinf(x) else x)\n",
    "\n",
    "full['Mean_Amenities'] = (full.Horizontal_Distance_To_Fire_Points +\n",
    "                          full.Horizontal_Distance_To_Hydrology +\n",
    "                          full.Horizontal_Distance_To_Roadways) / 3 \n",
    "full['Mean_Fire_Hyd']  = (full.Horizontal_Distance_To_Fire_Points +\n",
    "                          full.Horizontal_Distance_To_Hydrology) / 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split back into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = full[:train_N]\n",
    "test_x  = full[train_N:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,\n",
    "                              ExtraTreesClassifier, VotingClassifier)\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "\n",
    "TAKE_CV = False\n",
    "first_layer_train = pd.DataFrame()\n",
    "first_layer_val   = pd.DataFrame()\n",
    "first_layer_preds = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees 1\n",
    "\n",
    "This works really well, so I'm going to create a second model with different params and random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etc_model_1 = ExtraTreesClassifier(n_estimators=25, criterion='gini',\n",
    "                                   max_depth=None, min_samples_split=2,\n",
    "                                   min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                   max_features='auto', max_leaf_nodes=None,\n",
    "                                   min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                   bootstrap=False, random_state=17)\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(etc_model_1, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "etc_model_1.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"ETC1\"] = etc_model_1.predict(training_x)\n",
    "first_layer_val[\"ETC1\"]   = etc_model_1.predict(val_x)\n",
    "first_layer_preds[\"ETC1\"] = etc_model_1.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etc_model_2 = ExtraTreesClassifier(n_estimators=500, criterion='gini',\n",
    "                                   max_depth=None, min_samples_split=2,\n",
    "                                   min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                   max_features='auto', max_leaf_nodes=None,\n",
    "                                   min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                   bootstrap=False, random_state=71)\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(etc_model_2, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "etc_model_2.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"ETC2\"] = etc_model_2.predict(training_x)\n",
    "first_layer_val[\"ETC2\"]   = etc_model_2.predict(val_x)\n",
    "first_layer_preds[\"ETC2\"] = etc_model_2.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_model_1 = XGBClassifier(max_depth=4, learning_rate=1,\n",
    "                            n_estimators=100, gamma=0, min_child_weight=1,\n",
    "                            max_delta_step=0, subsample=1, colsample_bytree=0.6,\n",
    "                            colsample_bylevel=1, reg_alpha=3, reg_lambda=3,\n",
    "                            scale_pos_weight=1, base_score=0.5, random_state=17)\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(xgb_model_1, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "xgb_model_1.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"XGB1\"] = xgb_model_1.predict(training_x)\n",
    "first_layer_val[\"XGB1\"]   = xgb_model_1.predict(val_x)\n",
    "first_layer_preds[\"XGB1\"] = xgb_model_1.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Model 2\n",
    "\n",
    "This model is taken from [Siddharth Yadav](https://www.kaggle.com/thebrownviking20) and his excellent [kernel](https://www.kaggle.com/thebrownviking20/voting-classifier-for-victory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-185904b302c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Score is {score_mean} +/- {score_std}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mxgb_model_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfirst_layer_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"XGB2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_model_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost-0.7-py3.6.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[1;32m    504\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost-0.7-py3.6.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost-0.7-py3.6.egg/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost-0.7-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 895\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgb_model_2 = XGBClassifier(max_depth=20, n_estimators=1000, random_state=71)\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(xgb_model_2, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "xgb_model_2.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"XGB2\"] = xgb_model_2.predict(training_x)\n",
    "first_layer_val[\"XGB2\"]   = xgb_model_2.predict(val_x)\n",
    "first_layer_preds[\"XGB2\"] = xgb_model_2.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_model = RandomForestClassifier(n_estimators=25, criterion='gini',\n",
    "                                   max_depth=None, min_samples_split=6,\n",
    "                                   min_samples_leaf=3, min_weight_fraction_leaf=0.0,\n",
    "                                   max_features='auto', max_leaf_nodes=None,\n",
    "                                   min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                   random_state=17)\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(rfc_model, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "rfc_model.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"RFC\"] = rfc_model.predict(training_x)\n",
    "first_layer_val[\"RFC\"]   = rfc_model.predict(val_x)\n",
    "first_layer_preds[\"RFC\"] = rfc_model.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_model_1  = LGBMClassifier(num_leaves=45, max_depth=7,\n",
    "                              learning_rate=0.3,\n",
    "                              reg_lambda=0.5, reg_alpha=0.5,\n",
    "                              min_split_gain=0.1, min_child_weight=0.5,\n",
    "                              min_data_in_leaf=5,\n",
    "                              feature_fraction=0.5,\n",
    "                              random_state=17)\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(lgb_model_1, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "lgb_model_1.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"LGB1\"] = lgb_model_1.predict(training_x)\n",
    "first_layer_val[\"LGB1\"]   = lgb_model_1.predict(val_x)\n",
    "first_layer_preds[\"LGB1\"] = lgb_model_1.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_model_2  = LGBMClassifier(num_leaves=70, max_depth=8,\n",
    "                              learning_rate=0.1,\n",
    "                              reg_lambda=1, reg_alpha=1,\n",
    "                              min_split_gain=0.1, min_child_weight=0.5,\n",
    "                              min_data_in_leaf=5,\n",
    "                              feature_fraction=0.3,\n",
    "                              random_state=71)\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(lgb_model_2, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "lgb_model_2.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"LGB2\"] = lgb_model_2.predict(training_x)\n",
    "first_layer_val[\"LGB2\"]   = lgb_model_2.predict(val_x)\n",
    "first_layer_preds[\"LGB2\"] = lgb_model_2.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting 3\n",
    "\n",
    "This model is taken from [Siddharth Yadav](https://www.kaggle.com/thebrownviking20) and his excellent [kernel](https://www.kaggle.com/thebrownviking20/voting-classifier-for-victory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_model_3  = LGBMClassifier(n_estimators=2000, max_depth=15, random_state=171)\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(lgb_model_3, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "lgb_model_3.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"LGB3\"] = lgb_model_3.predict(training_x)\n",
    "first_layer_val[\"LGB3\"]   = lgb_model_3.predict(val_x)\n",
    "first_layer_preds[\"LGB3\"] = lgb_model_3.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost 1\n",
    "\n",
    "This model is taken from [Siddharth Yadav](https://www.kaggle.com/thebrownviking20) and his excellent [kernel](https://www.kaggle.com/thebrownviking20/voting-classifier-for-victory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada_model_1 = AdaBoostClassifier(\n",
    "                ExtraTreesClassifier(n_estimators=500),\n",
    "                n_estimators=250, learning_rate=0.01, algorithm='SAMME', random_state=17\n",
    ")\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(ada_model_1, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "ada_model_1.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"ADA1\"] = ada_model_1.predict(training_x)\n",
    "first_layer_val[\"ADA1\"]   = ada_model_1.predict(val_x)\n",
    "first_layer_preds[\"ADA1\"] = ada_model_1.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost 2\n",
    "\n",
    "This model is taken from [Siddharth Yadav](https://www.kaggle.com/thebrownviking20) and his excellent [kernel](https://www.kaggle.com/thebrownviking20/voting-classifier-for-victory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada_model_2 = AdaBoostClassifier(\n",
    "                GradientBoostingClassifier(n_estimators=1000, max_depth=10),\n",
    "                n_estimators=1000, learning_rate=0.01, algorithm=\"SAMME\", random_state=17\n",
    ")\n",
    "\n",
    "if TAKE_CV:\n",
    "    scores = cross_val_score(ada_model_2, train_x, train_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "\n",
    "ada_model_2.fit(train_x, train_y)\n",
    "\n",
    "first_layer_train[\"ADA2\"] = ada_model_2.predict(training_x)\n",
    "first_layer_val[\"ADA2\"]   = ada_model_2.predict(val_x)\n",
    "first_layer_preds[\"ADA2\"] = ada_model_2.predict(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voting_model = SVC()\n",
    "\n",
    "if TAKE_CV:\n",
    "    t_x = pd.concat([first_layer_train, first_layer_val])\n",
    "    t_y = pd.concat([training_y, val_y])\n",
    "    scores = cross_val_score(voting_model, t_x, t_y, cv=5, verbose=1)\n",
    "    score_mean = round(np.mean(scores), 4)\n",
    "    score_std  = round(np.std(scores), 3)\n",
    "    print(f\"Score is {score_mean} +/- {score_std}\")\n",
    "    \n",
    "voting_model.fit(first_layer_train, training_y)\n",
    "predictions = voting_model.predict(first_layer_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({\"Cover_Type\": predictions, \"Id\": test_id} )\n",
    "predictions.to_csv(\"../\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
